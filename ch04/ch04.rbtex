%%chapter%% 04
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '04',
    %q{More about limits; curve sketching},
    'ch:more-limits'
  )
%>

<% begin_sec("Computing limits",nil,'computing-limits') %>
In ch.~\ref{ch:limits} we did very few direct computations of limits. In this section we
learn some techniques for doing such computations.

<% begin_sec("The ``don't choose $\\delta$ to be too small'' trick",nil,'dont-choose') %>
Say we want to prove that $\lim_{x\to 1}x^2 = 1$. This may not seem to require a fancy
proof, since obviously plugging in $x=1$ gives $x^2=1$. But since functions can be
discontinuous, plugging in does not always prove the value of a limit.
Also, this example will be an excuse to develop a technique that can be useful in
less trivial cases.

We have $f(x) = x^2$, $a=1$, $L=1$, and as usual when computing a limit the question is, ``how small
should $|x-1|$ be to guarantee $|x^2-1|<\varepsilon$?''

We begin by estimating the difference $|x^2-1|$
\[
|x^2-1| = |(x-1)(x+1)| = |x+1|\cdot|x-1|.
\]
As $x$ approaches 1 the factor $|x-1|$ becomes small, and if the other
factor $|x+1|$ were a constant (e.g.\ $2$ as in the previous example) then
we could find $\delta$ as before, by dividing $\varepsilon$ by that
constant.

Here is a trick that allows you to replace the factor $|x+1|$ with a
constant.  We hereby agree \textit{that we always choose our $\delta$ so
that $\delta\leq 1$.}  If we do that, then we will always have
\[
|x-1|<\delta\leq 1, \text{i.e. }|x-1|<1,
\]
and $x$ will always be beween $0$ and $2$. Therefore
\[
|x^2-1| = |x+1|\cdot|x-1|<3|x-1|.
\]
If we now want to be sure that $|x^2-1|<\varepsilon$, then this calculation
shows that we should require $3|x-1|<\varepsilon$, i.e.\
$|x-1|<\frac13\varepsilon$.  So we should choose $\delta\leq
\frac13\varepsilon$.  We must also live up to our promise never to choose
$\delta>1$, so if we are handed an $\varepsilon$ for which
$\frac13\varepsilon>1$, then we choose $\delta=1$ instead of $\delta =
\frac13\varepsilon$.  To summarize, we are going to choose
\[
\delta = \text{the smaller of }1\text{ and }\frac13\varepsilon.
\]
We have shown that if you choose $\delta$ this way, then $|x-1|<\delta$
implies $|x^2-1|<\varepsilon$, no matter what $\varepsilon>0$ is.
                                                                                                                 
The expression ``the smaller of $a$ and $b$'' shows up often, and is
abbreviated to $\min(a, b)$.  We could therefore say that in this problem
we will choose $\delta$ to be
\[
\delta = \min \bigl(1, \tfrac13 \varepsilon\bigr).
\]

\begin{eg}{}
\egquestion Show that $\lim_{x\to 4}1/x = 1/4$.

\eganswer We apply the definition with $a=4$, $L=1/4$ and $f(x) = 1/x$.
Thus, for any $\varepsilon>0$ we try to show that if $|x-4|$ is small
enough then one has $|f(x)-1/4|<\varepsilon$.

We begin by estimating $|f(x)-\frac14|$ in terms of $|x-4|$:
\[
|f(x)-1/4| = \left|\frac1x-\frac14\right| = \left| \frac{4-x}{4x}\right| =
\frac{|x-4|}{|4x|} =\frac{1}{|4x|}\,|x-4|.
\]
As before, things would be easier if $1/|4x|$ were a constant.  To achieve
that we again agree not to take $\delta>1$.  If we always have $\delta\leq
1$, then we will always have $|x-4|<1$, and hence $3<x<5$.  How large can
$1/|4x|$ be in this situation?  Answer: the quantity $1/|4x|$ increases as
you decrease $x$, so if $3<x<5$ then it will never be larger than
$1/|4\cdot 3| = \frac1{12}$.

We see that if we never choose $\delta>1$, we will always have
\[
|f(x) - \tfrac14|\leq \tfrac1{12}|x-4| \quad\text{for}\quad |x-4|<\delta.
\]
To guarantee that $|f(x)-\frac14|<\varepsilon$ we could threfore require
\[
\tfrac1{12} |x-4|<\varepsilon, \quad\text{i.e.}\quad |x-4| <12\varepsilon.
\]
Hence if we choose $\delta=12\varepsilon$ or any smaller number, then
$|x-4|<\delta$ implies $|f(x)-4|<\varepsilon$.  Of course we have to honor
our agreement never to choose $\delta>1$, so our choice of $\delta$ is
\[
\delta = \text{the smaller of }1\text{ and }12\varepsilon = \min \bigl(1,
12\varepsilon\bigr).
\]
\end{eg}

<% end_sec('dont-choose') %>
<% begin_sec("Properties of the limit",nil,'properties-of-the-limit') %>
Epsilon-delta proofs are hard work, and by building up a more sophisticated
set of tools we can usually avoid having to apply the epsilon-delta definition directly.

<% begin_sec("Limits of constants and of $x$",nil,'limits-of-constants-and-x') %>
If $a$ and $c$ are constants,
then
\begin{equation}
  \lim_{x\to a}c=c \tag{$P_1$}
\end{equation}
and
\begin{equation}
  \lim_{x\to a} x= a.\tag{$P_2$}
\end{equation}
<% end_sec('limits-of-constants-and-x') %>

<% begin_sec("Limits of sums, products and quotients",nil,'limits-of-sum-prod-quot') %>
Let $F_1$ and $F_2$ be
two given functions whose limits for $x\to a$ we know,
\[
\lim_{x\to a}F_1(x)=L_1, \qquad \lim_{x\to a}F_2(x)=L_2.
\]
Then
\begin{align}
  \lim_{x\to a}\bigl(F_1(x)+F_2(x)\bigr)=L_1+L_2, \tag{$P_3$} \\
  \lim_{x\to a}\bigl(F_1(x)-F_2(x)\bigr)= L_1 - L_2, \tag{$P_4$} \\
  \lim_{x\to a}\bigl(F_1(x)\cdot F_2(x)\bigr)= L_1\cdot L_2 \tag{$P_5$}
\end{align}
Finally, if $ \lim_{x\to a}F_2(x)\ne0$,
\begin{equation}
  \lim_{x\to a}\frac{F_1(x)}{F_2(x)}= \frac{L_1}{L_2}.\tag{$P_6$}
\end{equation}
   
In other words the limit of the sum is the sum of the limits, etc.  One can
prove these laws using the definition of the limit, but we will not do this here.  However, I hope
these laws seem like common sense: if, for $x$ close to $a$, the quantity
$F_1(x)$ is close to $L_1$ and $F_2(x)$ is close to $L_2$, then certainly
$F_1(x)+F_2(x)$ should be close to $L_1+L_2$.

\begin{eg}{}
In this example we compute several limits, building up from simple examples to
more complicated ones.

First let's  $\lim_{x\to2}x^2$.
We have
\begin{align*}
  \lim_{x\to2} x^2 &= \lim_{x\to2} x\cdot x \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x\bigr)
  &\text{by $(P_5)$}\\
  &= 2\cdot 2 = 4.
\end{align*}
Similarly,
\begin{align*}
  \lim_{x\to2} x^3 &= \lim_{x\to2} x\cdot x^2 \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x^2\bigr)
  &\text{$(P_5)$ again}\\
  &= 2\cdot 4 = 8,
\end{align*}
and, by $(P_4)$
\[
\lim_{x\to2} x^2-1 = \lim_{x\to2} x^2 - \lim_{x\to2} 1 = 4-1 = 3,
\]
and, by $(P_4)$ again,
\[
\lim_{x\to2} x^3-1 = \lim_{x\to2} x^3 - \lim_{x\to2} 1 = 8-1 = 7,
\]
Putting all this together, we gets
\[
\lim_{x\to 2}\frac{x^3-1}{x^2-1} = \frac{2^3-1}{2^2-1} = \frac{8-1}{4-1}=
\frac{7}{3}
\]
because of $(P_6)$.  To apply $(P_6)$ we must check that the denominator
(``$L_2$'') is not zero.  Since the denominator is 3, this was all right.
\end{eg}

\begin{eg}{The limit of a square root}
\egquestion Find $\lim_{x\to 2}\sqrt x$.

\eganswer Of course, you would think that $\lim_{x\to 2}\sqrt x = \sqrt 2$ and you
can indeed prove this using $\delta$ \& $\varepsilon$ (See
problem~\ref{ex:03limitofsqrt4}.) But is there an easier way?  There is
nothing in the limit properties which tells us how to deal with a square
root, and using them we can't even prove that there is a limit.  However,
if you \emph{assume} that the limit exists then the limit properties allow
us to find this limit.

The argument goes like this: suppose that there is a number $L$ with
\[
\lim_{x\to 2} \sqrt x = L.
\]
Then property $(P_5)$ implies that
\[
L^2 = \bigl(\lim_{x\to2}\sqrt x\bigr)\cdot\bigl(\lim_{x\to2}\sqrt x\bigr) =
\lim_{x\to2} \sqrt x\cdot \sqrt x =\lim_{x\to2}x =2.
\]
In other words, $L^2=2$, and hence $L$ must be either $\sqrt 2$ or $-\sqrt
2$.  We can reject the latter because whatever $x$ does, its squareroot is
always a positive number, and hence it can never ``get close to'' a
negative number like $-\sqrt 2$.


Our conclusion: if the limit exists, then
\[
\lim_{x\to2} \sqrt x = \sqrt 2.
\]
The result is not surprising: if $x$ gets close to 2 then $\sqrt x$ gets
close to $\sqrt 2$.
\end{eg}
<% end_sec('limits-of-sum-prod-quot') %>

<% end_sec('properties-of-the-limit') %>
<% begin_sec("When limits fail to exist",nil,'when-limits-fail-to-exist') %>
In the last couple of examples we worried about the possibility that a
limit $\lim_{x\to a}g(x)$ actually might not exist.  This can actually
happen, and in this section we'll see a few examples of what failed limits
look like.  First let's agree on what we will call a ``failed limit.''

\begin{lessimportant}
If there is no number $L$ such that $\lim_{x\to a}f(x) = L$, then we say
that the limit $\lim_{x\to a}f(x)$ does not exist. 
\end{lessimportant}

<% marg(0) %>
<%
  fig(
    'sign-function',
    %q{The sign function.}
  )  
%>
<% end_marg %>

\begin{eg}{The sign function near $x=0$}\label{eg:limit-of-sign-function}
The ``sign function'' is defined by
\[
\operatorname{sign} (x) =
\begin{cases}
  -1 & \text{for $x<0$}\\ 0 & \text{for $x=0$}\\ 1 & \text{for $x>0$}
\end{cases}
\]
Note that ``the sign of zero'' is defined to be zero.  But does the sign
function have a limit at $x=0$, i.e.\ does $ \lim_{x\to0} \operatorname{sign}(x) $ exist?
And is it also zero? The answers are \emph{no} and \emph{no}, and here is
why: suppose that for some number $L$ one had
\[
  \lim_{x\to 0}\operatorname{sign}(x) = L,
\]
then since for arbitrary small positive values of $x$ one has $\operatorname{sign}(x) =
+1$ one would think that $L=+1$.  But for arbitrarily small negative values
of $x$ one has $\operatorname{sign}(x)=-1$, so one would conclude that $L=-1$.  But one
number $L$ can't be both $+1$ and $-1$ at the same time, so there is no
such $L$, i.e.\ there is no limit.
\[
\lim_{x\to 0} \operatorname{sign} (x) \text{ does not exist.}
\]

In examples like this one, it is possible to define a one-sided limit; see
section \ref{subsec:one-sided-limit}.
\end{eg}

%%graph%% backward-sine-raw func=sin(3.1416/x) format=svg xlo=0.02 xhi=3 ylo=-1 yhi=1 with=lines samples=1000

<%
  fig(
    'backward-sine',
    %q{Example \ref{eg:backward-sine}.},
    {
      'width'=>'fullpage'
    }
  )
%>

\begin{eg}{The ``backward sine''}\label{eg:backward-sine}
Figure \figref{backward-sine} shows the  ``backward sine'' function
$f(x)=\sin(\pi/x)$.
Contemplate its limit as $x\to0$:
\[
\lim_{x\to 0}\sin \bigl(\frac\pi x\bigr) \qquad .
\]

When $x=0$ the function $f(x)$ is not defined, because its
definition involves division by $x$.  What happens to $f(x)$ as $x\to0$?
First, $\pi /x$ becomes larger and larger (``goes to infinity'') as
$x\to0$.  Then, taking the sine, we see that $ \sin(\pi /x)$ oscillates
between $+1$ and $-1$ infinitely often as $x\to0$.  This means that $f(x)$
gets close to any number between $-1$ and $+1$ as $x\to0$, but that the
function $f(x)$ \emph{never stays close} to any particular value because it
keeps oscillating up and down. The limit fails to exist, but for a different
reason than in example \ref{eg:limit-of-sign-function}.
\end{eg}

\begin{eg}{Trying to divide by zero using a limit}\label{eg:limit-to-divide-by-zero}
The expression $1/0$
is not defined, but what about
\[
\lim_{x\to0}\frac1x?
\]
This limit also does not exist.  Here are two reasons:

It is common wisdom that if you divide by a small number you get a large
number, so as $x\searrow 0$ the quotient $1/x$ will not be able to stay
close to any particular finite number, and the limit can't exist.

``Common wisdom'' is not always a reliable tool in mathematical proofs, so
here is a better argument.  The limit can't exist, because that would
contradict the limit properties $(P_1)\cdots(P_6)$.  Namely, suppose that
there were an number $L$ such that
\[
\lim_{x\to0} \frac 1 x = L.
\]
Then the limit property $(P_5)$ would imply that
\[
\lim_{x\to0}\bigl(\frac 1x\cdot x \bigr) = \bigl(\lim_{x\to0}\frac
1x\bigr)\cdot \bigl(\lim_{x\to0} x\bigr) = L\cdot 0 =0.
\]
On the other hand $\frac 1x \cdot x =1$ so the above limit should be 1!  A                                       
number can't be both 0 and 1 at the same time, so we have a
contradiction. The assumption that $\lim_{x\to0}1/x$ exists is to blame, so
it must go.
\end{eg}

<% begin_sec("Using limit properties to show a limit does \\emph{not} exist",nil,'props-to-show-no-limit') %>
The limit properties tell us how to prove that certain limits exist (and
how to compute them).  Although it is perhaps not so obvious at first
sight, they also allow you to prove that certain limits do not exist. 
Example \ref{eg:limit-to-divide-by-zero} shows one instance of such use.  Here is another.

Property $(P_3)$ says that if both $\lim_{x\to a}g(x)$ and $\lim_{x\to a}
h(x) $ exist then $\lim_{x\to a}g(x)+h(x)$ also must exist.  You can turn
this around and say that if $\lim_{x\to a}g(x)+h(x)$ does not exist then
either $\lim_{x\to a}g(x)$ or $\lim_{x\to a }h(x)$ does not exist (or both
limits fail to exist).

For instance, the limit
\[
\lim_{x\to 0} \frac 1x-x
\]
can't exist, for if it did, then the limit
\[
\lim_{x\to0} \frac 1x =\lim_{x\to 0} \bigl(\frac1x-x +x\bigr) =\lim_{x\to
0} \bigl(\frac1x-x\bigr) + \lim_{x\to 0} x
\]
would also have to exist, and we know $\lim_{x\to 0}\frac1x$ doesn't exist.

<% end_sec('props-to-show-no-limit') %>
<% end_sec('when-limits-fail-to-exist') %>
<% end_sec('computing-limits') %>


<% begin_sec("Variations on the theme of the limit",nil,'limit-variations') %>

Not all limits are ``for $x\to a$''.  Here we describe some variations on the
concept of limit.

<% begin_sec("Left and right limits",nil,'one-sided-limits') %>
When we let ``$x$ approach $a$'' we allow $x$ to be larger or smaller 
than $a$, as long as $x$ ``gets close to $a$''.  If we explicitly want to study
the behavior of $f(x)$ as $x$ approaches $a$ through values larger than
$a$, then we write
\[
\lim_{x\searrow a} f(x)\text{ or } \lim_{x\to a+} f(x) \text{ or }
\lim_{x\to a+0} f(x) \text{ or } \lim_{x\to a, x>a} f(x).
\]
All four notations are commonly used.  Similarly, to designate the value which  
$f(x)$ approaches as $x$ approaches $a$ through values below $a$ one writes
\[
\lim_{x\nearrow a} f(x)\text{ or } \lim_{x\to a-} f(x) \text{ or }
\lim_{x\to a-0} f(x) \text{ or } \lim_{x\to a, x<a} f(x).
\]
The precise definition of these ``one-sided'' limits goes like this:

\begin{important}[Definition of right- and left-limits]
Let $f$ be a function.  Then the right-limit notation
\begin{equation}\label{eq:one-sided-lim-formulation}
  \lim_{x\searrow a} f(x) = L \qquad .
\end{equation}
means that for every $\varepsilon>0$ one can find a $\delta>0$ such that
\[
a<x<a+\delta \implies |f(x)-L|<\varepsilon
\]
holds for all $x$ in the domain of $f$.

The definition of a left-limit is exactly analogous.
When we say
\begin{equation}\label{eq:left-sided-lim-formulation}
  \lim_{x\nearrow a} f(x) = L \qquad ,
\end{equation}
we mean that for every $\varepsilon>0$ one can find a $\delta>0$ such that
\[
a-\delta<x<a \implies |f(x)-L|<\varepsilon
\]
holds for all $x$ in the domain of $f$.
\end{important}

The following
theorem tells you how to use one-sided limits to decide if a function
$f(x)$ has a limit at $x=a$.

\begin{theorem}
The two-sided limit $\displaystyle \lim_{x\to a} f(x)$
exists if and only if the two one-sided limits
\[
  \lim_{x\searrow a} f(x), \quad\text{and}\quad \lim_{x\nearrow a} f(x)
\]
exist and have the same value.
\end{theorem}

<% end_sec('one-sided-limits') %>

<% begin_sec("Limits at infinity",nil,'limits-at-infinity') %>
So far we have defined the limit of a function $f(x)$ as $x$ gets closer and closer to some
finite value. It can also be of interest to let $x$ become
``larger and larger'' and ask what happens to $f(x)$.  If there is a number
$L$ such that $f(x)$ gets arbitrarily close to $L$ if one chooses $x$
sufficiently large, then we write   
\[
\lim_{x\to \infty} f(x) = L
\]
(``The limit for $x$ going to infinity is $L$.'')
We have an analogous definition for what happens to $f(x)$ as $x$ becomes very
large and negative: we write
\[
\lim_{x\to -\infty} f(x) = L  
\]
(``The limit for $x$ going to negative infinity is $L$.'')

Here are the precise definitions:

\begin{important}[Definitions of limits at infinity]
Let $f(x)$ be a function which is defined on an interval $x_0<x<\infty$.
If there is a number $L$ such that for every $\varepsilon>0$ we can find
an $A$ such that
\[
x>A \implies |f(x) - L| <\varepsilon
\]
for all $x$, then we say that the limit of $f(x)$ for $x\to\infty$ is $L$.

Simmilarly, let $f(x)$ be a function which is defined on an interval $-\infty < x < x_0$.
If there is a number $L$ such that for every $\varepsilon>0$ we can find
an $A$ such that
\[
x<-A \implies |f(x) - L| <\varepsilon
\]
for all $x$, then we say that the limit of $f(x)$ for $x\to-\infty$ is $L$.
\end{important}

These definitions are very similar to the original definition of the limit
in section \ref{sec:limit} on p.~\pageref{sec:limit}.
Instead of $\delta$ which specifies how close $x$ should be to $a$, we now
have a number $A$ that says how large $x$ should be, which is a way of
saying ``how close $x$ should be to infinity'' (or to negative infinity).

But although these definitions are similar to the original one, they are not
quite the same.
Note that there is no real number called $\infty$, and therefore we can't just
take the definition of $\lim_{x\rightarrow a}$ and substitute
$\infty$ for $a$. (Cf.~rule 2 on p.~\pageref{infinity-is-not-a-hyperreal}.)

%%graph%% dying-vibration-raw func=cos(3.14*x)*(0.7)**(x/6) format=svg xlo=0.0 xhi=18 ylo=-1 yhi=1 with=lines samples=1000

<%
  fig(
    'dying-vibration',
    %q{The value of $A$ is large enough for the given $\varepsilon$. The graph could represent the
       dying vibration of a gong as a function of time. Because we can find such an $A$ for every $\varepsilon$, the
       vibration dies out to zero as time approaches infinity.},
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

\begin{eg}{The limit of $1/x$}
The larger $x$ is, the smaller its reciprocal, so it seems natural that $1/x
\to 0$ as $x\to \infty$.  To \emph{prove} that $\lim_{x\to\infty}1/x = 0$, we
apply the definition to $f(x) = 1/x$, $L=0$.

For a given $\varepsilon>0$, we need to show that
\begin{equation}\label{eq:1overx-small-for-x-large}
  \left|\frac1x - L\right|<\varepsilon \text{ for all } x>A
\end{equation}
provided we choose the right $A$.
  
How do we choose $A$?  $A$ is not allowed to depend on $x$, but it may
depend on $\varepsilon$.

Let's decide that we will always take $A>0$, so that we only need consider
positive values of $x$.  Then \eqref{eq:1overx-small-for-x-large} simplifies to
\[
\frac 1x<\varepsilon
\]
which is equivalent to
\[
x>\frac1\varepsilon.
\]
This tells us how to choose $A$.  Given any positive $\varepsilon$, we will
simply choose
\[
  A= \text{ the larger of } 0 \text{ and } \frac1\varepsilon
\]
Then we have $|\frac1x-0| = \frac1x <\varepsilon$ for all $x>A$, so we
have proved that $\lim_{x\to\infty}1/x=0$.
\end{eg}

The properties of the limit given in section \ref{subsec:properties-of-the-limit},
p.~\pageref{subsec:properties-of-the-limit}, also apply to limits at infinity.
As with limits at finite $x$, it is usually more convenient to calculate limits
by using these properties than by direct application of the definition.

\begin{eg}{A rational function}\label{eg:rational-function-at-infinity}
A rational function is the quotient of two polynomials:
\begin{equation}
  R(x) = \frac{a_nx^n+\cdots +a_1x+a_0}{b_mx^m+\cdots+b_1x+b_0}.
\end{equation}
The following trick allows us to evaluate the limit of any such function at infinity.

For example, let's compute
\[
\lim_{x\to\infty}\frac{3x^2+3}{5x^2+7x-39}.
\]
The trick is to factor $x^2$ from top and bottom. You get
\begin{align*}
  \lim_{x\to\infty}\frac{3x^2+3}{5x^2+7x-39}
  &= \lim_{x\to\infty}\frac{x^2}{x^2} \; \frac{3+3/x^2}{5+7/x-39/x^2}
  & \text{(algebra)}\\
  &= \lim_{x\to\infty}\frac{3+3/x^2}{5+7/x-39/x^2}& \text{(more algebra)}\\
  &= \frac{\lim_{x\to\infty}(3+3/x^2)}{\lim_{x\to\infty}(5+7/x-39/x^2)}
  &\text{(limit properties)}\\
  &= \frac35.
\end{align*}
At the end of this computation, we used the limit properties
$(P_\ast)$ to break the limit down into simpler pieces like
$\lim_{x\to\infty}39/x^2$, which we can directly evaluate; for example, we have
\[
\lim_{x\to\infty} 39/x^2 =\lim_{x\to\infty} 39\cdot \left( \frac1x
\right)^2 = \left( \lim_{x\to\infty}39\right)\cdot \left(
  \lim_{x\to\infty}\frac1x \right)^2 = 39 \cdot 0^2 =0.
\]
  The other terms are similar.

\end{eg}

\begin{eg}{Another rational function}
Compute
\[
\lim_{x\to\infty} \frac {2x}{4x^3+5}.
\]
We apply the same trick as in example \ref{eg:rational-function-at-infinity}
and factor $x$ out of the numerator and $x^3$ out of the denominator.
This leads to
\begin{align*}
  \lim_{x\to\infty} \frac {2x}{4x^3+5}
  &=\lim_{x\to\infty} \Bigl(\frac{x}{x^3}\;\frac{2}{4+5/x^3}\Bigr)\\
  &=\lim_{x\to\infty} \Bigl(\frac{1}{x^2}\;\frac{2}{4+5/x^3}\Bigr)\\
  &=\lim_{x\to\infty} \Bigl(\frac{1}{x^2}\Bigr)\cdot \Bigl(\lim_{x\to\infty}\frac{2}{4+5/x^3}\Bigr)\\
  &=0\cdot \tfrac24\\
  &=0.
\end{align*}

\end{eg}

<% end_sec('limits-at-infinity') %>

<% begin_sec("Limits that equal infinity",nil,'limit-equals-infinity') %>
Figure \figref{telephone-wire} shows a telephone wire strung between two poles, which sags
by some amount $h$ in the middle. By increasing the tension $T$ in the wire, we can reduce
the sag. That is, the necessary tension $T$ is some function $T(h)$. There is a story, almost certainly
apocryphal, to the effect that a small-town mayor considered the sagging wires unsightly,
and instructed the public works department to tighten them up enough so that they wouldn't
sag at all.
<% marg(30) %>
<%
  fig(
    'telephone-wire',
    %q{A telephone wire sags by an amount $h$.}
  )  
%>
<% end_marg %>

It can be shown that the function $T(h)$ is approximately given by the equation
\begin{equation*}
  T = \frac{k}{h} \qquad ,
\end{equation*}
where $k$ is a constant.\footnote{The value of $k$ is $WL/8$, where $W$ is the weight of the
wire and $L$ is the horizontal length. The approximation is good if $h$ is small compared to $L$.}
When I ask students what happens to this equation when we plug in $h=0$, I always get
a chorus of ``undefined!'' This shows good mathematical training --- division by zero is
indeed undefined --- but doesn't give any real insight into what will go wrong when the
workers try to carry out the mayor's plan. If we make $h$ smaller and smaller $T$ will
get bigger and bigger. By making $h$ sufficiently small, we can make $T$ arbitrarily large.
The important insight here is that a quantity like $1/0$ isn't just undefined, it's undefined
because it's infinity, and infinity isn't a real number. If the workers actually
try to make $h=0$, they will simply have to tighten the wires so much that the wires break.

Another way of putting this is that the limit $\lim_{h\rightarrow0} T(h)$ fails to exist.
Although it's true that the limit doesn't exist, we can be more descriptive about the
reason that it doesn't. It's a limit that doesn't exist because it equals infinity.

Consider the limit
\[
\lim_{x\to 0} \frac{1} {x}.
\]
As $x$ decreases to $x=0$ through smaller and smaller positive values,
its reciprocal $1/x$ becomes larger and larger.  We say that instead
of going to some finite number, the quantity $1/x$ ``goes to
infinity'' as $x\searrow0$.  In symbols:
\begin{equation}
  \lim_{x\searrow 0} \frac{1} {x} = \infty.
  \label{eq:03limit-invx-is-infty}
\end{equation}%
%\marginpar{\input ../figures/221/03inverse-of-x.tex }%
Likewise, as $x$ approaches $0$ through negative numbers, its
reciprocal $1/x$ drops lower and lower, and we say that $1/x$ ``goes
to $-\infty$'' as $x\nearrow 0$.  Symbolically,
\begin{equation}
  \label{eq:03limit-invx-is-neg-infty}
    \lim_{x\nearrow 0} \frac{1} {x} = -\infty.
\end{equation}
The limits \eqref{eq:03limit-invx-is-infty} and
\eqref{eq:03limit-invx-is-neg-infty} are not like the normal limits we
have been dealing with so far.  Namely, when we write something like
\[
\lim_{x\to2} x^2 = 4
\]
we mean that the limit actually exists and that it is equal to $4$.
On the other hand, since we have agreed that $\infty$
is not a number (see p.~\pageref{infinity-is-not-a-hyperreal}),
the meaning of \eqref{eq:03limit-invx-is-infty} cannot be to say that
``the limit exists and its value is $\infty$.''

%%graph%% limit-of-one-over-x-raw func=1/x format=svg xlo=0.1 xhi=10 ylo=0 yhi=10 with=lines samples=300
<% marg(0) %>
<%
  fig(
    'limit-of-one-over-x',
    %q{The function $1/x$ behaves badly near $x=0$.}
  )  
%>
<% end_marg %>

Instead, when we write
\begin{equation}
  \lim_{x\to a} f(x) = \infty
  \label{eq:03limit-of-f-infty}
\end{equation}
for some function $y=f(x)$, we mean, \emph{by definition,} that the
limit of $f(x)$ does not exist, and that it fails to exist in a
specific way: as $x\to a$, the value of $f(x)$ becomes ``larger and
larger,'' and in fact eventually becomes larger than any finite
number.

The language in that last paragraph shows you that this is an
intuitive definition, at the same level as the first definition of
limit we gave in section \ref{subsec:limit-informal}, p.~\pageref{subsec:limit-informal}.  It contains the   
usual suspect phrases such as ``larger and larger,'' or ``finite
number'' (as if there were any other kind.)  A more precise definition
involving epsilons can be given, but in this course we will not go
into this much detail.  As a final comment on infinite limits, it is
important to realize that, since \eqref{eq:03limit-of-f-infty} is not
a normal limit \emph{you cannot apply the limit rules to infinite
  limits. }  Here is an example of what goes wrong if you try anyway.

\begin{eg}{Trouble with infinite limits}
If you apply the limit properties to $\lim_{x\searrow0} 1/x = \infty$,
then you could conclude
\[
1 = \lim_{x\searrow 0} x\cdot \frac{1} {x}
 = \lim_{x\searrow 0} x \times \lim_{x\searrow 0}  \frac{1} {x}
 = 0 \times \infty
 = 0,
\]
because ``anything multiplied with zero is zero.''

After using the limit properties in combination with this infinite
limit we reach the absurd conclusion that $1=0$.  The moral of this
story is that you can't use the limit properties when some of the
limits are infinite.
\end{eg}


<% end_sec('limit-equals-infinity') %>

<% end_sec('limit-variations') %>

<% begin_sec("Curve sketching",nil,'curve-sketching') %>
<% begin_sec("Sketching a graph without knowing its equation",nil,'sketch-without-equation') %>
The concepts of calculus, such as derivatives, limits, curvature, and concavity, can
guide us in analyzing the behavior of a function even when we don't know a formula for
the function. In economics, for example, these concepts are used heavily even though
real-world data can essentially never be described by a formula. This subsection
presents four examples in which we can use these concepts to sketch a function based on
our understanding of how the function should behave in real life.

<% begin_sec("The time to pay off a loan",nil,'loan-period') %>
Most people will end up borrowing money at some point in their lives, whether it's
credit card debt, a mortage, a loan to buy a car, or a cash advance from a payday loan company.
One of the warning signs that you may be walking into an exploitative situation is if the
person trying to sell you the loan emphasizes the low monthly payment. Suppose that
you're borrowing \$10,000 to buy a car, and the monthly interest rate is 1\%.
Let $p$ be the monthly payment, and $T$ the time required in order to pay off
the loan. To understand what's going on here, you want to be able to
\emph{visualize} the graph of $T$ as a function of $p$. One fairly tedious way to do this would
be to find the equation of the function,
take a piece of graph paper and plot points. Another method would be to use
an expensive graphing calculator. But your knowledge of calculus gives you a method
that provides more insight with less work.

Clearly the smaller the payment, the longer it will take to pay off the loan.
This tells us that $T(p)$ is a \emph{decreasing} function; its derivative will always
be negative.

If $p$ is large, then you will pay off the loan so quickly that no significant amount
of interest accrues. Therefore at large values of $p$, we will have $T\approx(\$10,000)/p$.
This tells us that $\lim_{p\rightarrow\infty} T=0$. The graph of $T$ will approach the
horizontal axis more and more closely as $p$ gets bigger and bigger. We say that
the function $T(p)$ has a \emph{horizontal asymptote} at zero.
<% marg(0) %>
<%
  fig(
    'loan-period',
    %q{The time required to pay off a loan, as a function of the monthly payment.}
  )  
%>
<% end_marg %>

Finally, what happens if $p$ is small? Remember, interest on the loan is accruing
at a rate of 1\% monthly, or \$100 every month. It may sound like a good deal if
you're offered this loan with a low monthly payment of \$101, but if you take the
loan and always make the minimum payment, then the principal on the loan will only
go down by \$1 every month. You will die of old age before you pay off the car.
We can therefore tell that $\lim_{p\searrow\$100} T=\infty$. This is a
\emph{vertical asymptote} on the graph.
<% end_sec('loan-period') %>

Figure \figref{loan-period} shows what the graph must look like.

<% begin_sec("The Laffer curve",nil,'laffer-curve') %>
This example, a famous one, also has to do with money. In 1974, economist Arthur Laffer
presented the following argument about taxes to politicians Dick Cheney and Donald Rumsfeld,
sketching the resulting graph on a paper napkin. Consider the government's
tax revenue as a function of the tax rate. Clearly if the tax rate is zero, the government
gets zero revenue. Most people would assume that the function was a purely increasing one,
since raising the tax rate would always garner the government more money.

But, Laffer
said, that isn't so. Imagine that the tax rate was 100\%, so that the government confiscated
all of everyone's earnings. Nobody would have any incentive to work, so they would stop
working, they would earn no taxable income, and revenue would drop to zero.
Laffer sketched a graph like figure \figref{laffer-curve} on a paper napkin for
Cheney and Rumsfeld. There should be some intermediate tax rate, he told them,
that would produce the maximum revenue. Later, when Ronald Reagan became president,
he cut taxes on the theory that the US was already on the right-hand side of the ``Laffer curve,''
so that, counterintuitively, the lower taxes would produce \emph{higher} revenue.
The results were not as Laffer had promised; the average annual budget deficit during the Reagan
administration was \$240 billion, compared to \$57 billion during the preceding Carter administration.
<% marg(0) %>
<%
  fig(
    'laffer-curve',
    %q{The Laffer curve.}
  )  
%>
<% end_marg %>

In calculus terms, our analysis of this function is an example of a result called Rolle's
theorem. The idea is that if the function is smooth, then we expect its derivative to be
continuous. If the derivative is positive on the left and negative on the right, then it
must be zero at some intermediate point. This would be the point at which the function was
maximized.
<% end_sec('laffer-curve') %>

<% begin_sec("Skydiving",nil,'skydiving') %>
Figure \figref{skydiving} shows a skydiver's altitude as a function of time.
Early in the motion, soon after the person jumps out of the plane, the only significant
force is gravity, and the person falls with constant acceleration (section \ref{subsec:velocity},
p.~\pageref{subsec:velocity}). The drop relative to the initial position
equals $(1/2)at^2$, which is the equation of a parabola. 
<% marg(50) %>
<%
  fig(
    'skydiving',
    %q{Altitude as a function of time for a skydiver.}
  )  
%>
<% end_marg %>

<% marg(-100) %>
<%
  fig(
    'anchor',
    %q{A rock-climbing anchor.}
  )  
%>
<% end_marg %>

But as the downward (negative) velocity increases,
the upward force of air friction gets stronger and stronger. In the opposite
limit of $t\rightarrow\infty$, the force of air friction gets closer and closer to
being strong enough to cancel the force of gravity. In this limit, Newton's
second law (section \ref{subsec:newton-second-law}, p.~\pageref{subsec:newton-second-law})
predicts an acceleration of zero. An acceleration of zero corresponds to constant
velocity, so that the graph asymptotically approaches a line whose slope is the velocity.

This graph demonstrates two mathematical properties. It has a \emph{y-intercept},
which is the initial altitude. It also has an oblique asymptote, i.e., an asymptotic line
that is neither horizontal nor vertical.
<% end_sec('skydiving') %>

<% begin_sec("A rock-climbing anchor",nil,'anchor') %>
For safety, rock climbers and mountaineers often
wear a climbing harness and tie in to other climbers on a rope
team or to anchors such as pitons or snow anchors. When using anchors, the climber usually wants to be
protected by
more than one, both for extra strength and for redundancy in case one fails.
Figure \figref{anchor} shows such an arrangement, with the climber hanging from a pair of anchors forming a ``Y''
at an angle $\theta$.  The usual advice is to make $\theta<90\degunit$; for large values of $\theta$,
the stress placed on the anchors can be many times greater than the actual load $L$,
so that two anchors are actually \emph{less} safe than one.

Consider the stress on the anchor $S$ as a function of $\theta$. For physical
reasons similar to those discussed in the example of the telephone wire
(section \ref{subsec:limit-equals-infinity}, p.~\pageref{subsec:limit-equals-infinity}),
$S$ must approach infinity as $\theta$ approaches 180 degrees; no matter how tight
the anchor strands are made, the carabiner (hook) at the center will never be pulled up
quite as high as the anchors.

At $\theta=0$, we can see that each anchor strand will support half the load. The
\emph{y-intercept} of the graph equals $L/2$.

We can gain further insight by extending the range of possible values for $\theta$ to
include negative angles. Physically, this corresponds to bringing the anchor strands
past one another and swapping the roles of the two anchors. Since the physical setup is
symmetrical, the function $S(\theta)$ must have the property $S(\theta)=S(-\theta)$,
i.e., it is an \emph{even} function. It might seem pointless to discuss this symmetry,
but it tells us something important. An argument identical to the one in
section \ref{subsec:derivative-of-x-squared}, p.~\pageref{subsec:derivative-of-x-squared}, tells
us that based on this symmetry, the derivative $S'$ must equal zero at $\theta=0$. This means
that for small values of $\theta$, the strain on the anchor will be very nearly the same as
for $\theta=0$, i.e., hardly any greater than half the load. Thus any small value of $\theta$
is about equally good, but very large values could be a deadly mistake.
<% end_sec('anchor') %>


<% end_sec('sketch-without-equation') %>
<% end_sec('curve-sketching') %>

<% end_chapter %>
