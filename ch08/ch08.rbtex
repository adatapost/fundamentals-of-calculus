%%chapter%% 08
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '08',
    %q{The integral},
    'ch:integral'
  )
%>

<% begin_sec("The accumulation of change",nil,'accumulation') %>
  <% begin_sec("Change that accumulates in discrete steps",nil,'discrete-sums') %>
    <% begin_sec("A schoolboy plays a trick",nil,'schoolboy-trick') %>
Toward the end of the eighteenth century, a German elementary school teacher
decided to keep his pupils busy by assigning them a long, boring arithmetic
problem: to add up all the numbers
from one to a hundred.\footnote{I'm giving my own retelling of a hoary legend. We don't really know the exact problem, just that it was
supposed to have been something of this flavor.}
% http://www.americanscientist.org/issues/pub/gausss-day-of-reckoning#
 The children set to work on their slates, and the teacher
lit his pipe, confident of a long break. But almost immediately, a boy named
Carl Friedrich Gauss brought up his answer: 5,050.\label{gauss-story}\index{Gauss, Carl Friedrich}

<% marg(-17) %>
<%
  fig(
    'gauss-square',
    %q{Adding the numbers from 1 to 7.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'gauss-solution',
    %q{A trick for finding the sum.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'gauss',
    %q{Carl Friedrich Gauss (1777-1855), a long time after graduating from elementary school.}
  )  
%>
<% end_marg %>

Figure \figref{gauss-square} suggests one way of solving this type of problem.
The filled-in columns of the graph represent the numbers from 1 to 7, and
adding them up means finding the area of the shaded region. Roughly half the
square is shaded in, so if we want only an approximate solution, we can
simply calculate $7^2/2=24.5$.

But, as suggested in figure \figref{gauss-solution}, it's not much more work to
get an exact result. There are seven sawteeth sticking out out above the diagonal,
with a total area of $7/2$, so the total shaded area is $(7^2+7)/2=28$.
In general, the sum of the first $n$ numbers will be $(n^2+n)/2$, which explains
Gauss's result: $(100^2+100)/2=5,050$.

There is a tantalizing hint here of a link with differential calculus, because the
derivative of a real function $f(n)=(n^2+n)/2$ is almost, but not quite, equal to $n$.
    <% end_sec('schoolboy-trick') %>

    <% begin_sec("Accumulation of change in discrete steps",nil,'discrete-accumulation') %>

Problems like this come up frequently. Imagine that each household in a certain
small town sends a total of one ton of garbage to the dump every year. Over
time, the garbage accumulates in the dump, taking up more and more space.
If the population is constant, then
garbage accumulates at a constant rate.
But maybe the town's population is growing. If the population starts out
as 1 household in year 1, and then grows to 2 in year 2, and so on, then
we have the same kind of problem that the young Gauss solved. After 100
years, the accumulated amount of garbage will be 5,050 tons. The pile of
refuse grows more quickly every year.
    <% end_sec('discrete-accumulation') %>

    <% begin_sec("Sigma notation",nil,'sigma-notation') %>
There is a convenient way of notating sums like the ones we've been doing,
which involves $\Sigma$, called ``sigma,'' the capital Greek letter ``S.''
Here the ``S'' stands for ``sum.'' The sigma notation looks like this:
\begin{equation}\label{eqn:sigma-notation}
  \sum_{i=1}^{100} \; i = 5,050
\end{equation}
This is read as ``the sum of $i$ for $i$ from 1 to 100 equals 5,050.''
The version without the sigma notation is much more cumbersome to write:
\begin{equation}\label{eqn:no-sigma-notation}
  1+2+3+\ldots+100 = 5,050
\end{equation}
In equation \eqref{eqn:sigma-notation}, $i$ is a dummy variable. We could have written
\begin{equation*}
  \sum_{j=1}^{100} \; j = 5,050
\end{equation*}
and it would have meant exactly the same thing.
We've already seen some examples of dummy variables. In set notation (box \figref{sets}, p.~\pageref{fig:sets}),
\begin{equation*}
 \zu{S}=\{x|x^2>0\} \quad \text{and} \quad \zu{T}=\{y|y^2>0\}
\end{equation*}
describe exactly the same set, and S=T. Similarly, the function $f$ defined by $f(u)=u^2$ and the function
$g$ defined by $g(v)=v^2$ are the same function, $f=g$.
    <% end_sec('sigma-notation') %>
  <% end_sec('discrete-sums') %>

  <% begin_sec("The area under a graph",nil,'area-under-a-graph') %>
    <% begin_sec("The geometrical problem",nil,'area-under-a-graph-geom') %>
The examples in section \ref{subsec:discrete-sums} involved change that occurred in discrete
steps. Calculus is concerned with \emph{continuous} change. The continuous analog of
a discrete sum is the area under a graph.
Let $f$ be a function that is defined on an interval\footnote{For
interval notation, see p.~\pageref{interval-notation}.} $[a,b]$
and assume the value of $f$ is always positive (so that its graph lies above
the $x$ axis). \emph{How large is the area of the region caught
  between the $x$ axis, the graph of $y=f(x)$ and the vertical lines
  $y=a$ and $y=b$?}
    <% end_sec('area-under-a-graph-geom') %>
  <% end_sec('area-under-a-graph') %>
<% marg(0) %>
<%
  fig(
    'riemann-portrait',
    %q{Bernhard Riemann (1826-1866).}
  )  
%>
<% end_marg %>

<%
  fig(
    'area-under-graph',
    %q{1.~The area under the graph of the function $f$. 2.~Approximating this area using 20 thin rectangles.},
    {
      'width'=>'wide',
      'sidecaption'=>false
    }
  )
%>

  <% begin_sec("Approximation using a Riemann sum",nil,'riemann-sum') %>

We can try to compute this area, figure \figref{area-under-graph}{1}, by approximating
the region with many thin rectangles, \figref{area-under-graph}{2}.  The idea is that
even though we don't know how to compute the area of a region bounded
by arbitrary curves, we do know how to find the area of one or more
rectangles. In this example, we've subdivided the interval from $a$ to $b$
into $n=20$ equal subintervals, each of width $\Delta x=(b-a)/n$.
Let's write $x_1$ for the $x$ value that lies in the center of the first
subinterval, etc.
We've chosen the height of each rectangle so that its top intersects the
graph at this midpoint, so that, e.g., the height of the first rectangle
is $f(x_1)$.
The area of the $k^{\textrm{th}}$ rectangle is the product of its
height and width, which is $f(x_k)\Delta x$.  Adding up all the rectangles'
areas yields
\begin{equation}\label{eqn:riemann-sum}
  R = \sum_{k=1}^n f(x_k)\Delta x \qquad .
\end{equation}
This is an example of what is called a \emph{Riemann sum}, meaning an approximation
to the area under a curve using rectangles. This particular type of Riemann sum
is one in which (a) the interval is subdivided into equal parts, and (b) the value
of the function is sampled at the center of each subinterval.

If $f$ is negative in certain places, then
we will hit certain values of $k$ for which the product $f(x_k)\Delta x$ is negative.
We will simply \emph{define} areas below the $x$ axis to be negative. We think of
the rectangle as having positive width $\Delta x$ but negative height $f(x_k)$. A similar
geometrical example is the use of negative numbers for angles that are directed contrary
to a standard direction of rotation.

If our rectangles are all sufficiently narrow then we expect the total
area of all the rectangles to be a good approximation of the area of the region
under the graph.
  <% end_sec('riemann-sum') %>
<% end_sec('accumulation') %>

<% begin_sec("The definite integral",nil,'definite-integral') %>
  <% begin_sec("Definition of the integral",nil,'integral') %>
This leads to the following definition.

\begin{important}[Definition of the integral]
  If $f$ is a function defined on an interval $[a, b]$, then
  the integral of $f(x)$ from $x=a$ to $b$ is defined as
  \begin{equation*}
    \lim_{\Delta x\rightarrow 0} R \qquad ,
  \end{equation*}
  where $R$ is the type of Riemann sum defined above, using equal subintervals
  sampled at their centers.
\end{important}


<%
  fig(
    'visualize-riemann-integral',
    %q{Three Riemann sums for the same function on the same interval. The total approaches
       the Riemann integral.},
    {
      'width'=>'wide',
      'sidecaption'=>false
    }
  )
%>

Up until now we've been doing differential calculus. The other half of calculus,
integral calculus, consists of the study of integrals. The type of integral defined
here is called a definite integral. We'll see later that there is another type, called
the indefinite integral.

In general, a Riemann sum need
not have equal subdivisions (the rectangles could be of unequal widths
$\Delta x_k$), and the
points $x_k$ at which we sample the function need not be at the center of each
interval.
For essentially all functions encountered in practical applications, this makes
no difference. As long as we force all the widths to get smaller, and sample
the function somewhere within each interval, we get the same answer. 
We are free, for example, to sample the
function at the left or right side of each subinterval rather than at the center.
The only case where this choice would matter would be for a function that was
spectacularly badly behaved: one that was discontinuous at infinitely
many points within the interval $[a,b]$. This is addressed
in box \figref{riemann-integral}.

<% marginbox(500,'riemann-integral','The Riemann integral',{},
%q~
Let $f$ be the real function defined such that $f(x)=0$ if $x$ is rational, and
$f(x)=1$ otherwise. Such a function can never, for example, be a result of any
physical process or any measurement, since measurements can't distinguish rational
numbers from irrational ones. If we integrate this function
from $0$ to $1$, 
using Riemann sums with equal subintervals, sampled at their centers, we get $0$
because all the $x_k$'s are rational. But this is in some sense the wrong answer,
since ``most'' real numbers are irrational, so $f$ is ``usually'' equal to 1, not 0.

For these reasons, Riemann defined an integral, now known as the Riemann integral,
as follows. Suppose we have a number $I$ such that
for every $\varepsilon >0$, there exists a $\delta>0$ such that
$|R -I|<\varepsilon$ for \emph{every} Riemann sum all of whose intervals have width $\Delta
  x_k<\delta$ with any arbitrary choice of sample points $x_1$, \dots , $x_n$.
Then $I$ is the Riemann integral of the function. For the function $f$ defined
above, the Riemann integral is undefined, as it should be.
      ~
   )
%>

\begin{eg}{A triangle}\label{eg:riemann-triangle-area}
Let $f(x)=x$. Then the integral of $f$ from $0$ to $1$ represents the area of a triangle
with height 1 and a base of width 1. We know from elementary geometry that this shape has
an area of $\frac{1}{2}(\text{base})(\text{height})=\frac{1}{2}$, so we don't need
integral calculus to determine it. But let's see how this works
out if we do it as an integral, in order to get comfortable with the tool and try it out
in a case where we know the answer.

I claimed above that for well-behaved functions, it didn't matter whether we sampled
the function at the center of each subinterval, on the left, or on the right. Let's
take advantage of this to make the calculation more convenient. If we divide the interval
$[0,1]$ into $n$ subintervals, and sample the function at the right side of each of these,
then we get a staircase that looks exactly like figure \figref{gauss-square} on
p.~\pageref{fig:gauss-square}. The only difference between the 
``staircase'' problem and our Riemann sum is that in the former, the graph covered a
region of graph paper $n$ squares wide and $n$ squares tall, whereas the graph of our
Riemann sum is scaled down to a square with a width of 1 and a height of 1. We can therefore
reduce the present problem to the previous one, by simply scaling the result down by a factor
of $1/n^2$. The result is
\begin{align*}
  R &= \frac{1}{n^2}\left(\frac{n^2+n}{2}\right) \\
    &= \frac{1}{2}+\frac{1}{2n} \qquad .
\end{align*}
Since $\Delta x=(b-a)/n=1/n$, the limit as $\Delta x\rightarrow 0$ is the same as
the limit as $n\rightarrow\infty$. The integral equals
\begin{equation*}
  \lim_{n\rightarrow\infty} R = \frac{1}{2} \qquad ,
\end{equation*}
as expected geometrically.

It is also possible here to apply the definition of the integral directly,
using Riemann sums that sample the function at the
center of each subinterval. Amusingly, the result is that $R=1/2$ exactly for all values of
$n$. For example, with $n=3$ the Riemann sum is 
$\frac{1}{6}\Delta x+\frac{1}{2}\Delta x+\frac{5}{6}\Delta x=\frac{9}{6}\Delta x=\frac{1}{2}$.
\end{eg}
  <% end_sec('integral') %>

  <% begin_sec("Leibniz notation",nil,'integral-leibniz') %>
If we substitute equation \eqref{eqn:riemann-sum}
into the definition of the integral $\lim_{\Delta x\rightarrow 0} R$, the result looks
like this:
\begin{equation*}
  \lim_{\Delta x\rightarrow 0} \sum_{k=1}^n f(x_k)\Delta x
\end{equation*}
Leibniz invented the following expressive, versatile, and useful notation for this limit:
\begin{equation*}
  \int_a^b f(x) \der x
\end{equation*}
The symbol $\int$ is an ``S'' that's been stretched like taffy.
It stands for ``sum,'' just as the sigma, $\Sigma$
stands for ``sum.'' But we think of $\int$ as meaning a \emph{smooth} sum, whose graphical
representation is the area under a smooth curve rather than under a staircase. Notice how the shape
of $\int$ is smooth. Like the $k$ in the sigma notation, the $x$ in this example is a dummy variable.
Therefore $\int_a^b f(x) \der x$ means exactly the same thing as $\int_a^b f(s) \der s$. The
dummy variable inside an integral is referred to as a variable of integration, and has no
meaning outside the integral. One of the reasons for writing the
$\der x$ is that it states what we're integrating with respect to.

\begin{eg}{Leibniz notation for the area of a triangle}\label{eg:leibniz-triangle-area}
In example \ref{eg:riemann-triangle-area}, we integrated the function $f(x)=x$ from $x=0$ to 1, and
found that it was 1/2. In Leibniz notation, the result is written like this:
\begin{equation*}
  \int_0^1 x\:\der x = \frac{1}{2}
\end{equation*}
It makes no difference if we notate this instead with $s$ as the variable of integration:
\begin{equation*}
  \int_0^1 s\:\der s = \frac{1}{2}
\end{equation*}
\end{eg}

We defined the Leibniz notation as simply a notation for a certain limit, but we can think of
it conceptually as a sum with infinitely many terms. That is, we make a Riemann sum with
infinitely many rectangles. Normally if you added up an infinite
number of things, you would expect to get an infinite result. But remember, each of these rectangles
is infinitely skinny. We think of $\der x$ as being the infinitely small width, so that the area
$f(x)\der x$ is infinitely small. We're therefore adding an infinite number of things, each of which
is infinitely small, so that the result can be finite. Recall that, as discussed in section
\ref{sec:safe-handling-of-dx}, p.~\pageref{sec:safe-handling-of-dx}, the real number system doesn't
have infinitely big or infinitely small numbers; however, if we handle our infinities
according to the simple rules given
in that section, nothing bad happens. Historically, these rules weren't clear, and practitioners just
knew that if they did their work according to certain methods, the Leibniz notation never led
to the wrong result. This confusion was definitively cleared up around 1965, but many mathematicians
have been influenced by the historical uneasiness about the Leibniz notation, so they prefer to think
of $\int\ldots\der x$ purely as a shorthand notation for a limit. This is a matter of taste. Those who prefer
to think of it only as a shorthand will consider the $\der x$ inside the integral to nothing
more than punctuation, like the period at the end of a sentence. From this point of view, its
only job is to tell us what the dummy variable is, i.e., what we're integrating with respect to.
None of these doubts about the Leibniz notation were ever influential among scientists and engineers;
they picked up the notation enthusiastically in the 1680's and continue using it, exclusively and
unmodified, to this day.

\begin{eg}{Moving the $\der x$ around}
One of the rules in section \ref{sec:safe-handling-of-dx} was that we were allowed to manipulate
differentials such as $\der x$ using any of the elementary axioms of the real numbers
(section \ref{sec:elementary-reals}, p.~\pageref{sec:elementary-reals}). One of these axioms is
that multiplication is commutative, $uv=vu$. Therefore the integral in example
\ref{eg:leibniz-triangle-area} on p.~\pageref{eg:leibniz-triangle-area} can be written in either
of the following equivalent ways:
\begin{equation*}
  \int_0^1 x\:\der x = \frac{1}{2}   \qquad    \int_0^1 \der x \: x = \frac{1}{2}
\end{equation*}
Similarly, all of the following are the same integral:
\begin{equation*}
  \int_1^2 \frac{1}{x}\:\der x = \int_1^2 \der x\:\frac{1}{x} = \int_1^2 \frac{\der x}{x}
\end{equation*}
Most people would write it with the $\der x$ on top, which makes it more compact.
\end{eg}

Another nice feature of the Leibniz notation is that it makes the units come out right.
Consider our earlier example of the town dump. Suppose that the rate of garbage
production is given by a function $p(t)$, where $t$ is in units of years
and $p$ in tons per year. Then the amount of garbage accumulated at the town dump
from year $a$ to year $b$ is given by
\begin{equation*}
  \int_a^b p(t)\:\der t \qquad .
\end{equation*}
The integral sign $\int$ is a kind of sum, and the units of a sum are the same
as the units of each term. Since $\der$ means ``a little bit of \ldots,''
$\der t$ stands for a little bit of time, and it therefore also has units of years.
The units of the terms in the sum are
\begin{equation*}
  \frac{\text{tons}}{\text{year}} \times \text{years} = \text{tons} \qquad ,
\end{equation*}
which makes sense.

We can now see three independent reasons why it would be wrong to write an integral
like $\int_0^1 x^2$ rather than $\int_0^1 x^2\:\der x$, with the $\der x$:
\begin{enumerate}
  \item If $x$ has units, then the expression without the $\der x$ has the wrong units.
  \item It would be a sum of infinitely many numbers, each of them finite, so it would probably be infinite.
  \item If we don't write the $\der x$, we haven't stated what we're integrating with respect to.
\end{enumerate}
  <% end_sec('integral-leibniz') %>

<% end_sec('definite-integral') %>

<% begin_sec("The fundamental theorem of calculus",nil,'fundamental-theorem') %>
We've already seen some clear indications of a link between derivatives and integrals.
A derivative is a rate of change, and an integral measures the accumulation of change.
Let's say for concreteness that we're talking about functions of time.
If a function $A$ tells us the rate at which function $B$ changes, then $B$ tells us
how the rate of change measured by $A$ has accumulated over time. That is, it seems
clear conceptually that the integral and the derivative are inverse operations: operations
that undo each other, in the same way that subtraction undoes addition, or a square
root undoes a square.
<% marg(0) %>
<%
  fig(
    'garbage-spreadsheet-bitmap',
    %q{Columns A and B in the spreadsheet relate to each other approximately as integral and derivative.}
  )  
%>
<% end_marg %>

Figure \figref{garbage-spreadsheet-bitmap} shows this in the context of discrete rather than
continuous functions. Column A shows how many tons of garbage are sent to the town dump per year.
It is the rate of change of the pile at the dump, which is given in column B. The population
is growing, so column A is not constant. Presumably one of these columns was typed into the
spreadsheet from data collected by the town, but we can't tell from looking at the spreadsheet
which one it was. It's possible that the raw data was column A, in which case column B would
have been constructed by telling the spreadsheet software to calculate a running sum based on A.
The running sum of a discrete function is conceptually similar to the integral of a continuous
one, so we can say that in some loose sense that B is the integral of A. On the other hand,
it's possible that the raw data was column B: a municipal employee has been going out to the dump at yearly
intervals and measuring how big the pile of trash was. Column A would then have been calculated from B
by taking differences of successive years. This is conceptually similar to saying that A is the
derivative of B.

\begin{theorem}[The fundamental theorem of calculus]
  Let $f$ be a function defined on the interval $[a,b]$, which is also differentiable
  on that interval. Then
  \begin{equation}
    \label{eqn:fundamental-theorem}
    \int_a^b \frac{\der f}{\der x} dx = f(b) - f(a) \qquad .
  \end{equation}
\end{theorem}

On the left-hand side, we have taken a function, differentiated it, and
then integrated it. The right-hand side is a simple expression involving the
original function, i.e., in some sense the integration has undone the
differentation, and we are left with the same function we started with.

Because of this theorem the expression on the right appears so often
that various abbreviations have been invented.  We will abbreviate
\[
f(b)-f(a) \stackrel{\text{def}}{=} \bigl. f(x)\bigr]_{x=a}^b =
\bigl.f(x)\bigr]_a^b .
\]

To see why the right-hand side contains a difference of two values of
$f$, imagine

<% end_sec('fundamental-theorem') %>


<% end_chapter %>
