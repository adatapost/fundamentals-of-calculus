%%chapter%% 02
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '02',
    %q{Limits; techniques of differentiation},
    'ch:limits'
  )
%>

In chapter \ref{ch:derivative} we started computing derivatives simply by appealing to
a list of geometrically plausible properties (section \ref{subsec:properties-of-the-derivative},
p.~\pageref{subsec:properties-of-the-derivative}). These properties are true, and by taking
them as axioms we were able to prove rigorously that, for example, the derivative of $x^2$
is $2x$ (section \ref{subsec:derivative-of-x-squared}, p.~\pageref{subsec:derivative-of-x-squared}).
But there are many problems that are messy to solve by this limited toolbox of techniques, and 
many others for which we need qualitatively different tools.

Historically, the way Newton and\label{discard-dx-squared}
Leibniz approached the problem was as follows. Suppose we want to take the derivative of
$x^2$ at the point P where $x=1$. We already know that we can get a good numerical approximation to this
derivative by taking a second point Q, close to P, and evaluating the slope of the line through P and
Q. (See section \ref{subsec:approximating-deriv}, p.~\pageref{subsec:approximating-deriv}).
Now instead of picking specific numbers, let's just take point Q to lie at $x=1+\der x$,
where $\der x$ is very small. Then the slope of the line through P and Q is
\begin{align*}
  \text{slope of line PQ} &= \frac{\Delta y}{\Delta x} \\
               &= \frac{(1+\der x)^2-1}{(1+\der x)-1} \\
               &= \frac{2\der x+\der x^2}{\der x} 
\end{align*}
Now comes the crucial leap of faith, which mathematicians of later centuries began to
feel was a little too sketchy.
The number $\der x$ is supposed to be small, and when you square a small number you get an even smaller number.
Since $\der x$ is supposed to be infinitely small, $\der x^2$ should be so small that it's utterly
unimportant, even compared to $\der x^2$. Therefore we throw away the $\der x^2$ term and find that the slope
of the tangent line is 2.

<% begin_sec("The definition of the limit",nil,'limit') %>
Starting in the 19th century, the real number system was formally defined, and it became clear that although
one could have a number system that obeyed the axioms given in section \ref{sec:elementary-reals}
(p.~\pageref{sec:elementary-reals}) and that included infinitely small numbers, such a system would not
be the same as the real numbers, and furthermore one would have a problem with the procedure of
treating a $\der x^2$ as if it were zero; one can prove from those axioms that zero itself is the
only number whose square is zero.\footnote{For more on this topic, see optional
section \ref{sec:safe-handling-of-dx} on p.~\pageref{sec:safe-handling-of-dx}.} For these reasons, mathematicians turned to a different way of
defining the derivative, by using the new notion of a \emph{limit}.\index{limit}

<% begin_sec("An informal definition",nil,'limit-informal') %>
While it is easy to define precisely in a few words what a square root is
($\sqrt{a}$ is the positive number whose square is $a$) the definition of
the limit of a function runs over several terse lines, and most people
don't find it very enlightening when they first see it.
So we postpone this momentarily and start by building up our intuition.

\begin{lessimportant}[Definition of limit (first attempt)]
If $f$ is some function then
\[
\lim_{x\to a} f(x) = L
\]
is read ``the limit of $f(x)$ as $x$ approaches $a$ is $L$.''  It means
that if you choose values of $x$ which are close \emph{but not equal} to
$a$, then $f(x)$ will be close to the value $L$; moreover, $f(x)$ gets
closer and closer to $L$ as $x$ gets closer and closer to $a$.
\end{lessimportant}

The following alternative notation is sometimes used
\[
f(x)\to L \quad\text{ as } \quad x\to a;
\]
(read ``$f(x)$ approaches $L$ as $x$ approaches $a$'' or ``$f(x)$ goes to
$L$ is $x$ goes to $a$''.)

\begin{eg}{}
If $f(x) = x+3$ then
\[
\lim_{x\to 4} f(x) = 7,
\]
is true, because if you substitute numbers $x$ close to $4$ in $f(x) = x+3$
the result will be close to $7$.
\end{eg}

\begin{eg}{Substituting numbers to guess a limit}\label{eg:limit-by-sub-good}
What (if anything) is
\[
\lim_{x\to 2}\frac{x^2 -2x}{x^2-4}?
\]
Here $f(x) = (x^2 - 2x)/(x^2-4)$ and $a=2$.

We first try to substitute $x=2$, but this leads to
\[
f(2) = \frac{2^2 - 2\cdot2}{2^2-4} = \frac 00
\]
which does not exist.  Next we try to substitute values of $x$ close but
not equal to $2$.  The table suggests that $f(x)$
approaches $0.5$.
\end{eg}

<% marg(80) %>
\begin{tabular}{l|l}
    \quad$x$ & \quad$f(x)$ \\
    \hline
    3.000000 & 0.600000\\
    2.500000 & 0.555556\\
    2.100000 & 0.512195\\
    2.010000 & 0.501247\\
    2.001000 & 0.500125\\
  \end{tabular}
\\\hspace{8mm}\formatlikecaption{Example \ref{eg:limit-by-sub-good}.}
<% end_marg %>

\begin{eg}{Substituting numbers can suggest the wrong answer.}\label{eg:limit-by-sub-bad}
Our first definition of ``limit'' was not
very precise, because it said ``$x$ close to $a$,'' but how close is close
enough?  Suppose we had taken the function
\[
g(x) = \frac{101\,000x}{100\,000x+1}
\]
and we had asked for the limit $\lim_{x\to0}g(x)$.
Then substitution of some ``small values of $x$,'' as shown in the table,
could lead us to believe
that the limit was $1$.  Only when you substitute even smaller
values do you find that the limit is zero!
\end{eg}

<% marg(80) %>
  \begin{tabular}{l|l}
    \quad$x$ & \quad$g(x)$ \\
    \hline
    1.000000 & 1.009990\\
    0.500000 & 1.009980\\
    0.100000 & 1.009899\\
    0.010000 & 1.008991\\
    0.001000 & 1.000000\\
  \end{tabular}
\\\noindent\formatlikecaption{Example \ref{eg:limit-by-sub-bad}.}
<% end_marg %>

<% end_sec('limit-informal') %>

<% begin_sec("The formal, authoritative definition of the limit",nil,'limit-formal') %>
The informal description of the limit
uses phrases like ``closer and closer'' and ``really very small.'' In the
end we don't really know what they mean, although they are suggestive.
Fortunately there is a better definition, i.e.~one which is unambiguous
and can be used to settle any dispute about the question of whether or not
$\lim_{x\to a} f(x)$ equals some number $L$. 

\begin{important}[Definition of the limit]
We say that $L$ is the limit of $f(x)$ as $x\to a$, if the following two conditions hold:
\begin{enumerate}

  \item The function $f(x)$ need not be defined at $x=a$, but it must be defined for all
    other $x$ in some interval which contains $a$.

  \item For every $\varepsilon>0$ one can find a $\delta>0$ such that 
        for all values of $x$ in the domain of $f$ with $|x-a|<\delta$, we have
      $|f(x) - L|<\varepsilon$ .
\end{enumerate}
\end{important}

\emph{Why the absolute values?} The quantity $|x-y|$ is the distance
between the points $x$ and $y$ on the number line, and one can measure how
close $x$ is to $y$ by calculating $|x-y|$.  The inequality $|x-y|<\delta$
says that ``the distance between $x$ and $y$ is less than $\delta$,'' or
that ``$x$ and $y$ are closer than $\delta$.''

\emph{What are $\varepsilon$ and $\delta$?} The quantity $\varepsilon$
is how close you would like $f(x)$ to be to its limit $L$; the quantity
$\delta$ is how close you have to choose $x$ to $a$ to achieve this.  To
prove that $\lim_{x\to a} f(x) = L$ you must assume that someone has given
you an unknown $\varepsilon>0$, and then find a positive $\delta$ for which
$x$ values that close to $a$ result in values of $f$ that lie with the
range the person has demanded.  The $\delta$ you find will depend on
$\varepsilon$.
<% marg(0) %>
<%
  fig(
    'epsilon-delta',
    %q{The value of $\varepsilon$ is imposed on us. We have succeeded in finding a value of $\delta$ 
       small enough so that the outputs of the function do lie within the desired range. If we can
       do this for \emph{every} value of $\varepsilon$, then the limit is $L$.}
  )  
%>
<% end_marg %>

\begin{eg}{}\label{eg:limit-of-linear-function}
\egquestion Show that $\lim_{x\to5}2x+1=11$.

\eganswer We have $f(x) = 2x+1$, $a=5$ and $L=11$, and the question we must answer is
``how close should $x$ be to $5$ if want to be sure that $f(x)=2x+1$
differs less than $\varepsilon$ from $L=11$?''

To figure this out we try to get an idea of how big $|f(x)-L|$ is:
\[
|f(x)-L| = \bigl|(2x+1)-11\bigr| = |2x-10| = 2\cdot |x-5| = 2\cdot |x-a|.
\]
So, if $2|x-a|<\varepsilon$ then we have $|f(x)-L|<\varepsilon$, i.e.
\[
\text{if }|x-a|<\tfrac12\varepsilon \text{ then } |f(x)-L|<\varepsilon.
\]
We can therefore choose $\delta = \frac12\varepsilon$.  No matter what
$\varepsilon>0$ we are given our $\delta$ will also be positive, and if
$|x-5|<\delta$ then we can guarantee $|(2x+1) - 11|<\varepsilon$.  That
shows that $\lim_{x\to 5}2x+1 = 11$.

\end{eg}

<% end_sec('limit-formal') %>

\startdq

\begin{dq}
Figure \figref{epsilon-delta} on p.~\pageref{fig:epsilon-delta} shows an example where $\delta$ is small enough for the given
value of $\varepsilon$. What would the figure look like in a case where the value of $\delta$
was \emph{not} small enough?
\end{dq}

<% end_sec('limit') %>

<% begin_sec("The definition of the derivative",nil,'def-of-derivative') %>
The single most important application of the limit is that it gives us a
way to formalize the idea of a derivative, which we have so far been using
on an informal basis. We start from the Newton-Leibniz approach described on
p.~\pageref{discard-dx-squared}, but modify it by using a limit to get rid of the questionable
procedure of discarding the square of an infinitesimally small number.

\begin{important}[Definition of the derivative]\index{derivative!defined as a limit}
The derivative of a function $f$ at a point $x$ is
\begin{equation*}
  f'(x) = \lim_{\Delta x\rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x} \qquad .
\end{equation*}
\end{important}

\noindent If this limit is undefined at a certain $x$, then the derivative is undefined there, and we
say that $f$ is not \emph{differentiable} at $x$.\index{differentiability}

\begin{eg}{The derivative of $x^2$, using limits}\label{eg:x-squared-with-limits}
Let's use the definition to find the derivative of $x^2$ at $x=1$. We have
\begin{align*}
  f'(1) &= \lim_{\Delta x\rightarrow 0} \frac{(1+\Delta x)^2-1}{\Delta x} \\
        &= \lim_{\Delta x\rightarrow 0} \frac{2\Delta x+\Delta x^2}{\Delta x} \\
        &= \lim_{\Delta x\rightarrow 0} (2+\Delta x) 
\end{align*}
We've already shown in example \ref{eg:limit-of-linear-function} on p.~\pageref{eg:limit-of-linear-function}
that this sort of limit of a linear function is just what you would expect by plugging in to the
equation of the line, and therefore we have $f'(1)=2$.
\end{eg}
<% marg(0) %>
<%
  fig(
    'x-squared-geometrically',
    %q{A geometrical interpretation of the expression $2\Delta x+\Delta x^2$ occurring in
       the second line of example \ref{eg:x-squared-with-limits}. The area gained by increasing
       the size of the square equals the area of the two thin strips plus the area of the small square.}
  )  
%>
<% end_marg %>

\begin{eg}{The derivative of an exponential function, with limits}\label{eg:exponential-with-limits}
In example \ref{eg:bunnies} on p.~\pageref{eg:bunnies}, we inferred using a simple geometrical trick
that the derivative of an exponential function like $f(x)=2^x$ must be proportional to $f$ itself,
\begin{equation*}
  f' = k f \qquad ,
\end{equation*}
where the constant of proportionality $k$ depends on the base 2. We can now prove the same
fact using limits, and say something about the value of the constant. Since this fact is supposed
to hold for all values of $x$, and $k$ is to be the same
for any $x$, we can pick any convenient value for $x$, say $x=0$. For the derivative we have
\begin{align*}
  f'(0) &= \lim_{\Delta x\rightarrow 0} \frac{2^{0+\Delta x}-2^0}{\Delta x} \\
        &= \lim_{\Delta x\rightarrow 0} \frac{2^{\Delta x}-1}{\Delta x} \qquad .
\end{align*}
Since $f(0)=1$, we have
\begin{equation*}
  k = \lim_{\Delta x\rightarrow 0} \frac{2^{\Delta x}-1}{\Delta x}
\end{equation*}
We can get as good an approximation to this limit as we like by plugging in small enough values
of $\Delta x$. For example, $\Delta x=10^{-4}$ gives $k\approx0.69317$,
which seems to be an approximation to $\ln 2=0.69314\ldots$ This naturally leads
us to conjecture that the derivative of $b^x$ equals $(\ln b) b^x$, and in particular
that the derivative of $e^x$ is simply $e^x$.
\end{eg}

We seldom evaluate a derivative by directly applying its definition as a limit.
Instead, we use a variety of other more convenient rules that follow from the
definition. Some of these are the properties in section \ref{subsec:properties-of-the-derivative},
p.~\pageref{subsec:properties-of-the-derivative}. In addition, we will learn two very
important and useful rules, the product rule and the chain rule.
<% end_sec('def-of-derivative') %>

<% begin_sec("The product rule",nil,'product-rule') %>
The idea behind the product rule is very similar to the geometrical intuition expressed by
figure \figref{x-squared-geometrically} on p.~\pageref{fig:x-squared-geometrically} for
the derivative of $x^2$. Suppose
that instead of $x$ multiplied by $x$ to make $x^2$, we have some other
function such as $(x^2+7)(x^3)$, which is also the product of two factors.
Call these factors $u(x)$ and $v(x)$, so that the function we're differentiating
is $f(x)=u(x)v(x)$. Then the expression we get by applying the definition of the
derivative to $f$ can be written in terms of the rectangular areas in figure
\figref{product-rule-geometrically} as
\begin{equation*}
  f'(x) = \lim_{\Delta x\rightarrow0} \frac{(\text{right strip})+(\text{top strip})
           +(\text{tiny box})}{\Delta x}
\end{equation*}
One can prove from the definition of the limit that the limit of a sum is equal to the sum
of the limits, provided that the individual limits exist (see section
\ref{subsec:properties-of-the-limit}, p.~\pageref{subsec:properties-of-the-limit},
property $P_3$), so:
\begin{align*}
  f'(x) =&\quad \lim_{\Delta x\rightarrow0} \frac{(\text{right strip})}{\Delta x} \\
      &+\lim_{\Delta x\rightarrow0} \frac{(\text{top strip})}{\Delta x} \\
      &+\lim_{\Delta x\rightarrow0} \frac{(\text{tiny box})}{\Delta x} 
\end{align*}
If the functions $u$ and $v$ are both well-behaved at $x$ (specifically, if both of them
are differentiable), then the ``tiny box'' term will vanish upon application of the limit
just as in example \ref{eg:x-squared-with-limits}. We then have
\begin{align*}
  f'(x) &= \lim_{\Delta x\rightarrow0} \frac{(\text{right strip})}{\Delta x} 
           +\lim_{\Delta x\rightarrow0} \frac{(\text{top strip})}{\Delta x} \\
     &= u'(x)v(x)+v'(x)u(x) \qquad .
\end{align*}
We have the following extremely important and useful rule for differentiation:

\begin{important}[Product rule]\index{product rule}
Let $f=uv$, where $f$, $u$, and $v$ are all functions. Then at any point where
$u$ and $v$ are both differentiable,
\begin{equation*}
  f' = u'v+v'u \qquad .
\end{equation*}
\end{important}

<% marg(0) %>
<%
  fig(
    'product-rule-geometrically',
    %q{A geometrical interpretation of the product rule.}
  )  
%>
<% end_marg %>

\begin{eg}{The product rule for $x^3$}\label{eg:derivative-of-x-cubed}
So far we have never actually proved any derivatives of powers of $x$ other than $x^2$;
although the proofs can be done by the methods of ch.~\ref{ch:derivative}, they are tedious.
These results come out much more easily by applying the product rule. We have already proved
that the derivative of $x^2$ was $2x$. To get the derivative of $x^3$, we can simply
rewrite it as the product $(x^2)\cdot(x)$. Applying the product rule then gives
\begin{align*}
  (x^3)' &= [(x^2)\cdot(x)]' \\
         &= (x^2)'\cdot(x) + (x^2)\cdot(x)' \\
         &= 2x\cdot x + x^2\cdot 1 \\
         &= 3x^2 \qquad .
\end{align*}
\end{eg}

<% end_sec('product-rule') %>

<% begin_sec("The chain rule",nil,'chain-rule') %>
<% begin_sec("Constant rates of change",nil,'chain-rule-constant-rates') %>
In addition to the chain rule, the other extremely important rule for differentiation is the chain
rule. We start with two examples that illustrate the idea but don't require calculus.

\begin{eg}{Burning calories}\label{eg:burning-calories}
\egquestion Jane hikes 3 kilometers in an hour, and hiking burns 70 calories per kilometer. At what rate
does she burn calories?

\eganswer We let $x$ be the number of hours she's spent hiking so far, $y$ the distance covered,
and $z$ the calories spent. Then
\begin{align*}
  \frac{\Delta z}{\Delta x}
          &= \frac{\Delta z}{\Delta y} \cdot \frac{\Delta z}{\Delta y} \\
          &= \left(\frac{70\ \zu{cal}}{1\ \cancel{\zu{km}}}\right)
                                     \left(\frac{3\ \cancel{\zu{km}}}{1\ \zu{hr}}\right) \\
          &= 210\ \zu{cal}/\zu{hr} \qquad .
\end{align*}
\end{eg}
<% marg(-10) %>
<%
  fig(
    'gear-ratio',
    %q{Example \ref{eg:gear-ratio}.}
  )  
%>
<% end_marg %>

\begin{eg}{Gear ratios}\label{eg:gear-ratio}
\egquestion Figure \ref{fig:gear-ratio} shows a piece of farm equipment containing a train of
gears with 13, 21, and 42 teeth. If the smallest gear is driven by a motor, relate the rate of
rotation of the biggest gear to the rate of rotation of the motor.

\eganswer
Let $x$, $y$, and $z$ be the angular positions of the three gears. Then
\begin{align*}
  \frac{\Delta z}{\Delta x} &= \frac{\Delta z}{\Delta y}  \cdot \frac{\Delta y}{\Delta x} \\
                        &= \frac{13}{21} \cdot \frac{21}{42} \\
                        &= \frac{13}{42} \qquad .
\end{align*}

\end{eg}

These examples both used the following relationship among three rates of change:
\begin{equation}\label{eq:chain-rule-deltas}
  \frac{\Delta z}{\Delta x} = \frac{\Delta z}{\Delta y}  \cdot \frac{\Delta y}{\Delta x} 
\end{equation}
Because the rates of change were stated to be constant, it was valid to measure them with
expressions of the form $\Delta\ldots/\Delta\ldots$, and because the deltas were real numbers,
it was valid to use the normal rules of algebra
and cancel the factors $\Delta y$.
<% end_sec('chain-rule-constant-rates') %>
<% begin_sec("Varying rates of change",nil,'chain-rule-varying-rates') %>
The Leibniz notation makes it tempting to simply write down and
believe the following analogous-looking expression involving derivatives:
\begin{equation*}
  \frac{\der z}{\der x} = \frac{\der z}{\der y}  \cdot \frac{\der y}{\der x} 
\end{equation*}
In problems \ref{hw:chain-rule-circle}-\ref{hw:chain-rule-triangle} on p.~\pageref{hw:chain-rule-circle}
we verified that this seemed to work. But how do we know that this really always works
with derivatives? If we're really defining the Leibniz notation as standing for a limit, then
we really need to show this:
\begin{equation}\label{eg:chain-rule-limit-product}
     \lim_{\Delta x\rightarrow0 }\frac{\Delta z}{\Delta x}
   = \left(\lim_{\Delta y\rightarrow0 }\frac{\Delta z}{\Delta y}  \right)
     \left(\lim_{\Delta x\rightarrow0 }\frac{\Delta y}{\Delta x} \right)
\end{equation}
If all three derivatives exist, then
this essentially works because the limit of a product is the product of a limit (provided
that the limits exist); this is property $P_5$ of the limit, to be discussed more formally
in section
\ref{subsec:properties-of-the-limit}, p.~\pageref{subsec:properties-of-the-limit}.

There are two technical issues to worry about. First, equation \eqref{eq:chain-rule-deltas}
is not true if $\Delta y=0$, because we can't divide by zero, and if the derivative of
$y$ with respect to $x$ happens to be zero somewhere, then it's reasonable to worry that
this might be forced upon us for a certain value of $\Delta x$. Although we won't prove it
here, this issue doesn't actually cause the chain rule to fail. The second issue is that in
equation \eqref{eg:chain-rule-limit-product}, two of the limits involve $\Delta x\rightarrow 0$,
but one has $\Delta y\rightarrow 0$. This turns out not to be a problem because, as discussed
in ch.~\ref{ch:more-limits}, a differentiable function must be \emph{continuous} (i.e., there
are no gaps in its graph), and therefore if, by assumption, $y$ is differentiable as
a function of $x$, then
$y$ is also continuous, and therefore taking $\Delta x\rightarrow 0$ also causes
$\Delta y\rightarrow 0$.

We therefore have:\index{chain rule}\\*
\begin{important}[The chain rule]
If $z$ is a function of $y$, and $y$ is a function of $x$, and if
the derivatives $\der z/\der y$ and $\der y/\der x$ exist at a certain point,
then at that point,
\begin{equation*}
  \frac{\der z}{\der x} = \frac{\der z}{\der y}  \cdot \frac{\der y}{\der x}  \qquad .
\end{equation*}
\end{important}
<% end_sec('chain-rule-varying-rates') %>
<% begin_sec("Composition of functions",nil,'composition') %>
The \emph{composition} of functions $f$ and $g$, $f\circ g$, means the function that takes an input $x$
and gives back an output $f(g(x))$. The chain rule tells us how to differentiate a function built
out of such a composition. That is, we take the input $x$, stick it into $g$, take $g$'s output, put
it in $f$, and finally take $f$'s output.
<% end_sec('composition') %>
<% end_sec('chain-rule') %>

<% begin_sec("Review: exponents that aren't natural numbers",nil,'non-natural-exponents') %>
In section \ref{sec:power-rule-general} we will exploit the product and chain rules to
prove the rule $(x^n)'=nx^{n-1}$ for all values of $n$ that are nonzero rational numbers.
As preparation, we review in this section the basic idea of exponentiation, and then
the interpretation of exponents that aren't natural numbers.

<% begin_sec("Basic ideas",nil,'basic-ideas') %>
We can represent repeated multiplication
\begin{equation*}
  2\times2\times2 = 8 \qquad ,
\end{equation*}
using the notation for exponents,
\begin{equation*}
  2^3 = 8 \qquad .
\end{equation*}
Because multiplication is associative,
\begin{equation*}
  2\times2\times2\times2\times2\times2\times2 = 128
\end{equation*}
is the same as
\begin{equation*}
  (2\times2\times2)(2\times2\times2\times2) \qquad ,
\end{equation*}
so $2^7$ is the same as $(2^3)(2^4)$. In other words, multiplication is the same
as adding exponents,
\begin{equation}\label{eqn:add-exponents}
  b^ub^v=b^{u+v} \qquad .
\end{equation}
An important special case is scientific notation, which uses powers of 10.
For example, $(10^2)(10^7)=10^9$.
<% end_sec('basic-ideas') %>

<% begin_sec('Zero as an exponent',nil,'exponent-zero') %>
Suppose we compute the list of decreasing powers of
a given base, for example $2^3=8$, $2^2=4$, and $2^1=2$. Each result is half
as big as the previous one. Therefore if we want to continue reducing the exponent,
we should clearly have $2^0=1$ in order to continue the pattern. In general,
$b^0=1$ for any nonzero base $b$. (The special case $0^0$ is undefined.)
<% end_sec('exponent-zero') %>

<% begin_sec('Negative exponents',nil,'negative-exponents') %>
Continuing this pattern, we must have $2^{-1}=1/2$.
In general, negative exponents indicate the inverse of the corresponding positive
exponent.
<% end_sec('negative-exponents') %>

<% begin_sec('Fractional exponents',nil,'fractional-exponents') %>
Our rules for zero and negative exponents were
consistent with equation \eqref{eqn:add-exponents}. We can also define fractional
exponents that obey this rule. For example, if $3^{1/2}$ is a number, then
equation \eqref{eqn:add-exponents} requires that $(3^{1/2})(3^{1/2})=3$, so
an exponent $1/2$ must mean the same thing as a square root.
<% end_sec('fractional-exponents') %>

<% begin_sec('Irrational exponents',nil,'irrational-exponents') %>
If we want to define an expression such as $2^\pi$, we can take it to be
the limit of the list of numbers
$2^3$, $2^{3.1}$,  $2^{3.14}$,  $2^{3.141}$, \ldots
<% end_sec('irrational-exponents') %>

<% end_sec('non-natural-exponents') %>

<% begin_sec("Proof of the power rule in general",nil,'power-rule-general') %>
In section \ref{sec:deriv-polynomial}, p.~\pageref{sec:deriv-polynomial},
I presented the rule $(x^n)'=nx^{n-1}$ for all natural numbers $n$,  but only explicitly proved it
for $n=1$ and 2.
A good application of the product and chain rules is to extend
the proof to all nonzero integers $n$ and to show that it also holds for fractional exponents.

Only $n=0$ requires special treatment. Since $x^0=1$, its derivative should be zero.
Our rule sort of, but not quite, works here, since it gives $0x^{-1}$, or $0/x$. This is certainly
zero if $x\ne0$, but in the case where $x=0$ it gives $0/0$, which is undefined.

<% begin_sec("Exponents that are natural numbers",nil,'natural-powers') %>
Example \ref{eg:derivative-of-x-cubed} on p.~\pageref{eg:derivative-of-x-cubed} showed that the product rule can be used
to prove special cases of the power rule.
Since we knew the derivative of $x^2$,
we were able to find the derivative of $x^3$ by rewriting it as $(x^2)(x)$ and
applying the power rule. In the same way, we can prove the rule for any exponent $n$
if it has already been established for $n-1$. We rewrite $x^n$ as $(x^{n-1})(x)$,
differentiate using the product rule, and find:
\begin{align*}
  (x^n)' &= (x^{n-1})'(x)+(x^{n-1})(x)' \\
         &= (n-1)x^{n-2}x+x^{n-1} \\
         &= nx^{n-1}
\end{align*}
By establishing the fact for $n=1$, and then proving that it must hold for $n$ if it
holds for $n-1$, we establish that it holds for all natural numbers $n$. This is
called \emph{proof by induction}.\index{induction}\index{proof!by induction}
<% end_sec('natural-powers') %>
<% begin_sec("Negative exponents",nil,'negative-powers') %>
If we write $1=(x)(1/x)$, then we can differentiate on both sides, and we find that
$(1/x)'=-1/x^2$. This is exactly what we would have expected from applying the
power rule to the exponent $-1$. It is then straightforward to extend the result
to all negative integers by applying the chain rule to $(x^n)^{-1}$.
<% end_sec('negative-powers') %>
<% begin_sec("Exponents that aren't integers",nil,'fractional-powers') %>
What about fractional exponents, such as $x^{1/2}$, i.e., the square root of $x$?
We don't know what this derivative is yet, but let's
give it a name. Call it $f$, i.e., $f(x)=(\sqrt{x})'$. Then
\begin{align*}
  1 &= x' \\
    &= (\sqrt{x}\sqrt{x})' \\
    &= f(x)\sqrt{x}+\sqrt{x}f(x) \\
    &= 2f(x)\sqrt{x} \\
  f(x) &= \frac{1}{2\sqrt{x}} \\
       &= \frac{1}{2}x^{-1/2}
\end{align*}
This is exactly what we would have inferred from the power rule $(x^n)'=nx^{n-1}$, with $n=1/2$.
A similar argument can be carried out for any fractional exponent. The generalization to irrational
exponents is deferred until example \ref{eg:power-any-exponent} on p.~\pageref{eg:power-any-exponent}.
<% end_sec('fractional-powers') %>
<% end_sec('power-rule-general') %>

<% begin_sec("Quotients",nil,'quotients') %>
Suppose that we want to differentiate the function
\begin{equation*}
  \frac{1}{x} \qquad .
\end{equation*}
The product rule tells us how to differentiate an expression involving multiplication,
but this one uses division. However, division by a certain number is the same as multiplication by its
multiplicative inverse, so we can rewrite this function in a form that we know how to differentiate.
\begin{align*}
  \left(\frac{1}{x}\right)' &= \left(x^{-1}\right)' \\
          &= -x^{-2} \qquad \text{[power rule]}
\end{align*}
If the expression in the denominator is more complicated, we can do the same thing, but use
the chain rule as well:
\begin{align*}
  \left(\frac{1}{1+x^2}\right)' &= \left((1+x^2)^{-1}\right)' \\
          &= -(1+x^2)^{-2}(2x)
\end{align*}
If the numerator is not just 1, then we also have to use the product rule:
\begin{align*}
  \left(\frac{x^3}{1+x^2}\right)' &= \left(x^3(1+x^2)^{-1}\right)' \\
          &= (x^3)'(1+x^2)^{-1}+x^3\left[(1+x^2)^{-1}\right]' \qquad \text{[product rule]} \\
          &= 3x^2(1+x^2)^{-1}+x^3\left[-(1+x^2)^{-2}(2x)\right] \\
          &= \frac{x^4+3x^2}{\left(1+x^2\right)^2} \qquad \text{[simplify]}
\end{align*}
% calc -e "x=3.7; y=x^3/(1+x^2); dx=.001; x=x+dx; y2=x^3/(1+x^2); (y2-y)/dx; (x^4+3x^2)/(1+x^2)^2"
% ... verifies that it's right

The foregoing examples show a technique for differentiating quotients that works
in all cases, and this is how I do that type of derivative. Some people, however,
prefer to memorize the following rule, which can be proved by running through the steps
above for a function $f=p/q$, where $p$ and $q$ can be any functions at all.

\begin{lessimportant}[Quotient rule]
Let $f=p/q$, where $f$, $p$, and $q$ are all functions. Then at any point where
$p$ and $q$ are both differentiable and $q\ne0$,
\begin{equation*}
  f' = \frac{p'q-q'p}{q^2} \qquad .
\end{equation*}
\end{lessimportant}

<% end_sec('quotients') %>

<% begin_sec("Continuity",nil,'continuity') %>
Intuitively, a continuous function is one whose graph
has no sudden jumps in it; the graph is all a single connected piece. 
Such a function can be drawn without picking the pen up off of the paper.
Formally, continuity is defined as follows.

\begin{important}
A function $g$ is \emph{continuous} at $a$ if
\begin{equation}\label{eq:continuity-def}
  \lim_{x\to a} g(x) = g(a)
\end{equation}
A function is continuous if it is continuous at every $a$ in its
domain.
\end{important}

If a function is discontinuous at a given point, then it is not differentiable at that point.
On the other hand, the example $y=|x|$ shows that a function can be continuous without
being differentiable.

<% marg(0) %>
<%
  fig(
    'discontinuous',
    %q{A discontinuous function.}
  )  
%>
<% end_marg %>

In most cases, there is no need to invoke the definition explicitly in order to check whether
a function is continuous. Most of the functions we work with are defined by putting together
simpler functions as building blocks. For example, let's say we're already convinced that the
functions defined by $g(x)=3x$ and $h(x)=\sin x$ are both continuous.\footnote{The reader who
has forgotten all of his/her trig is directed to the review in section \ref{sec:trig}.} Then if we encounter the
function $f(x)=\sin(3x)$, we can tell that it's continuous because its definition corresponds
to $f(x)=h(g(x))$. The functions $g$ and $h$ have been set up like a bucket brigade, so that
$g$ takes the input, calculates the output, and then hands it off to $h$ for the final step
of the calculation. This method of combining functions is called \emph{composition}.\index{composition}
The composition of two continuous functions is also continuous. Just watch out for division.
The function $f(x)=1/x$ is continuous everywhere except at $x=0$, so for example $1/\sin(x)$
is continuous everywhere except at multiples of $\pi$, where the sine has zeroes.

<% end_sec('continuity') %>

<% begin_sec("Safe handling of $\\der y$ and $\\der x$ (optional)",nil,'safe-handling-of-dx',{'optional'=>true}) %>
We've seen that although the real number system doesn't include infinitely big or infinitely small
quantities, it can nevertheless be extremely useful to think of
a notation like $\der y/\der x$ as the quotient of two infinitely small numbers.
For example, it allows us to check our work in differentiation by checking the units
of the result (example \ref{eg:pest}, p.~\pageref{eg:pest}), and it makes the chain rule
look so obvious that there would never be any danger of forgetting it. 
When the calculus was first invented, these infinitely small numbers were
referred to as \emph{infinitesimal} numbers. The idea is that just as a decimal is
one tenth, an infinitesimal is one ``infinitieth.''

We now confront the
question of when it's safe to treat $\der y$ and $\der x$ as if they were numbers.
This kind of manipulation is like nuclear energy: it can be used for good and for evil,
and if you want to use it safely, you have to know what you're doing.
In this section we lay out some simple safety rules which, if followed, will prevent
all nuclear meltdowns. Just as we enriched the set of natural numbers to make
the rational numbers, and the rational numbers to make the reals, we continue the march of
progress by making an even larger number system, called the hyperreal numbers,
which includes infinitesimals.
For a more detailed exposition of this kind of thing
at the freshman-calculus level, see the excellent free online book by Keisler,
\emph{Elementary Calculus: An Approach Using Infinitesimals}.

We start with two preliminary definitions.

Definition:
Suppose that for a certain nonzero number $d$, we have $|d|<1$, $|d|<1/(1+1)$, $|d|<1/(1+1+1)$, \ldots
and so on for all inequalities of this form.\footnote{Cf.~example \ref{eg:archimedean}, p.~\pageref{eg:archimedean}.}
Then we say that $d$ is  \emph{infinitesimal}.

Definition: Let $H$ be a hyperreal number (which may or may not also be a real number).
Suppose that there exists some real number $r$ such that $|H-r|$ is infinitesimal.
Then we say that $r$ is the \emph{standard part} of $H$.

\emph{Rule 1.} The hyperreal numbers obey all the same elementary axioms as the
      real numbers (section \ref{sec:elementary-reals}, p.~\pageref{sec:elementary-reals}).

The hyperreals numbers include at least one infinitesimally small number, call it $d$.
By the multiplicate inverse axiom, it follows that $1/d$ is also a well-defined hyperreal
number, and clearly $1/d$ is bigger than $1$, bigger than $1+1$, and so on, so the hyperreal
number system includes both infinitely big and infinitely small quantities.

It can be proved from the elementary axioms that if $d$ is nonzero, then $2d\ne d$.
Therefore the hyperreal number system includes a variety of sizes of infinitesimals.
This is important, because if all infinitesimals were the same size, then $\der y/\der x$
would always have to equal one!
It also follows from the axioms that $1/d \ne 1/(2d)$, so infinite numbers come in different sizes as well.
We therefore have:

\emph{Rule 2.}  The symbol $\infty$ and the term ``infinity'' do not stand for any real number, and do
      not stand for any specific hyperreal number. They are in fact not very useful in the context
      of the hyperreals.\label{infinity-is-not-a-hyperreal}

\begin{eg}{Breaking the rules gives a nuclear meltdown}
Suppose that the universe is infinite, so that there are infinitely many animals in the universe
that, like us, have two eyes. The number of left eyes is some infinite hyperreal number $H$, and $H$ is
also the number of right eyes. The total number of eyes is then
\begin{equation*}
  H+H=2H \qquad .
\end{equation*}
Everything is all right, and $2H$ is an infinite number that happens to be twice as big as $H$.

But now suppose we break rule 2 and use the symbol $\infty$ indiscriminately for any positive, infinite
quantity. Then we have
\begin{equation*}
  \infty+\infty=\infty \qquad .
\end{equation*}
Applying the additive inverse axiom, we can cancel an $\infty$ from each side, giving
\begin{equation*}
  \infty=0 \qquad ,
\end{equation*}
which is absurd.

The paradox didn't result from talking about infinite numbers. It came from
breaking one of the rules for manipulating them correctly.
\end{eg}

Historically, one of the main sources of confusion about infinitesimals was the
sketchy practice of discarding the square of an infinitesimal (p.~\pageref{discard-dx-squared}).
This is resolved as follows:

\emph{Rule 3.} The derivative of $y$ with respect to $x$ equals the standard part of
  $\der y/\der x$.

Redoing the example from p.~\pageref{discard-dx-squared} according to this rule, we
have the following calculation of the derivative of $y=x^2$ at $x=1$:
\begin{align*}
  \frac{\der y}{\der x} &= \frac{(1+dx)^2-1}{(1+dx)-1} \\
                        &= 2+dx \\
  y' &= \text{standard part of}\ 2+dx \\
     &= 2
\end{align*}


<% end_sec('safe-handling-of-dx') %>

<% end_chapter %>
