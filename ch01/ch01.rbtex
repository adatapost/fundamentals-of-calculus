%%chapter%% 01
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '01',
    %q{An informal introduction to the derivative},
    'ch:derivative'
  )
%>

<% begin_sec("Review: functions and the slope of a linear function",nil,'functions') %>
Calculus is the study of rates of change, and of how change accumulates.
For example, figure \figref{stock-market} shows the changes in the United States
stock market over a period of 24 years. The $y$ axis of this graph is a certain
weighted average of the prices of stock, and the $x$ axis is time, measured in
years. This is an example of the concept of a mathematical \emph{function},\index{function}
which you've learned about in a previous course. We say that the stock index is a function
of time, meaning that it depends on time. What makes this graph the graph of a function is that
a vertical line only intersects it in one place. This means that at any given time, there is only
one value of the index, not more than one.

<% marg(0) %>
<%
  fig(
    'stock-market',
    %q{The S\&P 500 stock index is a function of time.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'rise-over-run',
    %q{Given two points on a line, we can find its slope by computing $\Delta y/\Delta x$,
       the rise over the run.}
  )  
%>
<% end_marg %>
Figure \figref{stock-market} shows a function that was determined by measurement and
observation, but functions can also be defined by a formula. For example, we could define
a function $y$ by stating that for any number $x$, the value of the function is given by $y(x)=x^2$.
We sometimes state this kind of thing more casually by referring to ``the function $y=x^2$''
or ``the function $x^2$.''

I drew figure \figref{stock-market} by graphing yearly data, so it's made of line segments that
connect one year to the next. Each of these line segments has a \emph{slope},\index{slope}
defined as
\begin{equation}
  \text{slope} = \frac{y_2-y_1}{x_2-x_1} \qquad .
\end{equation}
The slope measures how fast the function is changing. A positive slope says the function is increasing,
negative decreasing. If the slope is zero, the function is not changing at all.

It's often convenient to express this kind of thing with the notation $\Delta$, the capital Greek
letter delta, which is the equivalent of our Latin ``D'' and here stands for ``difference.'' In terms of this notation,
we have
\begin{equation}
  \text{slope} = \frac{\Delta y}{\Delta x} \qquad .
\end{equation}
A symbol like $\Delta y$ indicates the \emph{change} in $y$,
$\Delta y=y_2-y_1$. It doesn't
mean a number $\Delta$ multiplied by a number $y$.

<% end_sec('functions') %>

<% begin_sec("The derivative",nil,'derivative') %>
<% begin_sec("An informal definition of the derivative",nil,'informal-derivative') %>
In many real-world applications, it makes sense to think of change as occurring smoothly and
continuously. For example, the level of water in a reservoir rises and falls with time.
Although it's true that this change happens one molecule at a time, so that in theory
there are abrupt jumps, these jumps are too tiny to matter in practice.

<%
  fig(
    'reservoir',
    %q{The original graph, on the left, shows
       the water level in Trinity Lake, California, for the thirty-day period
       beginning March 7, 2014.
       Each successive magnification to the right is by a factor of four.
    },
    {
      'width'=>'fullpage'
    }
  )
%>

We want to keep track of the net rate of flow into the reservoir. We would
like to define this rate as the slope of the graph, but the graph isn't a line, so how do we
do that? We could pick two points on the graph and connect them with a line segment, but that
would only represent an average rate of flow, not the actual rate of flow as it
would be measured by a flow gauge at one particular time.

To get around these difficulties, we imagine picking a point of interest on the graph
and then zooming in on it more and more, as if through a microscope capable of unlimited
magnification. As we zoom in, the curviness of the graph becomes less and less apparent.
(Similarly, we don't notice in everyday life that the earth is a sphere.)
In figure \figref{reservoir}, we zoom in by 400\%, and then again by 400\%, and so on.
After a series of these zooms, the graph appears indistinguishable from a line, and we can
measure its slope just as we would for a line. This is an intuitive description of what we mean
by the slope of a function that isn't a line. We call this slope the
\emph{derivative}\index{derivative} of the function at the point of interest.
This is admittedly not a mathematically rigorous definition, but it fixes our minds on the
concept we want. A useful example is that if we consider a car's odometer reading as a function of
time in hours, then its speedometer reading is the derivative of the odometer reading.
<% marg(0) %>
<%
  fig(
    'reservoir-tangent-line',
    %q{The tangent line at a point on a curved graph.}
  )  
%>
<% end_marg %>

If we were only shown the ultra-magnified view in the rightmost part of figure \figref{reservoir},
we wouldn't know that the graph was curved at all. We would think the whole thing was a line.
This hypothetical line is called the \emph{tangent line}\index{tangent line}\label{tangent-line-informal}
at the point marked with a dot. When you stand on the earth's surface and look at a point on
the horizon, your line of sight is a tangent line to the surface. The derivative of a function is the slope
of the tangent line.

<% end_sec('informal-derivative') %>

<% begin_sec("Locality of the derivative",nil,'deriv-local') %>
From this informal definition it seems that the derivative
of a function at a certain point should depend only on the behavior of the function near that point, not
far away. To state this idea precisely, we need to use some notation referring to sets,
reviewed in box \figref{sets}, and intervals.

<% marginbox(300,'sets','Sets',{},
%q~
A \emph{set} is a collection of things. The things can, for example, be numbers. They can even be other sets.
A set can be defined by listing the things it holds, which are called its elements or members.
For example, the solutions of the equation $x^2=1$
are the members of the set $\{-1,1\}$. Often we deal with infinite sets such as the set of all the natural
numbers, and it is then impossible to list all the elements. Instead, we can
define a set using notation like this:
\begin{equation*}
  \zu{S} = \{x|x^2>0\} \qquad ,
\end{equation*}
read as, ``the set of all $x$ such that $x$ squared is greater than zero.'' Often, as in this example,
we don't explicitly say what to consider as the \emph{possible} values of $x$; since the focus of
calculus is on real numbers, the implication in this course 
is usually that ``the set of all $x$ such that \ldots''
means ``the set of all \emph{real numbers} $x$ such that \ldots''

The notation $\in$ means ``is a member of,'' e.g., $1\in\zu{S}$ for the set S defined above.

Two sets are the same if they have the same members. For example, let
\begin{align*}
  \zu{T} &= \{a|a^2>0\} \qquad \text{and}\\\\
  \zu{U} &= \{g|g\ne 0\} 
\end{align*}
Because S, T, and U have the same members, they are equal, $\zu{S}=\zu{T}=\zu{U}$.
      ~
   )
%>

Often it is useful to define a set of all the real numbers that lie within a certain range, between
numbers $a$ and $b$. This is called
an interval.\label{interval-notation}
We can define intervals that contain or don't contain their endpoints.

\begin{lessimportant}[Definition]
\begin{tabular}{lll}
\emph{type of interval} & \emph{definition} & \emph{abbreviation} \\
closed  & $\{x|x\ge a\ \text{and}\ x\le b\}$  & $[a,b]$\\
open    & $\{x|x> a\ \text{and}\ x< b\}$  & $(a,b)$
\end{tabular}
\end{lessimportant}
We can also have intervals like $[a,b)$ and $(a,b]$, which are defined in the obvious way.

\begin{lessimportant}[Locality of the derivative]
The derivative is \emph{local}, in the following sense. Suppose there is
an interval $\zu{I}=(a,b)$ on
which the functions $f$ and $g$ are equal. That is, for any $x\in \zu{I}$, $f(x)=g(x)$.
Then at any point in I, the derivatives of $f$ and $g$ are the same.
\end{lessimportant}

<%
  fig(
    'radar-gun',
    %q{Fred and Ginger are both driving on the freeway. As Ginger is about to pass Fred,
       she notices a motorcycle cop, so she abruptly decelerates and then
       stays alongside Fred. The derivative of their
       position is their speed. The derivative is local, so by the time
       the cop measures their speeds, at point P, they
       are the same.},
    {
      'width'=>'wide',
      'sidecaption'=>false
    }
  )
%>

<% end_sec('deriv-local') %>


<% begin_sec("Properties of the derivative",4,'properties-of-the-derivative') %>
The following properties of the derivative are intuitively reasonable
based on our conceptual definition, and they will be enough to allow us to do quite a bit
of interesting calculus before we come back and make a more rigorous definition.
<% marg(0) %>
<%
  fig(
    'properties-of-derivative',
    %q{Some properties of the derivative.}
  )  
%>
<% end_marg %>


\begin{description}\label{properties-of-derivative}
 \item[constant] The derivative of a constant function is zero.
 \item[line] The derivative of a linear function is its slope.
 \item[shift] Shifting a function $y(x)$ horizontally or vertically to form a new function 
                   $y(x+a)$ or $y(x)+b$
                   gives a derivative at any newly shifted point that is the same as
                   the derivative at the corresponding point on the unshifted graph.
 \item[flip] Flipping the function $y(x)$ horizontally or vertically to form a new function $y(-x)$ or
                   $-y(x)$ negates its derivative at corresponding points.
 \item[addition] The derivative of the sum of two functions is the sum
                   of their derivatives.
 \item[stretch] Stretching a function $y(x)$ vertically to form a new function $ry(x)$ multiplies its
                   derivative by $r$ at the corresponding points, while stretching it horizontally
                   to make $y(x/s)$ divides its derivative by $s$.
 \item[no-cut] Suppose that for a certain point P on the graph of a function, there is a unique linear function
                   $\ell$ that passes through P but doesn't cut through the graph at P. Then the graph of $\ell$
                   is the tangent line, and
                   the derivative of the function at P equals the slope of the line.\label{no-cut}
\end{description}

As an example of the stretch rule, cars sold in the U.S.~have odometers that read out in units of miles,
while those sold elsewhere are calibrated in kilometers, so their readings are greater by
the conversion factor $r=1.6$. By the stretch property, cars outside the U.S.~also have speedometer
readings that are greater by this factor: they read out in \emph{kilometers} per hour.

There is usually, but not always, a line like the one described by the no-cut property.
Sometimes there is a tangent line but it isn't a no-cut line. If this kind of mathematical
puzzle interests you, try sketching the 
graphs of the functions $x^3$ and $\sqrt{x}$. You should be able to convince yourself that
their tangent lines at $x=0$ can't be described by no-cut functions.

By the way, these are just names I've given to these properties, and if you use them with other
people, they won't know what you mean. Once we've done more calculus, we'll see that several
of these properties are actually special cases of a more general rule called the chain rule.

<% end_sec('properties-of-the-derivative') %>

<% begin_sec("The derivative of the function $y=x^2$",nil,'derivative-of-x-squared') %>

%%graph%% x-squared func=x**2 format=eps xlo=-3 xhi=3 ylo=0 yhi=10 with=lines

As our first example of a derivative, let's use the function $y=x^2$. Its graph is a parabola.
The simplest point at which
to find its derivative is $x=0$. Based on the graph, figure \figref{x-squared}, it seems like
zooming in more and more on this point would give something that looked more and more like a
horizontal line, and this suggests that the derivative at this point is zero. We can confirm this
by using the flip property. Flipping the graph horizontally doesn't change the graph. (Recall that
a function with this symmetry is called an \emph{even} function.)\index{even function}
Since the flip doesn't change the function, it can't change the derivative of the function, and
yet the flip rule says that when we do this the derivative should flip its sign. Zero is the
only number that remains the same when we flip its sign, so the derivative is zero.
<% marg(200) %>
<%
  fig(
    'x-squared',
    %q{The function $y=x^2$.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'tangent-line',
    %q{The line $2x-1$ intersects the function $x^2$ without cutting it.}
  )  
%>
<% end_marg %>

How about the derivative at the point $x=1$? Here we can apply the no-cut rule.
By laying a ruler against this point, we find that the linear function $\ell(x)=2x-1$ seems
to intersect the parabola without cutting across it. To prove that this is
true, we can compute the difference between the two functions, $y(x)-\ell(x)=x^2-2x+1$.
Completing the square allows us to rewrite this as $(x-1)^2$, which is clearly positive for
any value of $x$ other than 1. Therefore the function $\ell$ meets the conditions of the
no-cut rule, and the derivative of $x^2$ at $x=1$ is 2.

%%graph%% tangent-line func=x**2 format=eps xlo=0 xhi=2 ylo=0 yhi=2 with=lines x=t y=x samples=300 ytic_spacing=1 ; func=2*x-1.0 with=lines

Having found the derivative of $x^2$ at $x=1$, we can now use the stretch rule to find it
at any other point. For example, suppose we want to know the derivative at $x=3$. If we were to
take the graph of the function $x^2$ and stretch it by a factor of $3$ horizontally and $9$ vertically, we would
get the same graph again. These stretches take the point $(1,1)$, where we know the derivative,
to the point $(3,9)$, where we want to know it.
The stretch rule tells us that the horizontal stretch decreases the
derivative to $1/3$ of its original value, but the vertical stretch increases it by $9$ times,
so that over all, the derivative at $(3,9)$ is $(1/3)(9)=3$ times greater than its value
at $(1,1)$. Thus the derivative at $x=3$ equals 6.

There is nothing special about the number 3. 
The method that we applied to $x=3$ would work for any other number $x$, not just for 3.
We find that the derivative of the function $x^2$ at any point $x$ equals $2x$. Taking stock of
what we've done, we started with the function $x^2$, and found that at any point $x$, the derivative
was $2x$.
<% end_sec('derivative-of-x-squared') %>
<% begin_sec("The derivative of a function is a function itself.",4,'derivative-is-a-function') %>
We've found that the derivative of the function $x^2$ at a point $x$ equals $2x$.
The expression $2x$ can be thought of as a function of $x$. So what we've really done
is to take a function and construct a new function that gives the derivative of the original
function at each point. One way of notating this new function is $y'$, read ``$y$ prime.'' We have
\begin{align*}
  y &= x^2 \\
  y' &= 2x \qquad .
\end{align*}
The craft of finding this kind of derivative-function from the original function is called
\emph{differentiation}. We have differentiated the function $x^2$ and gotten its derivative,
the function $2x$.
<% marg(200) %>
<%
  fig(
    'derivative-as-a-function',
    %q{The derivative of $x^2$ is itself a function. As we change $x$, the slope of the tangent line
       changes.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'baldy-elevation-graph',
    %q{Example \ref{eg:baldy}.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'indifference-curve',
    %q{Example \ref{eg:indifference-curve}.}
  )  
%>
<% end_marg %>

\begin{eg}{Hiking}\label{eg:baldy}
Figure \figref{baldy-elevation-graph} shows a graph of my favorite route for climbing a mountain
near where I live. (My wife rolls her eyes when I tell her the dog and I are doing this hike yet
again.) How steep is the hike? There is no generic answer to this question, since the derivative
of this function is itself a function. The derivative depends on $x$, so it has different values in
different places. The slope of the graph at point P appears to be the steepest, with
$y'\approx 0.80$. At other points, $y'$ has smaller values. At Q, it's slightly negative.
The derivative $y'$ is a \emph{function} of $x$; it depends on which part of the hike you're presently
climbing.
\end{eg}

\begin{eg}{An indifference curve}\label{eg:indifference-curve}
Let's say you enjoy beer, and you also enjoy sushi. How much would you prefer to have of each?
Economists define a graph, figure \figref{indifference-curve}, called an \emph{indifference curve}.
For a particular person, any two points on the curve are supposed to be equal in preference;
the person is indifferent as to which one they get. For example, the person whose indifference
curve is drawn in figure \figref{indifference-curve} is equally happy having one piece of sushi
and five beers, or having three pieces of sushi and two beers. 

There is a quantity called the marginal rate of substitution (MRS), which is defined as minus the slope
of the indifference curve, $-y'$. At point P in figure \figref{indifference-curve}, the MRS is
high, which means that the person would have been just as happy to have another piece of
sushi and a \emph{lot} less beer. The MRS, $-y'$, is a \emph{function} of where you are on the
curve. If the person is at point Q on the graph, they have a moderate amount of beer and a moderate
amount of sushi, so they consider them of more comparable value.
Indifference curves are discussed further in section \ref{subsec:indifference-curves},
p.~\pageref{subsec:indifference-curves}.
\end{eg}

\pagebreak

\begin{eg}{What if $x$ is in the exponent rather than the base?}\label{eg:bunnies}
The method used above to differentiate $x^2$ was basically a trick, and it depended on a special
property of the function $x^2$, which is that its graph can be stretched horizontally and vertically in
such a way that it can be brought back on top of itself again. The reason that this subject is
called ``calculus'' rather than ``trickery'' is that we will soon (in ch.~\ref{ch:limits})
develop more systematic
methods for \emph{calculating} rates of change --- methods that don't depend on tricks.

It may nevertheless be of interest to note that a similar trick is capable of telling us
something about a different type of function,
one in which $x$ appears in the exponent rather than the base. What about the function $2^x$, for example?
A pair of rabbits marches off of Noah's ark. Two bunnies become four, then 8, 16, 32, and so on.
What is the derivative of this function, i.e., the rate of change of the rabbit population per
generation? (Strictly speaking, the derivative is only meaningful if we fill in all the non-integer
values of $x$, which isn't really meaningful in terms of rabbits, since you can't have a fraction of
a rabbit.)

It happens that the function $2^x$, like $x^2$, can be brought back on top of itself again in a simple
geometrical way. Instead of a horizontal stretch and a vertical stretch, we use a horizontal shift
and a vertical stretch. For example, if we shift the graph of $2^x$ to the right by 3 units, and then
stretch it vertically by a factor of 8, we get back the same graph again. This has come about because of the
more fundamental property of exponential functions $b^{c+d}=b^cb^d$. (In our example, the base $b$ is 2.)
As a result, we find that after
3 generations, when the rabbit population goes up by a factor of 8, its derivative \emph{also} goes
up by a factor of 8. That is, the derivative of an exponential function $y=b^x$ is proportional
to $y$, or
\begin{equation*}
  y'=(\ldots)y \qquad ,
\end{equation*}
where ``\ldots'' is a constant of proportionality that depends on the base $b$. What is the constant
of proportionality? We'll return to this question in example \ref{eg:exponential-with-limits}
on p.~\pageref{eg:exponential-with-limits}.

A similar example is
credit card debt. The more credit card debt you have, the faster your debt grows; in this example, the
constant of proportionality relates to the interest rate.
\end{eg}
<% end_sec('derivative-is-a-function') %>
<% end_sec('derivative') %>

<% begin_sec("Derivatives of powers and polynomials",4,'deriv-polynomial') %>
In section \ref{subsec:derivative-of-x-squared}, we found that the derivative of $x^2$ was $2x$.
Straightforward application of the same technique to $x^3$ gives $3x^2$. We see a pattern:

\begin{important}[Derivatives of powers]
The derivative of $x^n$ equals $nx^{n-1}$, if $n$ is any integer greater than or equal to 1.
\end{important}

\noindent To prove this for all these values of $n$, rather than carrying out the proof for one value at
a time, it will be more convenient to use techniques developed later in the book
(section \ref{sec:power-rule-general}, p.~\pageref{sec:power-rule-general}).

If we combine this with the addition and stretch rules, we know enough to differentiate any polynomial.
<% marg(-50) %>
<%
  fig(
    'derivative-of-a-polynomial',
    %q{Example \ref{eg:derivative-of-a-polynomial}. The top graph shows the original function, the
       bottom its derivative.
       }
  )  
%>
<% end_marg %>

\begin{eg}{Differentiating a polynomial}\label{eg:derivative-of-a-polynomial}
\egquestion Find the derivative of $y'=x^3-7x+1$.

\egquestion The addition property of the derivative
tells us that we can break this problem down into three parts,
\begin{equation*}
  (x^3-7x+1)' = (x^3)'+(-7x)'+(1)' \qquad ,
\end{equation*}
where the primes indicate ``derivative of \ldots'' The stretch property says that $(-7x)'$ is the same
as $(-7)(x)'$, so the derivative of our polynomial becomes
\begin{equation*}
   (x^3)'+(-7)(x)'+(1)' \qquad .
\end{equation*}
We know how to differentiate powers: $(x^3)'=3x^2$, $(x')=1$, and $(1)'=0$. (We could have found the
second term from the line property, and the final one from the constant property.) The result is
\begin{equation*}
   y' = 3x^2-7 \qquad .
\end{equation*}

The functions $y$ and $y'$ are graphed in figure \figref{derivative-of-a-polynomial}, and
five points are marked as examples of how the \emph{slope} of $y$ corresponds
to the \emph{value} of $y'$. Reading across from left to right on the top graph, the slopes are
positive, zero, negative, zero, and positive. On the bottom graph, the values of $y'$ are
easily seen to be positive, zero, negative, zero, and positive.
\end{eg}

%%graph%% polynomial-to-differentiate func=x**3-7*x+1 format=svg xlo=-3 xhi=3 ylo=-10 yhi=10 xtic_spacing=1 ytic_spacing=5 with=lines
%%graph%% deriv-of-polynomial func=3*x**2-7 format=svg xlo=-3 xhi=3 ylo=-10 yhi=10 xtic_spacing=1 ytic_spacing=5 with=lines

<% end_sec('deriv-polynomial') %>

<% begin_sec("Two trivial hangups",nil,'hangups') %>
<% begin_sec("Changing letters of the alphabet",nil,'changing-letters') %>
The following point is relatively trivial, but nevertheless hangs up many students
in applying calculus to real life. In a calculus
textbook, we typically use the letters $x$ and $y$, with $y$ being a function of $x$. That is, $x$ is the independent variable,
and $y$ is the dependent one. In real-life applications, however, the variables have definite meanings, and we want to
use letters that make it easy to remember what they stand for. 

For example, suppose that a social media company has a certain
number of users, and they need to have enough computing power at their data center to be able to handle all of those users.
This computing power will cost them a certain amount of money per month. In this
example, it would be natural to use the notation $u$ for
the number of users, and $c$ for the monthly cost in dollars. Then $c$ depends on $u$, and we have a function $c(u)$. Let's
say the function is this:
\begin{equation*}
  c = u^2
\end{equation*}
This is not an unrealistic equation to imagine for this example, since the company has to keep track of every user's
relationship to every other user. For example, user Andy may be able to mark himself
as a ``fan'' or ``follower'' of user Betty, and then the company has to store a piece
of information in a database to record this relationship.
If there are
a thousand users, there are $1000\times 1000$ or a million such possible
relationships that may need to be stored in a database.

Now if the company's user base is growing, it's of interest to them to know how much their costs will go up for each
additional user (the marginal cost). This would be expressed by the derivative $c'(u)$. Although the letters of the alphabet are different
than the ones we used in our earlier examples, that makes no difference in how we do the math. If differentiating
$y=x^2$ with respect to $x$ gives $y'=2x$, then differentiating $c=u^2$ with respect to $u$ gives the same result but
with the letters changed,
\begin{equation*}
  c' = 2u
\end{equation*}
<% end_sec('changing-letters') %>
<% begin_sec("Symbolic constants",nil,'symbolic-constants') %>
The vertical stretch property of the derivative tells us that if we know a derivative such as
\begin{equation*}
  (x^2)'=2x \qquad ,
\end{equation*}
then we can differentiate a function like $5x^2$
by simply letting the factor of 5 ``come along for the ride,'' 
\begin{align*}
  (5x^2)' &= (5)(x^2)' \\
          &= (5)(2x) \\
          &= 10x \qquad .
\end{align*}
Now suppose that we want to differentiate
$bx^2$, where $b$ is a constant, i.e., $b$ doesn't depend on $x$. To many students this looks like a much
more difficult and abstract problem, but the procedure is the same:
\begin{align*}
  (bx^2)' &= (b)(x^2)' \\
          &= (b)(2x) \\
          &= 2bx \qquad .
\end{align*}
The same goes for a vertical shift. If we aren't intimidated by computing
\begin{equation*}
  (x^2+5)' = (x^2)' = 2x \qquad ,
\end{equation*}
then there is no reason to be scared of the similar computation (again with
$b$ being a constant) of
\begin{equation*}
  (x^2+b)' = (x^2)' = 2x \qquad .
\end{equation*}
<% end_sec('symbolic-constants') %>
<% end_sec('hangups') %>

<% begin_sec("Applications",nil,'apps-of-derivative') %>

<% begin_sec("Velocity",nil,'velocity') %>
<% begin_sec("Defining velocity",nil,'defining-velocity') %>
One of our prototypical examples has been the odometer and speedometer on a car's dashboard. In fact, if we want to \emph{define}
what velocity means, we have to define it as a derivative. Suppose an object (it could be a car, a galaxy, or a subatomic
particle) is moving in a straight line. By choosing a unit of distance and a location that we define as zero, we can superimpose
a number line onto this line. (In the example of the car, the unit of distance might be kilometers, and the zero
position would be the point on the road at which we last pushed the button to zero the odometer.) Let the position
defined in this way be $x$. Then $x$ is a function of time $t$ (such as the time measured on a clock), and we notate this
function as $x(t)$. Note that although we typically use the letters $x$ and $y$ in a generic mathematical context, with
$y$ being a function of $x$, in our present example it is more natural to use different letters, and now $x$ is the
\emph{dependent} variable, not the independent one. That is, $x$ is a function of $t$, but $t$ may not be a function of
$x$; for example, if a car stops and backs up, then it can visit the same position twice, so that a graph of $t$ versus
$x$ would fail the vertical line test for a function. In this notation, the velocity $v$ is defined as the
derivative 
\begin{equation*}
  v(t) = x'(t) \qquad .
\end{equation*}
<% end_sec('defining-velocity') %>
<% begin_sec("Constant acceleration",nil,'constant-acceleration') %>
An important special case is the one in which the position function is of
the form
\begin{equation*}
  x(t) = \frac{1}{2}at^2 \qquad ,
\end{equation*}
where $a$ is a constant, and the factor of $1/2$ is conventional, and convenient for
reasons that will become more apparent in a moment. Differentiating with respect to
$t$, we have the velocity function
\begin{equation*}
  v(t) = at \qquad ,
\end{equation*}
where the symbolic constant $a$ has been treated like any other constant,
and the $1/2$ in front has been canceled by the factor of 2 that comes down from the
exponent. We see that the velocity is proportional to the amount of time that has passed.
If $t$ is measured in seconds and $v$ in meters per second (m/s), then the constant $a$,
called the \emph{acceleration},\index{acceleration} tells us how much speed the object
gains with every second that goes by, in units of m/s/s, which can be written as $\munit/\sunit^2$.
Falling objects have an acceleration of about $9.8\ \munit/\sunit^2$. This is a measure of
the strength of the earth's gravity near its own surface.

\begin{eg}{Dropping a rock down a well}
\egquestion Looking down into a dark well, you can't see how deep it is. If you drop a rock
in and hear it hit the bottom in 2 seconds, how deep is the well?

\eganswer 
\begin{equation*}
  x(t) = \frac{1}{2}at^2 \approx 20\ \munit
\end{equation*}
\end{eg}

\begin{eg}{The shift property applied to constant acceleration}
The equations for constant acceleration were given above with the unstated assumption that
both the position and the velocity would be zero at the time $t=0$. If we relax this assumption, then
the position function can be of the more general form
\begin{equation*}
  x(t) = x_\zu{o}+\frac{1}{2}a(t-t_\zu{o})^2 \qquad ,
\end{equation*}
where $t_\zu{o}$ is some initial time, at which the position equals $x_\zu{o}$. By the shift property of the
derivative (p.~\pageref{properties-of-derivative}), the velocity function is then
\begin{equation*}
  v(t) = a(t-t_\zu{o}) \qquad .
\end{equation*}
\end{eg}
<% end_sec('constant-acceleration') %>
<% end_sec('velocity') %>

<% begin_sec("When do you need a derivative?",nil,'when-derivative-is-needed') %>
Finding velocity from position data is a classic application of calculus, and yet how
do we know when we really need calculus for this application? After all, many people
do simple computations involving velocity without knowing calculus.

Here's an example where calculus really is required.
In July 1999, Popular Mechanics carried out tests to
find which car sold by a major auto maker could cover a
quarter mile (402 meters) in the shortest time, starting
from rest. Because the distance is so short, this type of
test is designed mainly to favor the car with the greatest
acceleration, not the greatest maximum speed (which is
irrelevant to the average person). The winner was the Dodge
Viper, with a time of 12.08 s. If we divide the distance by the time, we get
\begin{equation*}
  v = \frac{\Delta x}{\Delta t} = 33.3\ \munit/\sunit \qquad ,
\end{equation*}
which is about 74 miles per hour or 120 kilometers per hour. Not a very impressive
speed, is it? That's because it's wrong. During those twelve seconds of acceleration,
the car didn't have just one speed. It started at a velocity of zero and went up from
there. The top speed was nearly double the one calculated above
($53\ \munit/\sunit\approx 119\ \zu{mi}/\zu{hr}\approx 191\ \zu{km}/\zu{hr}$). The important
point here is that when we measure a rate of change using an expression of the form
\begin{equation*}
  \frac{\Delta\ldots}{\Delta\ldots} \qquad ,
\end{equation*}
we only get the right answer if the rate of change is \emph{constant}. In this example
the rate of change is the velocity, and the velocity is not constant. To find the correct velocity,
we first need to decide at which time we want to know the velocity, and then evaluate the derivative
at that time.
<% end_sec('when-derivative-is-needed') %>

<% begin_sec("Optimization",nil,'optimization') %>
An extremely important use of the derivative is in optimization. For example, suppose that
the operators of a privately owned mountain tram in Switzerland want to optimize
their profit from transporting sightseers to a mountain summit in the Alps. The cost of
building the tram is a sunk cost, and operating it for one day costs the same amount of money regardless
of the number of passengers. Therefore the only goal is to get the maximum number of Swiss
francs in the cash registers at the end of each day. The operators can raise the fare $f$ in order
to make more money, but if the fare is too high then not as many people will be willing to pay it.
Suppose that the number of riders in a given day is given by $a-bf$, where $a$ and $b$ are constants.
That is, if the ride was free, $a$ passengers would ride each day, but for every one-franc
increase in the fare, $b$ people will decide not to go. The tram's daily revenue is then found
by multiplying the number of riders by the fare, which gives
the function
\begin{equation}\label{eqn:tram-revenue}
  r(f) = (a-bf)f \qquad .
\end{equation}

For insight into what's going on, figure \figref{tram-fare-edited} shows this function in the case where $a=100$ and
$b=1$. When the fare is zero, we get plenty of customers every day, but they don't pay anything,
so our revenue is zero. When the fare is 100 francs, the number of paying passengers goes down
to zero, so again we have no revenue. 
<% marg(0) %>
<%
  fig(
    'tram-fare-edited',
    %q{Revenue from a tram as a function of the fare charged.}
  )  
%>
<% end_marg %>

Somewhere in between these extremes  we have the fare that would optimize our
revenue: the \emph{maximum} of the function $r$. At this point on the graph, the derivative is zero,
so to find it, we should differentiate $r$, set it equal to zero, and solve for $f$.

We haven't yet learned enough of the techniques of calculus to
know how to find the derivative of a function with the form of equation
\eqref{eqn:tram-revenue}, but by multiplying out the product we can make it into a polynomial,
which is a form that we do know how to differentiate:
\begin{align*}
  r(f) &= -bf^2+af \\
  r'(f) &= -2bf+a
\end{align*} 
Setting $r'$ equal to zero, we have
\begin{align*}
  0 &= -2bf+a \\
  f&=\frac{a}{2b} \qquad .
\end{align*} 
With the particular numerical values used to construct the graph, this gives an optimal
fare of 50 francs, which looks about right from the graph.

By searching for points where the derivative is zero we can often, but not always, find
the the points where a function takes on its maximum and minimum values. The term extremum
(plural extrema) is used to refer to these points. Figure \figref{cases-of-extrema} shows that
quite a few different things can happen, and that searching for a zero derivative doesn't
always tell us the whole story. We have a zero derivative at point G, but G is only a maximum
compared to nearby points; we call G a local maximum, as opposed to the global maximum D.
The zero-derivative test doesn't distinguish a local minimum like B from a local maximum.
A zero derivative may not indicate a local extremum at all, as at C and H. We can have points
such as E and F where the derivative is undefined. An extremum can occur at a point like A
that is the endpoint of the function's domain.\footnote{For a more thorough review of notions such as the
domain of a function, see section \ref{sec:inverse-of-function}, p.~\pageref{sec:inverse-of-function}.}
We will come back to these technical points in more detail later in the 
book.\footnote{section \ref{subsec:second-derivative-for-extrema},
p.~\pageref{subsec:second-derivative-for-extrema}}
<% marg(80) %>
<%
  fig(
    'cases-of-extrema',
    %q{A zero derivative often, but not always, indicates a local extremum. Sometimes we have
       a zero derivative without a local extremum, and sometimes a local extremum with an
       undefined or nonzero derivative.}
  )  
%>
<% end_marg %>

%%graph%% tram-fare func=(100.0-x)*x x=f y=r format=svg xlo=0 xhi=100 ylo=0 yhi=2500 xtic_spacing=20 ytic_spacing=1000 with=lines

<% end_sec('optimization') %>
<% end_sec('apps-of-derivative') %>

<% begin_sec("Review: elementary properties of the real numbers",nil,'elementary-reals') %>

I began this chapter by defining calculus as the study of rates of change, but
it could equally well be described as the study of infinity. The intuition behind the derivative
is that we zoom in on a selected point on a smooth curve, until the curve appears like a line
and we can measure the slope of the line. But the curve won't appear perfectly
straight until we've cranked up our microscope to an \emph{infinitely big} magnification, at which point
we'll be seeing values of $\Delta x$ and $\Delta y$ that are \emph{infinitely small} (but not zero).
Calculus was invented by Isaac Newton and
Gottfried Wilhelm von Leibniz back in the era of powdered wigs and silk stockings, and
in those days the concept of number hadn't been formalized. Newton and Leibniz simply assumed
that it was all right to have infinitely big and infinitely small numbers. But modern
mathematicians approach this kind of thing differently. They think in terms of specific,
well-defined sets of numbers, including:

\begin{description}
\item[the integers:] whole numbers such as $-1$, 0, and $1$
\item[the rational numbers:] ratios of integers such as $2/1$ and $3/4$
\item[the real numbers,] including quantities like $\pi$ and $\sqrt{2}$
\item[the complex numbers,] such as $\sqrt{-1}$
\end{description}

Do these systems include infinitely big and infinitely small numbers? Can they? Should they?


To answer these questions, we need to give a more definite account of how these number systems
are defined. A good way to define them is with a list of their axioms.
Here is a list of axioms for the system of real numbers. Except as otherwise stated,
each of these properties holds for \emph{any} real-number values of the symbols $x$, $y$, \ldots

\begin{description}
\item[commutativity] $x+y=y+x$ and $xy=yx$
\item[identities] There exist numbers 0 and 1 such that for any $x$, $x+0=x$ and $1x=x$.
\item[inverses] For any $x$, there exists a number $-x$ such that $x+(-x)=0$. For any nonzero $x$,
          there exists $1/x$ such that $(x)(1/x)=1$.
\item[associativity] $x+(y+z)=(x+y)+z$ and $x(yz)=(xy)z$
\item[distributivity] $x(y+z)=xy+xz$
\item[ordering] We can define whether or not $x<y$, and this ordering relates to the addition and
      multiplication operations in specific ways, which you've seen defined in a previous course
      on algebra and which for brevity we will not explicitly give here.
\end{description}

This list of axioms holds for the real numbers, but it fails for the integers, since for example
the integer 2 doesn't have an inverse that is an integer. It also fails for the complex numbers,
which don't have a well-defined ordering. The list seems complete and precise, so it may come
as a surprise that it does \emph{not} suffice to prove anything about whether or not infinite
numbers exist. The list of axioms is in fact incomplete as a characterization of the real numbers.
Later in this book we will add another axiom, called the completeness axiom,\index{completeness axiom}
to the list. The completeness axiom holds for the reals but not the rationals, and it also
rules out the existence of infinitely large or infinitely small real numbers.

<% end_sec('elementary-reals') %>

<% begin_sec("The Leibniz notation",nil,'leibniz-notation') %>
For better or for worse, this kind of thing wasn't a concern in Leibniz's era.
With what seems like kittenish na\"ivet\'e to a modern mathematician, he reasoned as follows.
Let's just make $\Delta x$ and $\Delta y$ infinitely small (but not zero). In modern terminology,
this means that they can't be real numbers. To make it clear that we're talking about infinitely small
differences in $x$ and $y$, we change the notation to $\der x$ and $\der y$. Recall that $\Delta$
is the Greek version of capital ``D,'' so we're using a smaller version of the letter, ``$\der$,''
to represent a change that is smaller (in fact, infinitely small). Dividing these two
``numbers'' (whatever mysterious species of number they may turn out to be), we get the
derivative,
\begin{equation*}
  \frac{\der y}{\der x} \qquad .
\end{equation*}
Although the notation's original justification was not up to modern standards of
rigor, it is one of the most expressive and well-designed mathematical notations ever devised,
and has been the most commonly used notation for the derivative ever since Leibniz published it
in 1686.
<% marg(80) %>
<%
  fig(
    'leibniz-portrait',
    %q{Gottfried Wilhelm Leibniz (1646-1716).}
  )  
%>
<% end_marg %>

One of the good things about the Leibniz notation is that it states clearly what we're differentiating
\emph{with respect to}. For example, $\der v/\der t$ could indicate how much a car was speeding up
with each passing second of time, while $\der v/\der x$ would measure the speed gained with
each meter that it moved down the road.

Another selling point of the notation is that it shows the units of the derivative. For example,
the definition of velocity, expressed in Leibniz notation, is
\begin{equation*}
  v = \frac{\der x}{\der t} \qquad .
\end{equation*}
On the left-hand side we have velocity, whose units in the SI are meters per second. On the
right we have a tiny change in position, which has units of meters, divided by a tiny change
in time, which has units of seconds. In terms of units, then, the equation reads as
\begin{equation*}
  \zu{m/s} = \frac{\munit}{\sunit} \qquad ,
\end{equation*}
which works out correctly. In more complicated examples, checking the units like this is a powerful
method for checking your answer to a calculus problem.

\begin{eg}{An insect pest}\label{eg:pest}
\egquestion An insect pest from the United States is inadvertently released in
a village in rural China. The pests spread outward at a rate of $s$ kilometers
per year, forming a widening circle of contagion. Find the number of square
kilometers per year that become newly infested. Check that the units of the result
make sense. Interpret the result.

\eganswer Let $t$ be the time, in years, since the pest was introduced.
The radius of the circle is $r=st$, and its area is $a=\pi r^2=\pi(st)^2$.
To make this look like a polynomial, we have to rewrite it as
$a=(\pi s^2)t^2$. The derivative is
\begin{align*}
  \frac{\der a}{\der t} &= (\pi s^2)(2t) \\
                        &= (2\pi s^2) t
\end{align*}
The units of $s$ are km/year, so squaring it gives $\zu{km}^2/\zu{year}^2$.
The 2 and the $\pi$ are unitless, and multiplying by $t$ gives units
of $\zu{km}^2/\zu{year}$, which is what we expect for $\der a/\der t$, since
it represents the number of square kilometers per year that become infested.

Interpreting the result, we notice a couple of things. First, the rate
of infestation isn't constant; it's proportional to $t$, so people might not
pay so much attention at first, but later on the effort required to combat the
problem will grow more and more quickly. Second, we notice that the
result is proportional to $s^2$. This suggests that anything that could be
done to reduce $s$ would be very helpful. For instance, a measure that cut
$s$ in half would reduce $\der a/\der t$ by a factor of four.
\end{eg}

Sometimes the Leibniz notation gives an unwieldy, top-heavy tower of symbols:
\begin{equation*}
  \frac{\der\left(\frac{x^2}{2}+\frac{1}{7}\right)}{\der x} = x
\end{equation*}
One way to avoid this awkwardness is to revert to the ``prime'' notation:
\begin{equation*}
  \left(\frac{x^2}{2}+\frac{1}{7}\right)' = x
\end{equation*}
But a more common solution is write the function being differentiated over on the right:
\begin{equation*}
  \frac{\der}{\der x}\left(\frac{x^2}{2}+\frac{1}{7}\right) = x
\end{equation*}
This can be seen simply as a typographical expedient, or it can be given
a mathematical interpretation: we can think of $\frac{\der}{\der x}$ as
meaning ``take the derivative of,'' in the same way that $\sqrt{\quad}$ means
``take the square root of.'' We call $\frac{\der}{\der x}$ the
\emph{operator} describing the operation of taking a function
and giving back the function that is its derivative. Math teachers who dislike the historical
connotations of the Leibniz notation in terms of infinitely small numbers
will sometimes present the operator interpretation as the \emph{only} correct interpretation, but
such a prescription robs the student of some of the utility of the notation,
e.g., by making it impossible to do the kind of reasoning shown in example \ref{eg:pest}.

<% end_sec('leibniz-notation') %>

<% begin_sec("Approximations",nil,'approximations') %>
We saw in section \ref{subsec:when-derivative-is-needed} on p.~\pageref{subsec:when-derivative-is-needed}
that the derivative can't be calculated as $\Delta y/\Delta x$ unless the derivative is constant, i.e., unless
the function's graph is a line. In the Leibniz notation, this is
\begin{equation*}
  \frac{\der y}{\der x} \ne \frac{\Delta y}{\Delta x} \qquad .
\end{equation*}
But if we take two points very close together on a graph, then the curvature doesn't matter too much,
and the line through those points is a good approximation to the tangent line, as in figure
\figref{approximating-tangent-line}. When then have the approximation
\begin{equation*}
  \frac{\der y}{\der x} \approx \frac{\Delta y}{\Delta x} \qquad .
\end{equation*}
It may be of interest to use either side of this as an approximation to the other.
<% marg(0) %>
<%
  fig(
    'approximating-tangent-line',
    %q{The dotted line through P and Q is a good approximation to the tangent line through P.}
  )  
%>
<% end_marg %>

<% begin_sec("Approximating the derivative",nil,'approximating-deriv') %>
Suppose you can't remember that the derivative of $x^2$ is $2x$, but you need to find the
value of the derivative at $x=1$. As in figure \figref{approximating-tangent-line}, let point P
be
\begin{equation*}
  (1.0000,1.0000) \qquad ,
\end{equation*}
and let Q be the nearby point
\begin{equation*}
  (1.0100,1.0201) \qquad .
\end{equation*}
We then have:
\begin{align*}
  \frac{\der y}{\der x} & \approx \frac{\Delta y}{\Delta x} \\
           &= \frac{1.0201-1.0000}{1.0100-1.0000} \\
           &= \frac{0.0201}{0.0100} \\
           &= 2.01
\end{align*}
This is quite a good approximation to the exact answer, 2. If we needed a better approximation,
we could take Q even closer to P. In reality we would use this technique in cases where we didn't
know the exact answer, and we would then want to know how accurate our result was. To do this,
we could redo the calculation with a smaller value of $\Delta x$, say 0.001, and look for the
most significant decimal place that changed.
<% end_sec('approximating-deriv') %>
<% begin_sec("Approximating finite changes",nil,'approximating-changes') %>
Sometimes we know the derivative and want to use it as an approximation to find out about
finite changes in the variables. 
For example,
the Women's National Basketball Association says that balls used in its games should have
a radius of 11.6 cm, with an allowable range of error of plus or minus 0.1 cm (one millimeter).
How accurately can we determine the ball's volume?

The equation for the volume of a sphere gives $V=(4/3)\pi r^3=6538\ \zu{cm}^3$ (about six and
a half liters). We have a function $V(r)$, and we want to know how much of an effect will be
produced on the function's output $V$ if its input $r$ is changed by a certain small amount.
Since the amount by which $r$ can be changed is small compared to $r$, it's reasonable to
apply the approximation
\begin{equation*}
  \frac{\Delta V}{\Delta r} \approx \frac{\der V}{\der r} \qquad ,
\end{equation*}
which gives
\begin{align*}
  \Delta V & \approx \frac{\der V}{\der r}\Delta r \\
           &= 4\pi r^2 \Delta r \qquad .
\end{align*}
(Note that the factor of $4\pi r^2$ can be interpreted as the ball's surface area.)
Plugging in numbers, we find that the volume could be off by
as much as $(4\pi r^2)(0.1\ \zu{cm})=170\ \zu{cm}^3$. The volume of the ball can therefore
be expressed as $6500\pm 170\ \zu{cm}^3$, where the original figure of 6538 has been rounded off
to the nearest hundred in order to avoid creating the impression that the 3 and the 8 actually
mean anything --- they clearly don't, since the possible error is out in the hundreds' place.
<% marg(100) %>
<%
  fig(
    'basketball',
    %q{How accurately can we determine the ball's volume?}
  )  
%>
<% end_marg %>

This calculation is an example of a very common situation that occurs in the sciences, and even
in everyday life, in which we base a calculation on a number that has some range of uncertainty
in it, causing a corresponding range of uncertainty in the final result. This is called
propagation of errors. The idea is that the derivative expresses how sensitive the function's
output is to its input.

The example of the basketball could also have been handled without calculus, simply by recalculating the volume
using a radius that was raised from 11.6 to 11.7 cm, and finding the difference between the two volumes.
Understanding it in terms of calculus, however, gives us a different way of getting at the same
ideas, and often allows us to understand more deeply what's going on. For example, we noticed in passing that
the derivative of the volume was simply the surface area of the ball, which provides a nice geometric
visualization. We can imagine inflating the ball so that its radius is increased by a millimeter. The amount of
added volume equals the surface area of the ball multiplied by one millimeter, just as the amount of volume
added to the world's oceans by global warming equals the oceans' surface area multiplied by the added depth.

As another example of an insight that we would have missed if we hadn't applied calculus,
consider how much error is incurred in the measurement of the width of a book if the ruler is
placed on the book at a slightly incorrect angle, so that it doesn't form an angle of exactly 90 degrees with
spine. The measurement has its minimum (and correct) value if the ruler is placed at exactly
90 degrees. Since the function has a minimum at this angle, its derivative is zero. That means that
we expect essentially no error in the measurement if the ruler's angle is just a tiny bit off.
This gives us the insight that it's not worth fiddling excessively over the angle in this measurement.
Other sources of error will be more important. For example, is the book a uniform rectangle? Are we using
the worn end of the ruler as its zero, rather than letting the ruler hang over both sides of the book
and subtracting the two measurements?
<% end_sec('approximating-changes') %>
<% begin_sec("Linear approximation to a curve",nil,'linear-approximation-to-a-curve') %>
Many people who, like me, learned to drive in the United States
were taught that when following another car, we should leave space equal to one car length for
every 10 miles per hour of speed. For example, at a freeway speed of 60 mi/hr, we would follow no
more closely than 6 car lengths. In a 2007 study, the Danish company Trafitec hired professional
test drivers\footnote{Nonprofessional drivers were also
tested, and took about 10-40\% more distance to stop, mainly because they didn't press as hard
on the brake pedal.} and measured how much distance $x$ they needed in order to stop on a test track, starting
from an initial velocity $v$. If we convert their speeds into U.S. units of mi/hr and the distances into units
of car lengths,\footnote{I used the length of a Honda Accord, which is 4.8 meters.} some sample numbers
are as follows:

\begin{tabular}{ll}
$v$ (mi/hr) & $x$ (car lengths) \\
60          & \hspace{2mm}8.8 \\
70          & 11.9
\end{tabular}

% calc -e "c=1000/3600; u=.0044; a=c^2/(2u)"
% ... 8.77 m/s2
% calc -e "v=96.6; .0044v^2+.0129v; ~/4.8"
%    8.8135425
% calc -e "v=112.7; .0044v^2+.0129v; ~/4.8"
%    11.9457304166667

The traditional rule does not allow enough distance to stop at either of these speeds.
Furthermore, the stopping distance is increasing much faster with speed than the rule
would have suggested: about 3 car lengths per 10 mi/hr.
The study found that the stopping distances were well approximated by a function
of the following form:\footnote{They also included a linear term, which relates to the
drivers' reaction times. This turns out to be negligible for our present purposes.}
\begin{equation}\label{eqn:stopping-distance}
  x = k v^2
\end{equation}

The coefficient
$k$ is related to the maximum acceleration (or in this case, deceleration) that the drivers
could achieve, which is limited by the friction between the tires and the road. In the same
units used in the table above, $k$ was about $2.4\times10^{-3}$.

% calc -e ".0044(1/4.8)(1.61^2)"
%    2.37609166666667*10^-3
% calc -e "k=.0044(1/4.8)(1.61^2); k*60^2"
%    8.6

Realistically, drivers are not going to be able to calculate the result of equation \eqref{eqn:stopping-distance}
in their heads while driving. The traditional rule, although not conservative enough
at high speeds, had the advantage that it was easy to work out mentally.

This is an example of a situation that occurs over and over again in real life, which is that
we would like to approximate a complicated nonlinear function using a simple linear one.
The derivative is the slope of the tangent line, and the tangent line is the \emph{best}
possible line to approximate a given function near a particular point. As an example,
let's find the best linear approximation to the stopping distance, for speeds near 60 mi/hr.
The tangent line has slope
\begin{equation*}
  \frac{\der x}{\der v} = 2kv \qquad ,
\end{equation*}
and plugging in 60 mi/hr for $v$ produces a slope $m$ of 0.3, meaning that 
\emph{according to the tangent line}, for every additional 1 mi/hr
of speed, the stopping distance increases by about 0.3 car lengths. If the speed is close enough
to 60 mi/hr so that the tangent line is a good approximation, then this is also approximately
true for the real stopping distance.
% calc -e "k=.0044(1/4.8)(1.61^2); 2k*60"
Writing down the definition of the slope of a line, we have
\begin{equation}
  \frac{x-x_\zu{0}}{v-v_\zu{0}} = m \qquad .
\end{equation}
where $(v_\zu{0},x_\zu{0})=(60,8.6)$ is the point at which we're taking the tangent line.
This is the equation of the \emph{tangent line}, not the curve itself,
which was equation \eqref{eqn:stopping-distance}. Solving this for $x$, we have
\begin{equation}\label{eqn:stopping-distance-linear}
  x = m(v-v_\zu{0})+x_\zu{0} \qquad .
\end{equation}
The result is shown in figure \figref{stopping-distance}, and we can see that it's
quite a good approximation for a range of speeds broad enough to encompass most freeway driving.


%%graph%% stopping-distance func=0.00238*x*x format=eps xlo=0 xhi=90 xtic_spacing=20 ylo=0 yhi=20 with=lines x=v y=x samples=100 ytic_spacing=5 ; func=0.29*(x-60)+8.6
<% marg(200) %>
<%
  fig(
    'stopping-distance',
    %q{Stopping distance $x$, in car lengths, as a function of initial speed $v$, in mi/hr. The
       linear approximation \eqref{eqn:stopping-distance-linear} is superimposed. }
  )  
%>
<% end_marg %>


<% end_sec('linear-approximation-to-a-curve') %>
<% end_sec('approximations') %>

\pagebreak

\startdq

\begin{dq}\label{dq:lead-fall}
Figure \figref{dq-lead-fall} shows a rock climber. The rope is tied to the climber
at the top and at the bottom to the  anchor, marked X. (There are provisions for
the climber's partner to pay out rope so that the climber can move upward.)
The rope is clipped into a removable anchor placed in the rock, and the climber
is now a height $h$ above this protection. If she falls, we would imagine that
she would fall a height equal to $2h$. However, the rope is intentionally designed
to be stretchy, so that when it comes up taut, it will stop the climber gently.
This causes the climber to fall a distance greater than $2h$, and we want this
distance to be short enough so that she doesn't hit the ground. 

The bottom panel of the figure shows data from laboratory testing of a climbing
rope.\footnote{Casey Johnson and Charlie Klonowski, student research paper, Cal Poly Pomona}
The stretchiness of a climbing rope is normally described in terms of the percentage
by which it stretches when a standard weight is hung from it, which is essentially
a measure of the slope (actually the inverse slope) of the graph. Since the graph
is only approximately linear, this quantity should really be measured by a derivative.
What could we say about the derivative of this function at the point P marked on the
graph, and what real-world implications would this have?
\end{dq}

<% marg(80) %>
<%
  fig(
    'dq-lead-fall',
    %q{Discussion question \ref{dq:lead-fall}.}
  )  
%>
<% end_marg %>

\begin{hwsection}

<% hw('diff-monomials',{'solution'=>true}) %>

<% hw('dummy-variable',{'solution'=>true}) %>

<% hw('zero-value-nonzero-derivative',{'solution'=>true}) %>

<% hw('perfectly-inelastic-demand') %>

  <% hw_block(2) %>

<% hw('diff-polynomial',{'solution'=>true}) %>
<% hw('diff-polynomial-2') %>
<% hw('diff-polynomial-3') %>
<% hw('diff-polynomial-4') %>
<% hw('diff-polynomial-5') %>

\hfill

<% hw('same-deriv',{'solution'=>true}) %>

  <% hw_block(2) %>

<% hw('zoom-1') %>
<% hw('zoom-2') %>
<% hw('zoom-3') %>

  <% hw_block(2) %>

\pagebreak

<% hw('diff-symbolic-const',{'solution'=>true}) %>

<% hw('can-you-do-fractions-1') %>
<% hw('can-you-do-fractions-2') %>

  <% hw_block(2) %>

<% hw('human-cylinder') %>
<% hw('observable-universe') %>
<% hw('car-ke-and-power') %>

  <% hw_block(1) %>

<% hw('deriv-props-add-implies-stretch',{'solution'=>true}) %>
<% hw('deriv-props-line-implies-constant') %>
<% hw('deriv-props-add-and-constant-imply-shift') %>
<% hw('deriv-props-odd-even') %>

  <% hw_block(1) %>


<% hw('rancher-rick',{'solution'=>true}) %>
<% hw('prove-n-extrema',{'solution'=>true}) %>
<% hw('overtime') %>
<% hw('cactus') %>
<% hw('immerse-sphere') %>

\pagebreak

<% hw('line-of-stability') %>
<%
  fig(
    'hw-line-of-stability',
    %q{Problem \ref{hw:line-of-stability}.},
    {
      'width'=>'fullpage'
    }
  )
%>




  <% hw_block(1) %>

<% hw('chain-rule-circle') %>



<% hw('chain-rule-sphere') %>
<% hw('chain-rule-triangle') %>

  <% hw_block(1) %>

\pagebreak

<% hw('tree') %>
<% hw('tank') %>

  <% hw_block(1) %>

<% hw('numerical-deriv-geometric',{'solution'=>true}) %>
<% hw('numerical-deriv-cos-x-cubed') %>
<% hw('numerical-deriv-sin-sqrt-x') %>
<% hw('numerical-deriv-e-to-cos-x') %>
<% hw('numerical-deriv-woods-saxon') %>

  <% hw_block(1) %>

<% hw('error-propagation-powers',{'solution'=>true}) %>
<% hw('error-propagation-rocket-height',{'solution'=>true}) %>
<% hw('error-propagation-paper-squares') %>

  <% hw_block(1) %>

<% hw('tangent-to-parabola-through-point') %>
<% hw('tangent-to-x3-through-point') %>
<% hw('tangent-x2-and-x4') %>

  <% hw_block(1) %>

<% hw('linear-approx-137th-power') %>

\end{hwsection}

<% end_chapter %>
