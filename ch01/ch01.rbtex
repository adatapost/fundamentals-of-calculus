%%chapter%% 01
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '01',
    %q{An informal introduction to the derivative},
    'ch:derivative'
  )
%>

<% begin_sec("Review: functions and the slope of a linear function",nil,'functions') %>
Calculus is the study of rates of change, and of how change accumulates.
For example, figure \figref{stock-market} shows the changes in the United States
stock market over a period of 24 years. The $y$ axis of this graph is a certain
weighted average of the prices of stock, and the $x$ axis is time, measured in
years. This is an example of the concept of a mathematical \emph{function},\index{function}
which you've learned about in a previous course. We say that the stock index is a function
of time, meaning that it depends on time. What makes this graph the graph of a function is that
a vertical line only intersects it in one place. This means that at any given time, there is only
one value of the index, not more than one.

<% marg(0) %>
<%
  fig(
    'stock-market',
    %q{The S\&P 500 stock index is a function of time.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'rise-over-run',
    %q{Given two points on a line, we can find its slope by computing $\Delta y/\Delta x$,
       the rise over the run.}
  )  
%>
<% end_marg %>
Figure \figref{stock-market} shows a function that was determined by measurement and
observation, but functions can also be defined by a formula. For example, we could define
a function $y$ by stating that for any number $x$, the value of the function is given by $y(x)=x^2$.
We sometimes state this kind of thing more casually by referring to ``the function $y=x^2$''
or ``the function $x^2$.''

I drew figure \figref{stock-market} by graphing yearly data, so it's made of line segments that
connect one year to the next. Each of these line segments has a \emph{slope},\index{slope}
defined as
\begin{equation}
  \text{slope} = \frac{y_2-y_1}{x_2-x_1} \qquad .
\end{equation}
The slope measures how fast the function is changing. A positive slope says the function is increasing,
negative decreasing. If the slope is zero, the function is not changing at all.

It's often convenient to express this kind of thing with the notation $\Delta$, the capital Greek
letter delta, which is the equivalent of our Latin ``D'' and here stands for ``difference.'' In terms of this notation,
we have
\begin{equation}
  \text{slope} = \frac{\Delta y}{\Delta x} \qquad .
\end{equation}
Here a symbol like $\Delta y$ indicates the change in $y$, as defined by subtraction,
$\Delta y=y_2-y_1$. It doesn't
mean a number $\Delta$ multiplied by a number $y$.

<% end_sec('functions') %>

<% begin_sec("The derivative",nil,'derivative') %>
<% begin_sec("An informal definition of the derivative",nil,'informal-derivative') %>
In many real-world applications, it makes sense to think of change as occurring smoothly and
continuously. For example, the level of water in a reservoir rises and falls with time.
Although it's true that this change happens one molecule at a time, so that in theory
there are abrupt jumps, these jumps are too tiny to matter in practice.

<%
  fig(
    'reservoir',
    %q{The original graph, on the left, shows
       the water level in Trinity Lake, California, for the thirty-day period
       beginning March 7, 2014.
       Each successive magnification to the right is by a factor of four.
    },
    {
      'width'=>'fullpage'
    }
  )
%>

We want to keep track of the net rate of flow into the reservoir. We would
like to define this rate as the slope of the graph, but the graph isn't a line, so how do we
do that? We could pick two points on the graph and connect them with a line segment, but that
would only represent an average rate of flow, not the actual rate of flow as it
would be measured by a flow gauge at one particular time.

To get around these difficulties, we imagine picking a point of interest on the graph
and then zooming in on it more and more, as if through a microscope capable of unlimited
magnification. As we zoom in, the curviness of the graph becomes less and less apparent.
(Similarly, we don't notice in everyday life that the earth is a sphere.)
In figure \figref{reservoir}, we zoom in by 400\%, and then again by 400\%, and so on.
After a series of these zooms, the graph appears indistinguishable from a line, and we can
measure its slope just as we would for a line. This is an intuitive description of what we mean
by the slope of a function that isn't a line. We call this slope the
\emph{derivative}\index{derivative} of the function at the point of interest.
This is admittedly not a mathematically rigorous definition, but it fixes our minds on the
concept we want. A useful example is that if we consider a car's odometer reading as a function of
time in hours, then its speedometer reading is the derivative of the odometer reading.
<% marg(0) %>
<%
  fig(
    'reservoir-tangent-line',
    %q{The tangent line at a point on a curved graph.}
  )  
%>
<% end_marg %>

If we were only shown the ultra-magnified view in the rightmost part of figure \figref{reservoir},
we wouldn't know that the graph was curved at all. We would think the whole thing was a line.
This hypothetical line is called the \emph{tangent line}\index{tangent line}
at the point marked with a dot. When you stand on the earth's surface and look at a point on
the horizon, your line of sight is a tangent line to the surface. The derivative of a function is the slope
of the tangent line.

<% end_sec('informal-derivative') %>

<% begin_sec("Properties of the derivative",nil,'properties-of-the-derivative') %>
The following properties of the derivative are intuitively reasonable
based on our conceptual definition, and they will be enough to allow us to do quite a bit
of interesting calculus before we come back and make a more rigorous definition.
<% marg(0) %>
<%
  fig(
    'properties-of-derivative',
    %q{Some properties of the derivative.}
  )  
%>
<% end_marg %>


\begin{description}\label{properties-of-derivative}
 \item[constant] The derivative of a constant function is zero.
 \item[line] The derivative of a linear function is its slope.
 \item[shift] Shifting a function $y(x)$ horizontally or vertically to form a new function 
                   $y(x+a)$ or $y(x)+b$
                   gives a derivative at any newly shifted point that is the same as
                   the derivative at the corresponding point on the unshifted graph.
 \item[flip] Flipping the function $y(x)$ horizontally or vertically to form a new function $y(-x)$ or
                   $-y(x)$ negates its derivative at corresponding points.
 \item[addition] The derivative of the sum of two functions is the sum
                   of their derivatives.
 \item[stretch] Stretching a function $y(x)$ vertically to form a new function $ry(x)$ multiplies its
                   derivative by $r$ at the corresponding points, while stretching it horizontally
                   to make $y(x/s)$ divides its derivative by $s$.
 \item[no-cut] Suppose that for a certain point P on the graph of a function, there is a unique linear function
                   $\ell$ that passes through P but doesn't cut through the graph. Then the graph of $\ell$
                   is the tangent line, and
                   the derivative of the function at P equals the slope of the line.
\end{description}

As an example of the stretch rule, cars sold in the U.S.~have odometers that read out in units of miles,
while those sold elsewhere are calibrated in kilometers, so their readings are greater by
the conversion factor $r=1.6$. By the stretch property, cars outside the U.S.~also have speedometer
readings that are greater by this factor: they read out in \emph{kilometers} per hour.

There is usually, but not always, a line like the one described by the no-cut property.
Sometimes there is a tangent line but it isn't a no-cut line. If this kind of mathematical
puzzle interests you, try sketching the 
graphs of the functions $x^3$ and $\sqrt{x}$. You should be able to convince yourself that
their tangent lines at $x=0$ can't be described by no-cut functions.

By the way, these are just names I've given to these properties, and if you use them with other
people, they won't know what you mean. Once we've done more calculus, we'll see that several
of these properties are actually special cases of a more general rule called the chain rule.

<% end_sec('properties-of-the-derivative') %>

<% begin_sec("The derivative of the function $y=x^2$",nil,'derivative-of-x-squared') %>

%%graph%% x-squared func=x**2 format=eps xlo=-3 xhi=3 ylo=0 yhi=10 with=lines

As our first example of a derivative, let's use the function $y=x^2$. Its graph is a parabola.
The simplest point at which
to find its derivative is $x=0$. Based on the graph, figure \figref{x-squared}, it seems like
zooming in more and more on this point would give something that looked more and more like a
horizontal line, and this suggests that the derivative at this point is zero. We can confirm this
by using the flip property. Flipping the graph horizontally doesn't change the graph. (Recall that
a function with this symmetry is called an \emph{even} function.)\index{even function}
Since the flip doesn't change the function, it can't change the derivative of the function, and
yet the flip rule says that when we do this the derivative should flip its sign. Zero is the
only number that remains the same when we flip its sign, so the derivative is zero.
<% marg(200) %>
<%
  fig(
    'x-squared',
    %q{The function $y=x^2$.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'tangent-line',
    %q{The line $2x-1$ intersects the function $x^2$ without cutting it.}
  )  
%>
<% end_marg %>

How about the derivative at the point $x=1$? Here we can apply the no-cut rule.
By laying a ruler against this point, we find that the linear function $\ell(x)=2x-1$ seems
to intersect the parabola without cutting across it. To prove that this is
true, we can compute the difference between the two functions, $y(x)-\ell(x)=x^2-2x+1$.
Completing the square allows us to rewite this as $(x-1)^2$, which is clearly positive for
any value of $x$ other than 1. Therefore the function $\ell$ meets the conditions of the
no-cut rule, and the derivative of $x^2$ at $x=1$ is 2.

%%graph%% tangent-line func=x**2 format=eps xlo=0 xhi=2 ylo=0 yhi=2 with=lines x=t y=x samples=300 ytic_spacing=1 ; func=2*x-1.0 with=lines

Having found the derivative of $x^2$ at $x=1$, we can now use the stretch rule to find it
at any other point. For example, suppose we want to know the derivative at $x=3$. If we were to
take the graph of the function $x^2$ and stretch it by a factor of $3$ horizontally and $9$ vertically, we would
get the same graph again. These stretches take the point $(1,1)$, where we know the derivative,
to the point $(3,9)$, where we want to know it.
The stretch rule tells us that the horizontal stretch decreases the
derivative to $1/3$ of its original value, but the vertical stretch increases it by $9$ times,
so that over all, the derivative at $(3,9)$ is $(1/3)(9)=3$ times greater than its value
at $(1,1)$. Thus the derivative at $x=3$ equals 6.

There is nothing special about the number 3. 
The method that we applied to $x=3$ would work for any other number $x$, not just for 3.
We find that the derivative of the function $x^2$ at any point $x$ equals $2x$. Taking stock of
what we've done, we started with the function $x^2$, and found that at any point $x$, the derivative
was $2x$. The expression $2x$ can be thought of as a function of $x$. So what we've really done
is to take a function and construct a new function that gives the derivative of the original
function at each point. One way of notating this new function is $y'$, read ``$y$ prime.'' We have
\begin{align*}
  y &= x^2 \\
  y' &= 2x \qquad .
\end{align*}
The craft of finding this kind of derivative-function from the original function is called
\emph{differentiation}. We have differentiated the function $x^2$ and gotten its derivative,
the function $2x$.
<% marg(200) %>
<%
  fig(
    'derivative-as-a-function',
    %q{The derivative of $x^2$ is itself a function. As we change $x$, the slope of the tangent line
       changes.}
  )  
%>
<% end_marg %>

<% end_sec('derivative-of-x-squared') %>
<% end_sec('derivative') %>

<% begin_sec("Derivatives of powers and polynomials",nil,'deriv-polynomial') %>
In section \ref{subsec:derivative-of-x-squared}, we found that the derivative of $x^2$ was $2x$.
Straightforward application of the same technique to $x^3$ gives $3x^2$. We see a pattern:

\begin{important}[Derivatives of powers]
The derivative of $x^n$ equals $nx^{n-1}$, if $n$ is any integer greater than or equal to 1.
\end{important}

\noindent To prove this for all these values of $n$, rather than carrying out the proof for one value at
a time, it will be more convenient to use techniques developed later in the book.

If we combine this with the addition and stretch rules, we know enough to differentiate any polynomial.
<% marg(-50) %>
<%
  fig(
    'derivative-of-a-polynomial',
    %q{Example \ref{eg:derivative-of-a-polynomial}. The top graph shows the original function, the
       bottom its derivative.
       }
  )  
%>
<% end_marg %>

\begin{eg}{Differentiating a polynomial}\label{eg:derivative-of-a-polynomial}
\egquestion Find the derivative of $y'=x^3-7x+1$.

\egquestion The addition property of the derivative
tells us that we can break this problem down into three parts,
\begin{equation*}
  (x^3-7x+1)' = (x^3)'+(-7x)'+(1)' \qquad ,
\end{equation*}
where the primes indicate ``derivative of \ldots'' The stretch property says that $(-7x)'$ is the same
as $(-7)(x)'$, so the derivative of our polynomial becomes
\begin{equation*}
   (x^3)'+(-7)(x)'+(1)' \qquad .
\end{equation*}
We know how to differentiate powers: $(x^3)'=3x^2$, $(x')=1$, and $(1)'=0$. (We could have found the
second term from the line property, and the final one from the constant property.) The result is
\begin{equation*}
   y' = 3x^2-7 \qquad .
\end{equation*}

The functions $y$ and $y'$ are graphed in figure \figref{derivative-of-a-polynomial}, and
five points are marked as examples of how the \emph{slope} of $y$ corresponds
to the \emph{value} of $y'$. Reading across from left to right on the top graph, the slopes are
positive, zero, negative, zero, and positive. On the bottom graph, the values of $y'$ are
easily seen to be positive, zero, negative, zero, and positive.
\end{eg}

%%graph%% polynomial-to-differentiate func=x**3-7*x+1 format=svg xlo=-3 xhi=3 ylo=-10 yhi=10 xtic_spacing=1 ytic_spacing=5 with=lines
%%graph%% deriv-of-polynomial func=3*x**2-7 format=svg xlo=-3 xhi=3 ylo=-10 yhi=10 xtic_spacing=1 ytic_spacing=5 with=lines

<% end_sec('deriv-polynomial') %>

<% begin_sec("Two trivial hangups",nil,'hangups') %>
<% begin_sec("Changing letters of the alphabet",nil,'changing-letters') %>
The following point is relatively trivial, but nevertheless hangs up many students
in applying calculus to real life. In a calculus
textbook, we typically use the letters $x$ and $y$, with $y$ being a function of $x$. That is, $x$ is the independent variable,
and $y$ is the dependent one. In real-life applications, however, the variables have definite meanings, and we want to
use letters that make it easy to remember what they stand for. 

For example, suppose that a social media company has a certain
number of users, and they need to have enough computing power at their data center to be able to handle all of those users.
This computing power will cost them a certain amount of money per month. In this
example, it would be natural to use the notation $u$ for
the number of users, and $c$ for the monthly cost in dollars. Then $c$ depends on $u$, and we have a function $c(u)$. Let's
say the function is this:
\begin{equation*}
  c = u^2
\end{equation*}
This is not an unrealistic equation to imagine for this example, since the company has to keep track of every user's
relationship to every other user. For example, user Andy may be able to mark himself
as a ``fan'' or ``follower'' of user Betty, and then the company has to store a piece
of information in a database to record this relationship.
If there are
a thousand users, there are $1000\times 1000$ or a million such possible
relationships that may need to be stored in a database.

Now if the company's user base is growing, it's of interest to them to know how much their costs will go up for each
additional user (the marginal cost). This would be expressed by the derivative $c'(u)$. Although the letters of the alphabet are different
than the ones we used in our earlier examples, that makes no difference in how we do the math. If differentiating
$y=x^2$ with respect to $x$ gives $y'=2x$, then differentiating $c=u^2$ with respect to $u$ gives the same result but
with the letters changed,
\begin{equation*}
  c' = 2u
\end{equation*}
<% end_sec('changing-letters') %>
<% begin_sec("Symbolic constants",nil,'symbolic-constants') %>
The vertical stretch property of the derivative tells us that if we know a derivative such as
\begin{equation*}
  (x^2)'=2x \qquad ,
\end{equation*}
then we can differentiate a function like $5x^2$
by simply letting the factor of 5 ``come along for the ride,'' 
\begin{align*}
  (5x^2)' &= (5)(x^2)' \\
          &= (5)(2x) \\
          &= 10x \qquad .
\end{align*}
Now suppose that we want to differentiate
$bx^2$, where $b$ is a constant, i.e., $b$ doesn't depend on $x$. To many students this looks like a much
more difficult and abstract problem, but the procedure is the same:
\begin{align*}
  (bx^2)' &= (b)(x^2)' \\
          &= (b)(2x) \\
          &= 2bx \qquad .
\end{align*}
The same goes for a vertical shift. If we aren't intimidated by computing
\begin{equation*}
  (x^2+5)' = (x^2)' = 2x \qquad ,
\end{equation*}
then there is no reason to be scared of the similar computation (again with
$b$ being a constant) of
\begin{equation*}
  (x^2+b)' = (x^2)' = 2x \qquad .
\end{equation*}
<% end_sec('symbolic-constants') %>
<% end_sec('hangups') %>

<% begin_sec("Applications",nil,'apps-of-derivative') %>

<% begin_sec("Velocity",nil,'velocity') %>
<% begin_sec("Defining velocity",nil,'defining-velocity') %>
One of our prototypical examples has been the odometer and speedometer on a car's dashboard. In fact, if we want to \emph{define}
what velocity means, we have to define it as a derivative. Suppose an object (it could be a car, a galaxy, or a subatomic
particle) is moving in a straight line. By choosing a unit of distance and a location that we define as zero, we can superimpose
a number line onto this line. (In the example of the car, the unit of distance might be kilometers, and the zero
position would be the point on the road at which we last pushed the button to zero the odometer.) Let the position
defined in this way be $x$. Then $x$ is a function of time $t$ (such as the time measured on a clock), and we notate this
function as $x(t)$. Note that although we typically use the letters $x$ and $y$ in a generic mathematical context, with
$y$ being a function of $x$, in our present example it is more natural to use different letters, and now $x$ is the
\emph{dependent} variable, not the independent one. That is, $x$ is a function of $t$, but $t$ may not be a function of
$x$; for example, if a car stops and backs up, then it can visit the same position twice, so that a graph of $t$ versus
$x$ would fail the vertical line test for a function. In this notation, the velocity $v$ is defined as the
derivative 
\begin{equation*}
  v(t) = x'(t) \qquad .
\end{equation*}
<% end_sec('defining-velocity') %>
<% begin_sec("Constant acceleration",nil,'constant-acceleration') %>
An important special case is the one in which the position function is of
the form
\begin{equation*}
  x(t) = \frac{1}{2}at^2 \qquad ,
\end{equation*}
where $a$ is a constant, and the factor of $1/2$ is conventional, and convenient for
reasons that will become more apparent in a moment. Differentiating with respect to
$t$, we have the velocity function
\begin{equation*}
  v(t) = at \qquad ,
\end{equation*}
where the symbolic constant $a$ has been treated like any other constant,
and the $1/2$ in front has been canceled by the factor of 2 that comes down from the
exponent. We see that the velocity is proportional to the amount of time that has passed.
If $t$ is measured in seconds and $v$ in meters per second (m/s), then the constant $a$,
called the \emph{acceleration},\index{acceleration} tells us how much speed the object
gains with every second that goes by, in units of m/s/s, which can be written as $\munit/\sunit^2$.
Falling objects have an acceleration of about $9.8\ \munit/\sunit^2$. This is a measure of
the strength of the earth's gravity near its own surface.

\begin{eg}{Dropping a rock down a well}
\egquestion Looking down into a dark well, you can't see how deep it is. If you drop a rock
in and hear it hit the bottom in 2 seconds, how deep is the well?

\eganswer 
\begin{equation*}
  x(t) = \frac{1}{2}at^2 \approx 20\ \munit
\end{equation*}
\end{eg}

\begin{eg}{The shift property applied to constant acceleration}
The equations for constant acceleration were given above with the unstated assumption that
both the position and the velocity would be zero at the time $t=0$. If we relax this assumption, then
the position function can be of the more general form
\begin{equation*}
  x(t) = x_\zu{o}+\frac{1}{2}a(t-t_\zu{o})^2 \qquad ,
\end{equation*}
where $t_\zu{o}$ is some initial time, at which the position equals $x_\zu{o}$. By the shift property of the
derivative (p.~\pageref{properties-of-derivative}), the velocity function is then
\begin{equation*}
  v(t) = a(t-t_\zu{o}) \qquad .
\end{equation*}
\end{eg}
<% end_sec('constant-acceleration') %>
<% end_sec('velocity') %>

<% begin_sec("When do you need a derivative?",nil,'when-derivative-is-needed') %>
Finding velocity from position data is a classic application of calculus, and yet how
do we know when we really need calculus for this application? After all, many people
do simple computations involving velocity without knowing calculus.

Here's an example where calculus really is required.
In July 1999, Popular Mechanics carried out tests to
find which car sold by a major auto maker could cover a
quarter mile (402 meters) in the shortest time, starting
from rest. Because the distance is so short, this type of
test is designed mainly to favor the car with the greatest
acceleration, not the greatest maximum speed (which is
irrelevant to the average person). The winner was the Dodge
Viper, with a time of 12.08 s. If we divide the distance by the time, we get
\begin{equation*}
  v = \frac{\Delta x}{\Delta t} = 33.3\ \munit/\sunit \qquad ,
\end{equation*}
which is about 74 miles per hour or 120 kilometers per hour. Not a very impressive
speed, is it? That's because it's wrong. During those twelve seconds of acceleration,
the car didn't have just one speed. It started at a velocity of zero and went up from
there. The top speed was nearly double the one calculated above
($53\ \munit/\sunit\approx 119\ \zu{mi}/\zu{hr}\approx 191\ \zu{km}/\zu{hr}$). The important
point here is that when we measure a rate of change using an expression of the form
\begin{equation*}
  \frac{\Delta\ldots}{\Delta\ldots} \qquad ,
\end{equation*}
we only get the right answer if the rate of change is \emph{constant}. In this example
the rate of change is the velocity, and the velocity is not constant. To find the correct velocity,
we first need to decide at which time we want to know the velocity, and then evaluate the derivative
at that time.
<% end_sec('when-derivative-is-needed') %>

<% begin_sec("Optimization",nil,'optimization') %>
An extremely important use of the derivative is in optimization. For example, suppose that
the operators of a privately owned mountain tram in Switzerland want to optimize
their profit from transporting sightseers to a mountain summit in the Alps. The cost of
building the tram is a sunk cost, and operating it for one day costs the same amount of money regardless
of the number of passengers. Therefore the only goal is to get the maximum number of Swiss
francs in the cash registers at the end of each day. The operators can raise the fare $f$ in order
to make more money, but if the fare is too high then not as many people will be willing to pay it.
Suppose that the number of riders in a given day is given by $a-bf$, where $a$ and $b$ are constants.
That is, if the ride was free, $a$ passengers would ride each day, but for every one-franc
increase in the fare, $b$ people will decide not to go. The tram's daily revenue is then found
by multiplying the number of riders by the fare, which gives
the function
\begin{equation}\label{eq:tram-revenue}
  r(f) = (a-bf)f \qquad .
\end{equation}

For insight into what's going on, figure \figref{tram-fare-edited} shows this function in the case where $a=100$ and
$b=1$. When the fare is zero, we get plenty of customers every day, but they don't pay anything,
so our revenue is zero. When the fare is 100 francs, the number of paying passengers goes down
to zero, so again we have no revenue. 
<% marg(0) %>
<%
  fig(
    'tram-fare-edited',
    %q{Revenue from a tram as a function of the fare charged.}
  )  
%>
<% end_marg %>

Somewhere in between these extremes  we have the fare that would optimize our
revenue: the \emph{maximum} of the function $r$. At this point on the graph, the derivative is zero,
so to find it, we should differentiate $r$, set it equal to zero, and solve for $f$.

We haven't yet learned enough of the techniques of calculus to
know how to find the derivative of a function with the form of equation
\eqref{eq:tram-revenue}, but by multiplying out the product we can make it into a polynomial,
which is a form that we do know how to differentiate:
\begin{align*}
  r(f) &= -bf^2+af \\
  r'(f) &= -2bf+a
\end{align*} 
Setting $r'$ equal to zero, we have
\begin{align*}
  0 &= -2bf+a \\
  f&=\frac{a}{2b} \qquad .
\end{align*} 
With the particular numerical values used to construct the graph, this gives an optimal
fare of 50 francs, which looks about right from the graph.

By searching for points where the derivative is zero we can often, but not always, find
the the points where a function takes on its maximum and minimum values. The term extremum
(plural extrema) is used to refer to these points. Figure \figref{cases-of-extrema} shows that
quite a few different things can happen, and that searching for a zero derivative doesn't
always tell us the whole story. We have a zero derivative at point G, but G is only a maximum
compared to nearby points; we call G a local maximum, as opposed to the global maximum D.
The zero-derivative test doesn't distinguish a local minimum like B from a local maximum.
A zero derivative may not indicate a local extremum at all, as at C and H. We can have points
such as E and F where the derivative is undefined. An extremum can occur at a point like A
that is the endpoint of the function's domain.
We will come back to these technical points in more detail later in the book.
<% marg(0) %>
<%
  fig(
    'cases-of-extrema',
    %q{A zero derivative often, but not always, indicates a local extremum.}
  )  
%>
<% end_marg %>

%%graph%% tram-fare func=(100.0-x)*x x=f y=r format=svg xlo=0 xhi=100 ylo=0 yhi=2500 xtic_spacing=20 ytic_spacing=1000 with=lines

<% end_sec('optimization') %>
<% end_sec('apps-of-derivative') %>

<% begin_sec("Review: elementary properties of the real numbers",nil,'elementary-reals') %>

I began this chapter by defining calculus as the study of rates of change, but
it could equally well be described as the study of infinity. The intuition behind the derivative
is that we zoom in on a selected point on a smooth curve, until the curve appears like a line
and we can measure the slope of the line. But the curve won't appear perfectly
straight until we've cranked up our microscope to an \emph{infinitely big} magnification, at which point
we'll be seeing values of $\Delta x$ and $\Delta y$ that are \emph{infinitely small} (but not zero).
Calculus was invented by Isaac Newton and
Gottfried Wilhelm von Leibniz back in the era of powdered wigs and silk stockings, and
in those days the concept of number hadn't been formalized. Newton and Leibniz simply assumed
that it was all right to have infinitely big and infinitely small numbers. But modern
mathematicians approach this kind of thing differently. They think in terms of specific,
well-defined sets of numbers, such as:

\begin{description}
\item[the integers:] whole numbers such as $-1$, 0, and $1$
\item[the rational numbers:] ratios of integers such as $2/1$ and $3/4$
\item[the real numbers,] including quantities like $\pi$ and $\sqrt{2}$
\item[the complex numbers,] such as $\sqrt{-1}$
\end{description}

Do these systems include infinitely big and infinitely small numbers? Can they? Should they?


To answer these questions, we need to give a more definite account of how these number systems
are defined. A good way to define them is with a list of their axioms.
Here is a list of axioms for the system of real numbers. Except as otherwise stated,
each of these properties holds for \emph{any} real-number values of the symbols $x$, $y$, \ldots

\begin{description}
\item[commutativity] $x+y=y+x$ and $xy=yx$
\item[identities] There exist numbers 0 and 1 such that for any $x$, $x+0=x$ and $1x=x$.
\item[inverses] For any $x$, there exists a number $-x$ such that $x+(-x)=0$. For any nonzero $x$,
          there exists $1/x$ such that $(x)(1/x)=1$.
\item[associativity] $x+(y+z)=(x+y)+z$ and $x(yz)=(xy)z$
\item[distributivity] $x(y+z)=xy+xz$
\item[ordering] We can define whether or not $x<y$, and this ordering relates to the addition and
      multiplication operations in specific ways, which you've seen defined in a previous course
      on algebra and which for brevity we will not explicitly give here.
\end{description}

This list of axioms holds for the real numbers, but it fails for the integers, since for example
the integer 2 doesn't have an inverse that is an integer. It also fails for the complex numbers,
which don't have a well-defined ordering. The list seems complete and precise, so it may come
as a surprise that it does \emph{not} suffice to prove anything about whether or not infinite
numbers exist. The list of axioms is in fact incomplete as a characterization of the real numbers.
Later in this book we will add another axiom, called the completeness axiom,\index{completeness axiom}
to the list. The completeness axiom holds for the reals but not the rationals, and it also
rules out the existence of infinitely large or infinitely small real numbers.

<% end_sec('elementary-reals') %>

<% begin_sec("The Leibniz notation",nil,'leibniz-notation') %>
For better or for worse, this kind of thing wasn't a concern in Leibniz's era.
With what seems like kittenish na\"ivet\'e to a modern mathematician, he reasoned as follows.
Let's just make $\Delta x$ and $\Delta y$ infinitely small (but not zero). In modern terminology,
this means that they can't be real numbers. To make it clear that we're talking about infinitely small
differences in $x$ and $y$, we change the notation to $\der x$ and $\der y$. Recall that $\Delta$
is the Greek version of capital ``D,'' so we're using a smaller version of the letter, ``$\der$,''
to represent a change that is smaller (in fact, infinitely small). Dividing these two
``numbers'' (whatever mysterious species of number they may turn out to be), we get the
derivative,
\begin{equation*}
  \frac{\der y}{\der x} \qquad .
\end{equation*}
Although the notation's original justification was not up to modern standards of
rigor, it is one of the most expressive and well-designed mathematical notations ever devised,
and has been the most commonly used notation for the derivative ever since Leibniz published it
in 1686.
<% marg(80) %>
<%
  fig(
    'leibniz-portrait',
    %q{Gottfried Wilhelm Leibniz (1646-1716).}
  )  
%>
<% end_marg %>

One of the good things about the Leibniz notation is that it states clearly what we're differentiating
\emph{with respect to}. For example, $\der v/\der t$ could indicate how much a car was speeding up
with each passing second of time, while $\der v/\der x$ would measure the speed gained with
each meter that it moved down the road.

Another selling point of the notation is that it shows the units of the derivative. For example,
the definition of velocity, expressed in Leibniz notation, is
\begin{equation*}
  v = \frac{\der x}{\der t} \qquad .
\end{equation*}
On the left-hand side we have velocity, whose units in the SI are meters per second. On the
right we have a tiny change in position, which has units of meters, divided by a tiny change
in time, which has units of seconds. In terms of units, then, the equation reads as
\begin{equation*}
  \zu{m/s} = \frac{\munit}{\sunit} \qquad ,
\end{equation*}
which works out correctly. In more complicated examples, checking the units like this is a powerful
method for checking your answer to a calculus problem.

\begin{eg}{An insect pest}\label{eg:pest}
\egquestion An insect pest from the United States is inadvertently released in
a village in rural China. The pests spread outward at a rate of $s$ kilometers
per year, forming a widening circle of contagion. Find the number of square
kilometers per year that become newly infested. Check that the units of the result
make sense. Interpret the result.

\eganswer Let $t$ be the time, in years, since the pest was introduced.
The radius of the circle is $r=st$, and its area is $a=\pi r^2=\pi(st)^2$.
To make this look like a polynomial, we have to rewrite it as
$a=(\pi s^2)t^2$. The derivative is
\begin{align*}
  \frac{\der a}{\der t} &= (\pi s^2)(2t) \\
                        &= (2\pi s^2) t
\end{align*}
The units of $s$ are km/year, so squaring it gives $\zu{km}^2/\zu{year}^2$.
The 2 and the $\pi$ are unitless, and multiplying by $t$ gives units
of $\zu{km}^2/\zu{year}$, which is what we expect for $\der a/\der t$, since
it represents the number of square kilometers per year that become infested.

Interpreting the result, we notice a couple of things. First, the rate
of infestation isn't constant; it's proportional to $t$, so people might not
pay so much attention at first, but later on the effort required to combat the
problem will grow more and more quickly. Second, we notice that the
result is proportional to $s^2$. This suggests that anything that could be
done to reduce $s$ would be very helpful. For instance, a measure that cut
$s$ in half would reduce $\der a/\der t$ by a factor of four.
\end{eg}
<% end_sec('leibniz-notation') %>

<% begin_sec("Approximations",nil,'approximations') %>
We saw in section \ref{subsec:when-derivative-is-needed} on p.~\pageref{subsec:when-derivative-is-needed}
that the derivative can't be calculated as $\Delta y/\Delta x$ unless the derivative is constant, i.e., unless
the function's graph is a line. In the Leibniz notation, this is
\begin{equation*}
  \frac{\der y}{\der x} \ne \frac{\Delta y}{\Delta x} \qquad .
\end{equation*}
But if we take two points very close together on a graph, then the curvature doesn't matter too much,
and the line through those points is a good approximation to the tangent line, as in figure
\figref{approximating-tangent-line}. When then have the approximation
\begin{equation*}
  \frac{\der y}{\der x} \approx \frac{\Delta y}{\Delta x} \qquad .
\end{equation*}
It may be of interest to use either side of this as an approximation to the other.

<% begin_sec("Approximating the derivative",nil,'approximating-deriv') %>
Suppose you can't remember that the derivative of $x^2$ is $2x$, but you need to find the
value of the derivative at $x=1$. As in figure \figref{approximating-tangent-line}, let point P
be
\begin{equation*}
  (1.0000,1.0000) \qquad ,
\end{equation*}
and let Q be the nearby point
\begin{equation*}
  (1.0100,1.0201) \qquad .
\end{equation*}
We then have:
\begin{align*}
  \frac{\der y}{\der x} & \approx \frac{\Delta y}{\Delta x} \\
           &= \frac{1.0201-1.0000}{1.0100-1.0000} \\
           &= \frac{0.0201}{0.0100} \\
           &= 2.01
\end{align*}
This is quite a good approximation to the exact answer, 2. If we needed a better approximation,
we could take Q even closer to P. In reality we would use this technique in cases where we didn't
know the exact answer, and we would then want to know how accurate our result was. To do this,
we could redo the calculation with a smaller value of $\Delta x$, say 0.001, and look for the
most significant decimal place that changed.
<% marg(200) %>
<%
  fig(
    'approximating-tangent-line',
    %q{The dotted line through P and Q is a good approximation to the tangent line through P.}
  )  
%>
<% end_marg %>
<% end_sec('approximating-deriv') %>
<% begin_sec("Approximating finite changes",nil,'approximating-changes') %>
Sometimes we know the derivative and want to use it as an approximation to find out about
finite changes in the variables. 
For example,
the Women's National Basketball Association says that balls used in its games should have
a radius of 11.6 cm, with an allowable range of error of plus or minus 0.1 cm (one millimeter).
How accurately can we determine the ball's volume?

The equation for the volume of a sphere gives $V=(4/3)\pi r^3=6538\ \zu{cm}^3$ (about six and
a half liters). We have a function $V(r)$, and we want to know how much of an effect will be
produced on the function's output $V$ if its input $r$ is changed by a certain small amount.
Since the amount by which $r$ can be changed is small compared to $r$, it's reasonable to
apply the approximation
\begin{equation*}
  \frac{\Delta V}{\Delta r} \approx \frac{\der V}{\der r} \qquad ,
\end{equation*}
which gives
\begin{align*}
  \Delta V & \approx \frac{\der V}{\der r}\Delta r \\
           &= 4\pi r^2 \Delta r \qquad .
\end{align*}
(Note that the factor of $4\pi r^2$ can be interpreted as the ball's surface area.)
Plugging in numbers, we find that the volume could be off by
as much as $(4\pi r^2)(0.1\ \zu{cm})=170\ \zu{cm}^3$. The volume of the ball can therefore
be expressed as $6500\pm 170\ \zu{cm}^3$, where the original figure of 6538 has been rounded off
to the nearest hundred in order to avoid creating the impression that the 3 and the 8 actually
mean anything --- they clearly don't, since the possible error is out in the hundreds' place.
<% marg(100) %>
<%
  fig(
    'basketball',
    %q{How accurately can we determine the ball's volume?}
  )  
%>
<% end_marg %>

This calculation is an example of a very common situation that occurs in the sciences, and even
in everyday life, in which we base a calculation on a number that has some range of uncertainty
in it, causing a corresponding range of uncertainty in the final result. This is called
propagation of errors. The idea is that the derivative expresses how sensitive the function's
output is to its input.

The example of the basketball could also have been handled without calculus, simply by recalculating the volume
using a radius that was raised from 11.6 to 11.7 cm, and finding the difference between the two volumes.
Understanding it in terms of calculus, however, gives us a different way of getting at the same
ideas, and often allows us to understand more deeply what's going on. For example, we noticed in passing that
the derivative of the volume was simply the surface area of the ball, which provides a nice geometric
visualization. We can imagine inflating the ball so that its radius is increased by a millimeter. The amount of
added volume equals the surface area of the ball multiplied by one millimeter, just as the amount of volume
added to the world's oceans by global warming equals the oceans' surface area multiplied by the added depth.

As another example of an insight that we would have missed if we hadn't applied calculus,
consider how much error is incurred in the measurement of the width of a book if the ruler is
placed on the book at a slightly incorrect angle, so that it doesn't form an angle of exactly 90 degrees with
spine. The measurement has its minimum (and correct) value if the ruler is placed at exactly
90 degrees. Since the function has a minimum at this angle, its derivative is zero. That means that
we expect essentially no error in the measurement if the ruler's angle is just a tiny bit off.
This gives us the insight that it's not worth fiddling excessively over the angle in this measurement.
Other sources of error will be more important. For example, is the book a uniform rectangle? Are we using
the worn end of the ruler as its zero, rather than letting the ruler hang over both sides of the book
and subtracting the two measurements?
<% end_sec('approximating-changes') %>
<% end_sec('approximations') %>

\begin{hwsection}

<% hw('diff-monomials',{'solution'=>true}) %>
<% hw('diff-polynomial',{'solution'=>true}) %>
<% hw('diff-polynomial-2') %>
<% hw('diff-polynomial-3') %>
<% hw('diff-polynomial-4') %>
<% hw('diff-polynomial-5') %>
<% hw('same-deriv',{'solution'=>true}) %>

<% hw('diff-symbolic-const',{'solution'=>true}) %>
<% hw('human-cylinder') %>
<% hw('observable-universe') %>
<% hw('car-ke-and-power') %>

\pagebreak

<% hw('deriv-props-add-implies-stretch',{'solution'=>true}) %>
<% hw('deriv-props-line-implies-constant') %>
<% hw('deriv-props-add-and-constant-imply-shift') %>

<% hw('rancher-rick',{'solution'=>true}) %>
<% hw('prove-n-extrema',{'solution'=>true}) %>
<% hw('overtime') %>

\pagebreak

<% hw('line-of-stability') %>

<%
  fig(
    'hw-line-of-stability',
    %q{Problem \ref{hw:line-of-stability}.},
    {
      'width'=>'fullpage'
    }
  )
%>


<% hw('chain-rule-circle') %>
<% hw('chain-rule-sphere') %>
<% hw('chain-rule-triangle') %>

<% hw('numerical-deriv-cos-x-cubed') %>
<% hw('numerical-deriv-sin-sqrt-x') %>
<% hw('numerical-deriv-e-to-cos-x') %>
<% hw('numerical-deriv-woods-saxon') %>




\end{hwsection}

<% end_chapter %>
