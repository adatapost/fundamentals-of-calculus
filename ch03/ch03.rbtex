%%chapter%% 03
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '03',
    %q{More about limits and continuity},
    'ch:more-limits'
  )
%>

<% begin_sec("Computing limits",nil,'computing-limits') %>
In ch.~\ref{ch:limits} we did very few direct computations of limits. In this section we
learn some techniques for doing such computations.

<% begin_sec("The ``don't choose $\\delta$ to be too small'' trick",nil,'dont-choose') %>
Say we want to prove that $\lim_{x\to 1}x^2 = 1$. This may not seem to require a fancy
proof, since obviously plugging in $x=1$ gives $x^2=1$. But as we'll see
in section \ref{sec:continuity}, plugging in does not always prove the value of a limit.
Also, this example will be an excuse to develop a technique that can be useful in
less trivial cases.

We have $f(x) = x^2$, $a=1$, $L=1$, and as usual when computing a limit the question is, ``how small
should $|x-1|$ be to guarantee $|x^2-1|<\varepsilon$?''

We begin by estimating the difference $|x^2-1|$
\[
|x^2-1| = |(x-1)(x+1)| = |x+1|\cdot|x-1|.
\]
As $x$ approaches 1 the factor $|x-1|$ becomes small, and if the other
factor $|x+1|$ were a constant (e.g.\ $2$ as in the previous example) then
we could find $\delta$ as before, by dividing $\varepsilon$ by that
constant.

Here is a trick that allows you to replace the factor $|x+1|$ with a
constant.  We hereby agree \textit{that we always choose our $\delta$ so
that $\delta\leq 1$.}  If we do that, then we will always have
\[
|x-1|<\delta\leq 1, \text{i.e. }|x-1|<1,
\]
and $x$ will always be beween $0$ and $2$. Therefore
\[
|x^2-1| = |x+1|\cdot|x-1|<3|x-1|.
\]
If we now want to be sure that $|x^2-1|<\varepsilon$, then this calculation
shows that we should require $3|x-1|<\varepsilon$, i.e.\
$|x-1|<\frac13\varepsilon$.  So we should choose $\delta\leq
\frac13\varepsilon$.  We must also live up to our promise never to choose
$\delta>1$, so if we are handed an $\varepsilon$ for which
$\frac13\varepsilon>1$, then we choose $\delta=1$ instead of $\delta =
\frac13\varepsilon$.  To summarize, we are going to choose
\[
\delta = \text{the smaller of }1\text{ and }\frac13\varepsilon.
\]
We have shown that if you choose $\delta$ this way, then $|x-1|<\delta$
implies $|x^2-1|<\varepsilon$, no matter what $\varepsilon>0$ is.
                                                                                                                 
The expression ``the smaller of $a$ and $b$'' shows up often, and is
abbreviated to $\min(a, b)$.  We could therefore say that in this problem
we will choose $\delta$ to be
\[
\delta = \min \bigl(1, \tfrac13 \varepsilon\bigr).
\]

\begin{eg}{}
\egquestion Show that $\lim_{x\to 4}1/x = 1/4$.

\eganswer We apply the definition with $a=4$, $L=1/4$ and $f(x) = 1/x$.
Thus, for any $\varepsilon>0$ we try to show that if $|x-4|$ is small
enough then one has $|f(x)-1/4|<\varepsilon$.

We begin by estimating $|f(x)-\frac14|$ in terms of $|x-4|$:
\[
|f(x)-1/4| = \left|\frac1x-\frac14\right| = \left| \frac{4-x}{4x}\right| =
\frac{|x-4|}{|4x|} =\frac{1}{|4x|}\,|x-4|.
\]
As before, things would be easier if $1/|4x|$ were a constant.  To achieve
that we again agree not to take $\delta>1$.  If we always have $\delta\leq
1$, then we will always have $|x-4|<1$, and hence $3<x<5$.  How large can
$1/|4x|$ be in this situation?  Answer: the quantity $1/|4x|$ increases as
you decrease $x$, so if $3<x<5$ then it will never be larger than
$1/|4\cdot 3| = \frac1{12}$.

We see that if we never choose $\delta>1$, we will always have
\[
|f(x) - \tfrac14|\leq \tfrac1{12}|x-4| \quad\text{for}\quad |x-4|<\delta.
\]
To guarantee that $|f(x)-\frac14|<\varepsilon$ we could threfore require
\[
\tfrac1{12} |x-4|<\varepsilon, \quad\text{i.e.}\quad |x-4| <12\varepsilon.
\]
Hence if we choose $\delta=12\varepsilon$ or any smaller number, then
$|x-4|<\delta$ implies $|f(x)-4|<\varepsilon$.  Of course we have to honor
our agreement never to choose $\delta>1$, so our choice of $\delta$ is
\[
\delta = \text{the smaller of }1\text{ and }12\varepsilon = \min \bigl(1,
12\varepsilon\bigr).
\]
\end{eg}

<% end_sec('dont-choose') %>
<% begin_sec("Properties of the limit",nil,'properties-of-the-limit') %>
Epsilon-delta proofs are hard work, and by building up a more sophisticated
set of tools we can usually avoid having to apply the epsilon-delta definition directly.

<% begin_sec("Limits of constants and of $x$",nil,'limits-of-constants-and-x') %>
If $a$ and $c$ are constants,
then
\begin{equation}
  \lim_{x\to a}c=c \tag{$P_1$}
\end{equation}
and
\begin{equation}
  \lim_{x\to a} x= a.\tag{$P_2$}
\end{equation}
<% end_sec('limits-of-constants-and-x') %>

<% begin_sec("Limits of sums, products and quotients",nil,'limits-of-sum-prod-quot') %>
Let $F_1$ and $F_2$ be
two given functions whose limits for $x\to a$ we know,
\[
\lim_{x\to a}F_1(x)=L_1, \qquad \lim_{x\to a}F_2(x)=L_2.
\]
Then
\begin{align}
  \lim_{x\to a}\bigl(F_1(x)+F_2(x)\bigr)=L_1+L_2, \tag{$P_3$} \\
  \lim_{x\to a}\bigl(F_1(x)-F_2(x)\bigr)= L_1 - L_2, \tag{$P_4$} \\
  \lim_{x\to a}\bigl(F_1(x)\cdot F_2(x)\bigr)= L_1\cdot L_2 \tag{$P_5$}
\end{align}
Finally, if $ \lim_{x\to a}F_2(x)\ne0$,
\begin{equation}
  \lim_{x\to a}\frac{F_1(x)}{F_2(x)}= \frac{L_1}{L_2}.\tag{$P_6$}
\end{equation}
   
In other words the limit of the sum is the sum of the limits, etc.  One can
prove these laws using the definition of the limit, but we will not do this here.  However, I hope
these laws seem like common sense: if, for $x$ close to $a$, the quantity
$F_1(x)$ is close to $L_1$ and $F_2(x)$ is close to $L_2$, then certainly
$F_1(x)+F_2(x)$ should be close to $L_1+L_2$.

\begin{eg}{}
In this example we compute several limits, building up from simple examples to
more complicated ones.

First let's  $\lim_{x\to2}x^2$.
We have
\begin{align*}
  \lim_{x\to2} x^2 &= \lim_{x\to2} x\cdot x \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x\bigr)
  &\text{by $(P_5)$}\\
  &= 2\cdot 2 = 4.
\end{align*}
Similarly,
\begin{align*}
  \lim_{x\to2} x^3 &= \lim_{x\to2} x\cdot x^2 \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x^2\bigr)
  &\text{$(P_5)$ again}\\
  &= 2\cdot 4 = 8,
\end{align*}
and, by $(P_4)$
\[
\lim_{x\to2} x^2-1 = \lim_{x\to2} x^2 - \lim_{x\to2} 1 = 4-1 = 3,
\]
and, by $(P_4)$ again,
\[
\lim_{x\to2} x^3-1 = \lim_{x\to2} x^3 - \lim_{x\to2} 1 = 8-1 = 7,
\]
Putting all this together, we gets
\[
\lim_{x\to 2}\frac{x^3-1}{x^2-1} = \frac{2^3-1}{2^2-1} = \frac{8-1}{4-1}=
\frac{7}{3}
\]
because of $(P_6)$.  To apply $(P_6)$ we must check that the denominator
(``$L_2$'') is not zero.  Since the denominator is 3, this was all right.
\end{eg}

\begin{eg}{The limit of a square root}
\egquestion Find $\lim_{x\to 2}\sqrt x$.

\eganswer Of course, you would think that $\lim_{x\to 2}\sqrt x = \sqrt 2$ and you
can indeed prove this using $\delta$ \& $\varepsilon$ (See
problem~\ref{ex:03limitofsqrt4}.) But is there an easier way?  There is
nothing in the limit properties which tells us how to deal with a square
root, and using them we can't even prove that there is a limit.  However,
if you \emph{assume} that the limit exists then the limit properties allow
us to find this limit.

The argument goes like this: suppose that there is a number $L$ with
\[
\lim_{x\to 2} \sqrt x = L.
\]
Then property $(P_5)$ implies that
\[
L^2 = \bigl(\lim_{x\to2}\sqrt x\bigr)\cdot\bigl(\lim_{x\to2}\sqrt x\bigr) =
\lim_{x\to2} \sqrt x\cdot \sqrt x =\lim_{x\to2}x =2.
\]
In other words, $L^2=2$, and hence $L$ must be either $\sqrt 2$ or $-\sqrt
2$.  We can reject the latter because whatever $x$ does, its squareroot is
always a positive number, and hence it can never ``get close to'' a
negative number like $-\sqrt 2$.


Our conclusion: if the limit exists, then
\[
\lim_{x\to2} \sqrt x = \sqrt 2.
\]
The result is not surprising: if $x$ gets close to 2 then $\sqrt x$ gets
close to $\sqrt 2$.
\end{eg}
<% end_sec('limits-of-sum-prod-quot') %>

<% end_sec('properties-of-the-limit') %>
<% begin_sec("When limits fail to exist",nil,'when-limits-fail-to-exist') %>
In the last couple of examples we worried about the possibility that a
limit $\lim_{x\to a}g(x)$ actually might not exist.  This can actually
happen, and in this section we'll see a few examples of what failed limits
look like.  First let's agree on what we will call a ``failed limit.''

\begin{lessimportant}
If there is no number $L$ such that $\lim_{x\to a}f(x) = L$, then we say
that the limit $\lim_{x\to a}f(x)$ does not exist. 
\end{lessimportant}

<% marg(0) %>
<%
  fig(
    'sign-function',
    %q{The sign function.}
  )  
%>
<% end_marg %>

\begin{eg}{The sign function near $x=0$}\label{eg:limit-of-sign-function}
The ``sign function'' is defined by
\[
\operatorname{sign} (x) =
\begin{cases}
  -1 & \text{for $x<0$}\\ 0 & \text{for $x=0$}\\ 1 & \text{for $x>0$}
\end{cases}
\]
Note that ``the sign of zero'' is defined to be zero.  But does the sign
function have a limit at $x=0$, i.e.\ does $ \lim_{x\to0} \operatorname{sign}(x) $ exist?
And is it also zero? The answers are \emph{no} and \emph{no}, and here is
why: suppose that for some number $L$ one had
\[
  \lim_{x\to 0}\operatorname{sign}(x) = L,
\]
then since for arbitrary small positive values of $x$ one has $\operatorname{sign}(x) =
+1$ one would think that $L=+1$.  But for arbitrarily small negative values
of $x$ one has $\operatorname{sign}(x)=-1$, so one would conclude that $L=-1$.  But one
number $L$ can't be both $+1$ and $-1$ at the same time, so there is no
such $L$, i.e.\ there is no limit.
\[
\lim_{x\to 0} \operatorname{sign} (x) \text{ does not exist.}
\]

In examples like this one, it is possible to define a one-sided limit; see
section \ref{subsec:one-sided-limit}.
\end{eg}

%%graph%% backward-sine-raw func=sin(3.1416/x) format=svg xlo=0.02 xhi=3 ylo=-1 yhi=1 with=lines samples=1000

<%
  fig(
    'backward-sine',
    %q{Example \ref{eg:backward-sine}.},
    {
      'width'=>'fullpage'
    }
  )
%>

\begin{eg}{The ``backward sine''}\label{eg:backward-sine}
Figure \figref{backward-sine} shows the  ``backward sine'' function
$f(x)=\sin(\pi/x)$.
Contemplate its limit as $x\to0$:
\[
\lim_{x\to 0}\sin \bigl(\frac\pi x\bigr) \qquad .
\]

When $x=0$ the function $f(x)$ is not defined, because its
definition involves division by $x$.  What happens to $f(x)$ as $x\to0$?
First, $\pi /x$ becomes larger and larger (``goes to infinity'') as
$x\to0$.  Then, taking the sine, we see that $ \sin(\pi /x)$ oscillates
between $+1$ and $-1$ infinitely often as $x\to0$.  This means that $f(x)$
gets close to any number between $-1$ and $+1$ as $x\to0$, but that the
function $f(x)$ \emph{never stays close} to any particular value because it
keeps oscillating up and down. The limit fails to exist, but for a different
reason than in example \ref{eg:limit-of-sign-function}.
\end{eg}

\begin{eg}{Trying to divide by zero using a limit}\label{eg:limit-to-divide-by-zero}
The expression $1/0$
is not defined, but what about
\[
\lim_{x\to0}\frac1x?
\]
This limit also does not exist.  Here are two reasons:

It is common wisdom that if you divide by a small number you get a large
number, so as $x\searrow 0$ the quotient $1/x$ will not be able to stay
close to any particular finite number, and the limit can't exist.

``Common wisdom'' is not always a reliable tool in mathematical proofs, so
here is a better argument.  The limit can't exist, because that would
contradict the limit properties $(P_1)\cdots(P_6)$.  Namely, suppose that
there were an number $L$ such that
\[
\lim_{x\to0} \frac 1 x = L.
\]
Then the limit property $(P_5)$ would imply that
\[
\lim_{x\to0}\bigl(\frac 1x\cdot x \bigr) = \bigl(\lim_{x\to0}\frac
1x\bigr)\cdot \bigl(\lim_{x\to0} x\bigr) = L\cdot 0 =0.
\]
On the other hand $\frac 1x \cdot x =1$ so the above limit should be 1!  A                                       
number can't be both 0 and 1 at the same time, so we have a
contradiction. The assumption that $\lim_{x\to0}1/x$ exists is to blame, so
it must go.
\end{eg}

<% begin_sec("Using limit properties to show a limit does \\emph{not} exist",nil,'props-to-show-no-limit') %>
The limit properties tell us how to prove that certain limits exist (and
how to compute them).  Although it is perhaps not so obvious at first
sight, they also allow you to prove that certain limits do not exist. 
Example \ref{eg:limit-to-divide-by-zero} shows one instance of such use.  Here is another.

Property $(P_3)$ says that if both $\lim_{x\to a}g(x)$ and $\lim_{x\to a}
h(x) $ exist then $\lim_{x\to a}g(x)+h(x)$ also must exist.  You can turn
this around and say that if $\lim_{x\to a}g(x)+h(x)$ does not exist then
either $\lim_{x\to a}g(x)$ or $\lim_{x\to a }h(x)$ does not exist (or both
limits fail to exist).

For instance, the limit
\[
\lim_{x\to 0} \frac 1x-x
\]
can't exist, for if it did, then the limit
\[
\lim_{x\to0} \frac 1x =\lim_{x\to 0} \bigl(\frac1x-x +x\bigr) =\lim_{x\to
0} \bigl(\frac1x-x\bigr) + \lim_{x\to 0} x
\]
would also have to exist, and we know $\lim_{x\to 0}\frac1x$ doesn't exist.

<% end_sec('props-to-show-no-limit') %>
<% end_sec('when-limits-fail-to-exist') %>
<% end_sec('computing-limits') %>

<% begin_sec("Continuity",nil,'continuity') %>
Intuitively, a continuous function is one whose graph
has no sudden jumps in it; the graph is all a single connected piece. 
Such a function can be drawn without picking the pen up off of the paper.
Formally, continuity is defined as follows.

\begin{important}
A function $g$ is \emph{continuous} at $a$ if
\begin{equation}\label{eq:continuity-def}
  \lim_{x\to a} g(x) = g(a)
\end{equation}
A function is continuous if it is continuous at every $a$ in its
domain.
\end{important}

If a function is discontinuous at a given point, then it is not differentiable at that point.
On the other hand, the example $y=|x|$ shows that a function can be continuous without
being differentiable.

<% marg(0) %>
<%
  fig(
    'discontinuous',
    %q{A discontinuous function.}
  )  
%>
<% end_marg %>

In most cases, there is no need to invoke the definition explicitly in order to check whether
a function is continuous. Most of the functions we work with are defined by putting together
simpler functions as building blocks. For example, let's say we're already convinced that the
functions defined by $g(x)=3x$ and $h(x)=\sin x$ are both continuous. Then if we encounter the
function $f(x)=\sin(3x)$, we can tell that it's continuous because its definition corresponds
to $f(x)=h(g(x))$. The functions $g$ and $h$ have been set up like a bucket brigade, so that
$g$ takes the input, calculates the output, and then hands it off to $h$ for the final step
of the calculation. This method of combining functions is called \emph{composition}.\index{composition}
The composition of two continuous functions is also continuous. Just watch out for division.
The function $f(x)=1/x$ is continuous everywhere except at $x=0$, so for example $1/\sin(x)$
is continuous everywhere except at multiples of $\pi$, where the sine has zeroes.

<% end_sec('continuity') %>

<% end_chapter %>
